{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SpectraNorm","text":"<p>A Python package for spectral normative modeling of high-dimensional data.</p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>You can install the package using pip:</p> <pre><code>pip install spectranorm --upgrade\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the tutorials to get started with using SpectraNorm.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>The API reference provides detailed documentation of several functions and classes available in the package.</p>"},{"location":"#dig-deeper","title":"Dig Deeper","text":"<p>For more in-depth information about the underlying theory and example uses of spectral normative modeling, check out this GitHub repository which hosts several notebooks that implement spectral normative modeling of cortical thickness phenotypes on a large scale population-wide imaging biobank. You may also be interested in the [original paper][link to be added] describing the method and its application to brain imaging data.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"license/","title":"Dual Licensing Options","text":"<p>The source code and models within this repository are dual licensed. You may choose to use it under:</p> <ol> <li>The terms of the GNU Affero General Public License version 3 (AGPLv3) for non-commercial purposes; OR</li> <li>A separate commercial license for commercial use.</li> </ol>"},{"location":"license/#non-commercial-license-open-source","title":"Non-Commercial License (Open Source)","text":"<p>For non-commercial uses and academic purposes, a license is granted under the terms of the GNU Affero General Public License version 3 (AGPLv3): \ud83d\udd17 https://www.gnu.org/licenses/agpl-3.0.en.html</p> <p>This includes the right to use, modify, and redistribute the code and models for non-commercial research and academic use only, subject to the AGPLv3 conditions.</p>"},{"location":"license/#commercial-license","title":"Commercial License","text":"<p>For any commercial use \u2014 including incorporation into products, services, or any revenue-generating activities \u2014 you must obtain a separate commercial license.</p> <p>Please contact the following for commercial licensing inquiries:</p> <ul> <li>Sina Mansour L. (sina.mansour.lakouraj@gmail.com)</li> <li>B.T. Thomas Yeo (thomas.yeo@nus.edu.sg)</li> <li>Jonathan Tan (jonathan_tan@nus.edu.sg)</li> </ul>"},{"location":"license/#reporting-unauthorized-use-licensing-inquiries","title":"Reporting Unauthorized Use / Licensing Inquiries","text":"<p>If you become aware of any unauthorized commercial use of this code and models, or have questions about licensing terms, please contact:</p> <ul> <li>Jonathan Tan (jonathan_tan@nus.edu.sg)</li> </ul> <p>\u00a9 2025 Sina Mansour L. and collaborators. All rights reserved.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page provides an overview of the modules and functions available in SpectraNorm. Click on a specific category to see the respective detailed API documentation.</p> Module Description SNM Core spectral normative modeling concepts and algorithms. General Utilities General-purpose helper functions used across the package. Graph Signal Processing Tools for spectral analysis on graphs. Metrics Utilities Functions for evaluating model performance. Neuroimaging Tools Functions for working with neuroimaging data. Parallel Utilities Functions for managing parallel execution and tracking progress. Statistical Utilities Functions for statistical analyses related to normative modeling."},{"location":"api/snm/","title":"Core SNM Functionality","text":"<p>This section covers the core set of functions and classes that implement the main spectral normative modeling algorithms and concepts. This includes classes for fitting univariate and multivariate (spectral) normative models, as well as classes used for defining covariates and how they influence the model.</p>"},{"location":"api/snm/#spectranorm.snm","title":"<code>spectranorm.snm</code>","text":"<p>snm.py</p> <p>Core implementation of spectral normative modeling (SNM).</p> <p>This module provides the code base for using spectral normative models. It can be used to fit direct and spectral normative models to data and also to predict normative centiles using pre-trained models.</p> <p>See full documentation at: https://sina-mansour.github.io/spectranorm</p>"},{"location":"api/snm/#spectranorm.snm.CovarianceModelSpec","title":"<code>CovarianceModelSpec</code>  <code>dataclass</code>","text":"<p>General specification of a normative model for covariance.</p> <p>This model aims to learn the relationships between two variables of interest for both of which a normative model is specified. This can capture patterns where the normative trends in two variables are related.</p> <p>Attributes:</p> Name Type Description <code>variable_of_interest_1</code> <code>str</code> <p>str Name of the first variable of interest.</p> <code>variable_of_interest_2</code> <code>str</code> <p>str Name of the second variable of interest.</p> <code>covariates</code> <code>list[CovariateSpec]</code> <p>list[CovariateSpec] Listing all model covariates and specifying how each covariate is modeled.</p> <code>influencing_covariance</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the covariance between the two variables of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovarianceModelSpec:\n    \"\"\"\n    General specification of a normative model for covariance.\n\n    This model aims to learn the relationships between two variables of interest for\n    both of which a normative model is specified. This can capture patterns where the\n    normative trends in two variables are related.\n\n    Attributes:\n        variable_of_interest_1: str\n            Name of the first variable of interest.\n        variable_of_interest_2: str\n            Name of the second variable of interest.\n        covariates: list[CovariateSpec]\n            Listing all model covariates and specifying how each covariate is modeled.\n        influencing_covariance: list[str]\n            List of covariate names that influence the covariance between the two\n            variables of interest.\n    \"\"\"\n\n    variable_of_interest_1: str\n    variable_of_interest_2: str\n    covariates: list[CovariateSpec]\n    influencing_covariance: list[str]\n\n    def __post_init__(self) -&gt; None:\n        if not isinstance(self.variable_of_interest_1, str):\n            err = \"variable_of_interest_1 must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.variable_of_interest_2, str):\n            err = \"variable_of_interest_2 must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.covariates, list):\n            err = \"covariates must be a list of CovariateSpec instances.\"\n            raise TypeError(err)\n        if not all(isinstance(cov, CovariateSpec) for cov in self.covariates):\n            err = \"All items in covariates must be CovariateSpec instances.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_covariance, list):\n            err = \"influencing_covariance must be a list of covariate names.\"\n            raise TypeError(err)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel","title":"<code>CovarianceNormativeModel</code>  <code>dataclass</code>","text":"<p>Covariance normative model implementation.</p> <p>This class implements covariance normative modeling, which models the covariance structure between a pair of variables as a normative random variable.</p> <p>Attributes:</p> Name Type Description <code>spec</code> <code>CovarianceModelSpec</code> <p>CovarianceModelSpec Specification of the covariance model including variables of interest, and list of covariates.</p> <code>defaults</code> <code>dict[str, Any]</code> <p>dict Default parameters for the model, including spline specifications, ADVI iterations, convergence tolerance, random seed, and Adam optimizer learning rates.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovarianceNormativeModel:\n    \"\"\"\n    Covariance normative model implementation.\n\n    This class implements covariance normative modeling, which models the covariance\n    structure between a pair of variables as a normative random variable.\n\n    Attributes:\n        spec: CovarianceModelSpec\n            Specification of the covariance model including variables of interest,\n            and list of covariates.\n        defaults: dict\n            Default parameters for the model, including spline specifications,\n            ADVI iterations, convergence tolerance, random seed, and Adam optimizer\n            learning rates.\n    \"\"\"\n\n    spec: CovarianceModelSpec\n    defaults: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"spline_df\": DEFAULT_SPLINE_DF,\n            \"spline_degree\": DEFAULT_SPLINE_DEGREE,\n            \"spline_extrapolation_factor\": DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n            \"advi_iterations\": DEFAULT_ADVI_ITERATIONS,\n            \"advi_convergence_tolerance\": DEFAULT_ADVI_CONVERGENCE_TOLERANCE,\n            \"random_seed\": DEFAULT_RANDOM_SEED,\n            \"adam_learning_rate\": DEFAULT_ADAM_LEARNING_RATE,\n            \"adam_learning_rate_decay\": DEFAULT_ADAM_LEARNING_RATE_DECAY,\n        },\n    )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the CovarianceNormativeModel instance.\n        \"\"\"\n        return f\"CovarianceNormativeModel(\\n\\tspec={self.spec}\\n)\"\n\n    @classmethod\n    def from_direct_model(\n        cls,\n        direct_model: DirectNormativeModel,\n        variable_of_interest_1: str,\n        variable_of_interest_2: str,\n        influencing_covariance: list[str] | None = None,\n        defaults_overwrite: dict[str, Any] | None = None,\n    ) -&gt; CovarianceNormativeModel:\n        \"\"\"\n        Initialize the model from a direct model instance, and two variable names.\n\n        Args:\n            direct_model: DirectNormativeModel\n                This model will be used to instantiate a similar covariance model.\n            variable_of_interest_1: str\n                Name of the first target variable to model.\n            variable_of_interest_2: str\n                Name of the second target variable to model.\n            influencing_covariance: list[str] | None\n                List of covariates that influence the covariance structure. If not\n                provided, this will be copied from the direct model's\n                `influencing_variance`.\n\n        Returns:\n            CovarianceNormativeModel\n                An instance of CovarianceNormativeModel initialized with the provided\n                data.\n        \"\"\"\n        # Validity checks for input parameters\n        if not isinstance(direct_model, DirectNormativeModel):\n            err = \"direct_model must be an instance of DirectNormativeModel.\"\n            raise TypeError(err)\n        if not (\n            isinstance(variable_of_interest_1, str)\n            and isinstance(variable_of_interest_2, str)\n        ):\n            err = \"Variables of interest must be strings.\"\n            raise TypeError(err)\n\n        # Substitute influencing_covariance if not provided\n        if influencing_covariance is None:\n            influencing_covariance = direct_model.spec.influencing_variance\n\n        # Use the same setup as the direct model\n        model = cls(\n            spec=CovarianceModelSpec(\n                variable_of_interest_1=variable_of_interest_1,\n                variable_of_interest_2=variable_of_interest_2,\n                covariates=direct_model.spec.covariates,\n                influencing_covariance=influencing_covariance,\n            ),\n        )\n\n        # update defaults\n        model.defaults.update(direct_model.defaults)\n        model.defaults.update(defaults_overwrite or {})\n\n        return model\n\n    def _validate_model(self) -&gt; None:\n        \"\"\"\n        Validate the covariance model instance.\n\n        This method checks if the model instance is complete and valid.\n        It raises errors if any required fields are missing or if there are\n        inconsistencies in the model instance.\n        \"\"\"\n        if self.spec is None:\n            err = (\n                \"Model specification is not set. Please initialize the model,\"\n                \" e.g., with 'from_dataframe'.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.covariates) == 0:\n            err = (\n                \"No covariates specified in the model. \"\n                \"Please add covariates to the specification.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.influencing_covariance) == 0:\n            err = (\n                \"No covariates specified to influence the covariance \"\n                \"between the variables of interest.\"\n            )\n            raise ValueError(err)\n\n    def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n        \"\"\"\n        Save the fitted model and it's posterior to a directory.\n        The model will be saved in a subdirectory named 'saved_model'.\n        If this directory is not empty, an error is raised.\n\n        Args:\n            directory: Path\n                Path to a directory to save the model.\n            save_posterior: bool (default=False)\n                If True, save the model's posterior trace inference data.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n        model_dict = {\n            \"spec\": self.spec,\n            \"defaults\": self.defaults,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n            if hasattr(self, \"model_inference_data\") and save_posterior:\n                self.model_inference_data.to_netcdf(\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n        joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n\n    @classmethod\n    def load_model(\n        cls,\n        directory: Path,\n        *,\n        load_posterior: bool = False,\n    ) -&gt; CovarianceNormativeModel:\n        \"\"\"\n        Load the model and its posterior from a directory.\n        The model will be loaded from a subdirectory named 'saved_model'.\n\n        Args:\n            directory: Path\n                Path to the directory containing the model.\n            load_posterior: bool (default=False)\n                If True, load the model's posterior trace from the saved inference data.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"saved_model\",\n        )\n\n        # Load the saved model dict\n        model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n        # Create an instance of the class\n        instance = cls(\n            spec=model_dict[\"spec\"],\n        )\n\n        # Set the attributes from the loaded model dictionary\n        instance.defaults.update(model_dict[\"defaults\"])\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n            if load_posterior:\n                instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n\n        return instance\n\n    def _validate_dataframe_for_fitting(self, train_data: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Validate the training DataFrame for fitting.\n        \"\"\"\n        utils.general.validate_dataframe(\n            train_data,\n            (\n                [cov.name for cov in self.spec.covariates]\n                + [\n                    self.spec.variable_of_interest_1,\n                    self.spec.variable_of_interest_2,\n                    f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_std_estimate\",\n                ]\n            ),\n        )\n\n    def _build_model_coordinates(\n        self,\n        observations: npt.NDArray[np.integer[Any]],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Build the model coordinates for the training DataFrame.\n        \"\"\"\n        # Data coordinates\n        model_coords = {\"observations\": observations, \"scalar\": [0]}\n\n        # Additional coordinates for covariates\n        for cov in self.spec.covariates:\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"spline\":\n                    if cov.spline_spec is not None:  # to satisfy type checker\n                        model_coords[f\"{cov.name}_splines\"] = np.arange(\n                            cov.spline_spec.df,\n                        )\n                elif cov.effect == \"linear\":\n                    model_coords[f\"{cov.name}_linear\"] = np.arange(1)\n            elif cov.cov_type == \"categorical\":\n                model_coords[cov.name] = cov.categories\n            else:\n                err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                raise ValueError(err)\n        return model_coords\n\n    def _model_linear_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        sigma_prior: float = 0.1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate.\n        \"\"\"\n        # Linear effect\n        if adapt is None:  # Model fitting\n            linear_beta = pm.Normal(\n                f\"linear_beta_{cov.name}\",\n                mu=0,\n                sigma=sigma_prior,\n                size=1,\n                dims=(f\"{cov.name}_linear\",),\n            )\n            # Increment parameter count for linear effect\n            self.model_params[\"n_params\"] += 1\n        else:  # Freeze during adaptation/fine-tuning\n            linear_beta = pm.Deterministic(\n                f\"linear_beta_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"linear_beta_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_linear\",),\n            )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                (\n                    (\n                        (cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()))\n                        - cov.moments[0]\n                    )\n                    / cov.moments[1]\n                )\n                * linear_beta,\n            )\n\n    def _model_spline_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            spline_betas = pm.ZeroSumNormal(\n                f\"spline_betas_{cov.name}\",\n                sigma=sigma_prior,\n                shape=spline_bases[cov.name].shape[1],\n                dims=(f\"{cov.name}_splines\",),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n            # Increment parameter count for spline effects\n            if cov.spline_spec is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n        else:  # Freeze during adaptation/fine-tuning\n            spline_betas = pm.Deterministic(\n                f\"spline_betas_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"spline_betas_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_splines\",),\n            )\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))\n\n    def _model_categorical_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 1,\n        hierarchical_sigma_prior: float = 0.1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.HalfNormal(\n                    f\"sigma_intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(\"scalar\",),\n                )\n\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.ZeroSumNormal(\n                    f\"intercept_offset_{cov.name}\",\n                    sigma=hierarchical_sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n\n                # Increment parameter count for hierarchical intercept\n                self.model_params[\"n_params\"] += 1\n\n            else:\n                # Non-hierarchical (linear) categorical effect\n                categorical_intercept = pm.ZeroSumNormal(\n                    f\"intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n            # Increment parameter count for categorical effects\n            if cov.categories is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += len(cov.categories) - 1\n        elif cov.name != adapt[\"covariate_to_adapt\"]:\n            # Freeze during adaptation/fine-tuning\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.Deterministic(\n                    f\"sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                # Non-hierarchical (linear) categorical effect\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n        else:\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                # Hyperpriors are fixed during adaptation\n                sigma_intercept_category = pm.Deterministic(\n                    f\"sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                new_categorical_intercept_offset = pm.Normal(\n                    f\"intercept_offset_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=hierarchical_sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new offsets\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept_offset,\n                            new_categorical_intercept_offset,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                # Non-hierarchical (linear) categorical effect\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                new_categorical_intercept = pm.Normal(\n                    f\"intercept_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new intercepts\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept,\n                            new_categorical_intercept,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n            self.model_params[\"n_params\"] += new_category_count\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n\n    def _model_all_correlation_effects(\n        self,\n        train_data: pd.DataFrame,\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; list[TensorVariable]:\n        \"\"\"\n        Model all covariate correlation effects.\n        \"\"\"\n        # Create a list to contain the effects of covariates on\n        # the z-transformed correlation\n        z_transformed_correlation_effects = []\n\n        # Model the z-transformed correlation between the variables of interest\n        # Model the global intercept for z\n        if adapt is None:\n            global_intercept_z = pm.Normal(\n                \"global_intercept_z\",\n                mu=0,\n                sigma=5,\n                dims=(\"scalar\",),\n            )\n            # Increment parameter count for global intercept\n            self.model_params[\"n_params\"] += 1\n        else:\n            # Use the pretrained global intercept\n            global_intercept_z = pm.Deterministic(\n                \"global_intercept_z\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        \"global_intercept_z\"\n                    ],\n                ),\n                dims=(\"scalar\",),\n            )\n        z_transformed_correlation_effects.append(global_intercept_z)\n        # Model additional covariate effects on the z estimate\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_covariance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_correlation_effect(\n                            train_data=train_data,\n                            cov=cov,\n                            effects_list=z_transformed_correlation_effects,\n                            adapt=adapt,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_correlation_effect(\n                            train_data=train_data,\n                            cov=cov,\n                            effects_list=z_transformed_correlation_effects,\n                            spline_bases=spline_bases,\n                            adapt=adapt,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_correlation_effect(\n                        train_data=train_data,\n                        cov=cov,\n                        effects_list=z_transformed_correlation_effects,\n                        category_indices=category_indices,\n                        adapt=adapt,\n                    )\n\n                else:\n                    err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    raise ValueError(err)\n        return z_transformed_correlation_effects\n\n    def _combine_all_correlation_effects(\n        self,\n        z_transformed_correlation_effects: list[TensorVariable],\n        combination_indices: npt.NDArray[np.integer[Any]],\n        combination_weights: npt.NDArray[np.floating[Any]],\n        standardized_vois: npt.NDArray[np.floating[Any]],\n        standardized_vois_mu_estimate: npt.NDArray[np.floating[Any]],\n        standardized_vois_std_estimate: npt.NDArray[np.floating[Any]],\n    ) -&gt; None:\n        \"\"\"\n        Combine all effects to model the observed data likelihood from the list of\n        correlation effects.\n        \"\"\"\n        # Combine all covariance effects\n        z_transformed_correlation_estimate = sum(z_transformed_correlation_effects)\n\n        # Convert z-transformed score to correlation\n        correlation_estimate = pt.tanh(z_transformed_correlation_estimate)\n\n        # Now apply the random combinations to get final distribution estimates\n        # Apply combination weights for mu estimate\n        combined_mu_estimate = pt.sum(\n            pt.mul(\n                standardized_vois_mu_estimate[combination_indices, :],\n                combination_weights,\n            ),\n            axis=1,\n        )\n        # Apply combination weights for sigma estimate\n        combined_sigma_estimate = pt.mul(\n            standardized_vois_std_estimate[combination_indices, :],\n            combination_weights,\n        )\n        # Apply combination weights for correlation estimate\n        combined_correlation_estimate = correlation_estimate[combination_indices]  # pyright: ignore[reportOptionalSubscript]\n        # Now build the std estimate\n        combined_std_estimate = pt.sqrt(\n            combined_sigma_estimate[:, 0] ** 2  # pyright: ignore[reportOptionalSubscript]\n            + combined_sigma_estimate[:, 1] ** 2  # pyright: ignore[reportOptionalSubscript]\n            + (\n                2\n                * combined_sigma_estimate[:, 0]  # pyright: ignore[reportOptionalSubscript]\n                * combined_sigma_estimate[:, 1]  # pyright: ignore[reportOptionalSubscript]\n                * combined_correlation_estimate\n            ),\n        )\n\n        # Apply combination to the variables of interest\n        combined_variable_of_interest = pt.sum(\n            pt.mul(\n                standardized_vois[combination_indices, :],\n                combination_weights,\n            ),\n            axis=1,\n        )\n\n        effective_sample_size = self.model_params[\"sample_size\"]\n\n        # Model likelihood estimation for covariance model\n        _likelihood = pm.Normal(\n            f\"likelihood_cov_{self.spec.variable_of_interest_1}_{self.spec.variable_of_interest_2}\",\n            mu=combined_mu_estimate,\n            sigma=combined_std_estimate,\n            observed=combined_variable_of_interest,\n            total_size=(effective_sample_size),\n        )\n\n    def _fit_model_with_advi(self, *, progress_bar: bool = True) -&gt; None:\n        \"\"\"\n        Fit the model using Automatic Differentiation Variational Inference (ADVI).\n        \"\"\"\n        base_lr = self.defaults[\"adam_learning_rate\"]\n        decay = self.defaults[\"adam_learning_rate_decay\"]\n        lr = shared(base_lr)\n        optimizer = pm.adam(learning_rate=cast(\"float\", lr))\n\n        # Adaptive learning rate schedule callback\n        def update_learning_rate(_approx: Any, _loss: Any, iteration: int) -&gt; None:\n            lr.set_value(base_lr * (decay**iteration))\n\n        # Run automatic differential variational inference to fit the model\n        self._trace = pm.fit(\n            method=\"advi\",\n            n=self.defaults[\"advi_iterations\"],\n            random_seed=self.defaults[\"random_seed\"],  # For reproducibility\n            obj_optimizer=optimizer,\n            callbacks=[\n                update_learning_rate,\n                pm.callbacks.CheckParametersConvergence(\n                    tolerance=self.defaults[\"advi_convergence_tolerance\"],\n                    diff=\"relative\",\n                ),\n            ],\n            progressbar=progress_bar,\n        )\n\n        # Sample from the posterior distribution and store the results\n        self.model_inference_data = self._trace.sample(\n            2000,\n            random_seed=self.defaults[\"random_seed\"],\n        )\n\n        # Compute posterior means and standard deviations\n        posterior_means = self.model_inference_data.posterior.mean(\n            dim=(\"chain\", \"draw\"),\n        )\n        posterior_stds = self.model_inference_data.posterior.std(\n            dim=(\"chain\", \"draw\"),\n        )\n\n        # Store posterior means and stds as a dictionary in model parameters\n        self.model_params[\"posterior_means\"] = {\n            x: posterior_means.data_vars[x].to_numpy()\n            for x in posterior_means.data_vars\n        }\n        self.model_params[\"posterior_stds\"] = {\n            x: posterior_stds.data_vars[x].to_numpy() for x in posterior_stds.data_vars\n        }\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the normative model to the training data.\n\n        This method implements the fitting logic for the normative model\n        based on the provided training data and model specification.\n\n        Args:\n            train_data: pd.DataFrame\n                DataFrame containing the training data. It must include the variable\n                of interest, their predicted moments, and all specified covariates.\n            save_directory: Path | None\n                A path to a directory to save the model. If provided, the fitted model\n                will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n            adapt: dict[str, Any] | None\n                If provided, adapt a pre-trained model to a new covariate.\n                Note: We recommended using the `adapt_fit` method, and not directly\n                changing this argument, unless you know what you are doing.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # Extract the variables of interest\n        variables_of_interest = train_data[\n            [self.spec.variable_of_interest_1, self.spec.variable_of_interest_2]\n        ].to_numpy()\n\n        # A dictionary to hold the model parameters after fitting\n        if adapt is None:\n            self.model_params = {}\n            self.model_params[\"mean_vois\"] = variables_of_interest.mean(axis=0)\n            self.model_params[\"std_vois\"] = variables_of_interest.std(axis=0)\n            self.model_params[\"sample_size\"] = variables_of_interest.shape[0]\n            # Initialize parameter count\n            self.model_params[\"n_params\"] = 0\n        else:\n            # Update the pretrained model parameters\n            if not hasattr(self, \"model_params\") or self.model_params is None:\n                self.model_params = copy.deepcopy(adapt[\"pretrained_model_params\"])\n            self.model_params[\"sample_size\"] += variables_of_interest.shape[0]\n\n        # Data preparation\n        # Combination weights\n        combination_weights = np.ones(shape=(train_data.shape[0], 2))\n\n        # Data coordinates\n        combination_indices = np.arange(train_data.shape[0])\n        model_coords = self._build_model_coordinates(\n            observations=combination_indices,\n        )\n\n        # Fitting logic\n        with pm.Model(coords=model_coords) as self._model:\n            # Standardize the variable of interest, and store mean and std\n            # This is done to ensure that the model is not sensitive to\n            # the scale of the variable\n            standardized_vois = (\n                variables_of_interest - self.model_params[\"mean_vois\"]\n            ) / self.model_params[\"std_vois\"]\n            variables_of_interest_mu_estimate = train_data[\n                [\n                    f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n                ]\n            ].to_numpy()\n            standardized_vois_mu_estimate = (\n                variables_of_interest_mu_estimate - self.model_params[\"mean_vois\"]\n            ) / self.model_params[\"std_vois\"]\n            variables_of_interest_std_estimate = train_data[\n                [\n                    f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_std_estimate\",\n                ]\n            ].to_numpy()\n            standardized_vois_std_estimate = (\n                variables_of_interest_std_estimate / self.model_params[\"std_vois\"]\n            )\n\n            # A dictionary for precomputed bspline basis functions\n            spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n            # A dictionary for factorized categories\n            category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n            # Model the covariance between the variables of interest\n            z_transformed_correlation_effects = self._model_all_correlation_effects(\n                train_data,\n                spline_bases,\n                category_indices,\n                adapt=adapt,\n            )\n\n            # Combine all covariance effects\n            self._combine_all_correlation_effects(\n                z_transformed_correlation_effects=z_transformed_correlation_effects,\n                combination_indices=combination_indices,\n                combination_weights=combination_weights,\n                standardized_vois=standardized_vois,\n                standardized_vois_mu_estimate=standardized_vois_mu_estimate,\n                standardized_vois_std_estimate=standardized_vois_std_estimate,\n            )\n\n            # Fit the model using ADVI\n            self._fit_model_with_advi(progress_bar=progress_bar)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(Path(save_directory))\n\n    def adapt_fit(\n        self,\n        covariate_to_adapt: str,\n        new_category_names: npt.NDArray[np.str_],\n        train_data: pd.DataFrame,\n        *,\n        pretrained_model_params: dict[str, Any] | None = None,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Using a previously fitted model, adapt the model to a new batch.\n        This method enables adaptation of the model to data from a new\n        batch/site by freezing all fitted parameters, and only estimating\n        new parameters for the new batch/site category.\n\n        Args:\n            covariate_to_adapt: str\n                Name of the categorical covariate representing the batch/site\n                to which the model should be adapted.\n                Note: This covariate must have been specified in the original\n                model.\n            new_category_names: list[str]\n                Names of the new categories in the covariate_to_adapt representing\n                the new batch/site labels (e.g. names of the new site).\n                Note: These names must not have been present in the original\n                fitted model.\n            train_data: pd.DataFrame\n                DataFrame containing the training data for adaptation.\n                It must include the variable of interest and all specified covariates.\n                Note: The covariate_to_adapt column must only contain the\n                new_category_names (no new data from previously trained batches).\n            pretrained_model_params: dict[str, Any] | None\n                The model parameters from a previously fitted model to adapt.\n                If None, the model parameters from the current instance will be used\n                (assuming fitting was done).\n            save_directory: Path | None\n                A path to a directory to save the adapted model. If provided,\n                the fitted model will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # Locate the covariate to adapt\n        cov_to_adapt_index = [cov.name for cov in self.spec.covariates].index(\n            covariate_to_adapt,\n        )\n\n        # Extend the covariate categories to include the new categories\n        self.spec.covariates[cov_to_adapt_index].extend_categories(new_category_names)\n\n        # Extract the pre-trained model parameters\n        if pretrained_model_params is None:\n            if not hasattr(self, \"model_params\") or self.model_params is None:\n                err = (\n                    \"No pretrained model parameters found. \"\n                    \"Please provide pretrained_model_params or fit the model first.\"\n                )\n                raise ValueError(err)\n            pretrained_model_params = copy.deepcopy(self.model_params)\n\n        # Fit the adapted model\n        self.fit(\n            train_data,\n            save_directory=save_directory,\n            progress_bar=progress_bar,\n            adapt={\n                \"covariate_to_adapt\": covariate_to_adapt,\n                \"new_category_names\": new_category_names,\n                \"pretrained_model_params\": pretrained_model_params,\n            },\n        )\n\n    def predict(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any] | None = None,\n        predict_without: list[str] | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict correlation for new data (from covariates) using the fitted model.\n\n        Args:\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n                Note: covariates listed in predict_without will be ignored and are\n                hence not required.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            predict_without: list[str] | None\n                Optional list of covariate names to ignore during prediction.\n                This can be used to check the effect of removing certain covariates\n                from the model.\n\n        Returns:\n            NormativePredictions: Object containing the predicted pairwise correlations\n                for the variables of interest.\n        \"\"\"\n        # Validate the new data\n        validation_columns = [\n            cov.name\n            for cov in self.spec.covariates\n            if cov.name not in (predict_without or [])\n        ]\n        utils.general.validate_dataframe(test_covariates, validation_columns)\n\n        # Parameters\n        model_params = model_params or self.model_params\n        if model_params is None:\n            err = \"No model parameters found. Please provide model_params.\"\n            raise ValueError(err)\n\n        # Posterior means\n        posterior_means = model_params[\"posterior_means\"]\n\n        # Calculate mean and variance effects\n        z_transformed_correlation_estimate = np.zeros(test_covariates.shape[0]) + float(\n            posterior_means[\"global_intercept_z\"],\n        )\n\n        for cov in self.spec.covariates:\n            if (cov.name in self.spec.influencing_covariance) and (\n                cov.name not in (predict_without or [])\n            ):\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is None:\n                            err = (\n                                f\"Covariate '{cov.name}' is missing moments for\"\n                                \" standardization.\"\n                            )\n                            raise ValueError(err)\n                        z_transformed_correlation_estimate += (\n                            (\n                                cast(\n                                    \"npt.NDArray[Any]\",\n                                    test_covariates[cov.name].to_numpy(),\n                                )\n                                - cov.moments[0]\n                            )\n                            / cov.moments[1]\n                        ) * posterior_means[f\"linear_beta_{cov.name}\"]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            cast(\n                                \"npt.NDArray[Any]\",\n                                test_covariates[cov.name].to_numpy(),\n                            ),\n                        )\n                        spline_betas = posterior_means[f\"spline_betas_{cov.name}\"]\n                        z_transformed_correlation_estimate += (\n                            spline_bases @ spline_betas\n                        )\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        cast(\"npt.NDArray[Any]\", test_covariates[cov.name].to_numpy()),\n                    )\n                    categorical_intercept = None\n                    if cov.hierarchical:\n                        categorical_intercept = (\n                            posterior_means[f\"intercept_offset_{cov.name}\"]\n                            * posterior_means[f\"sigma_intercept_{cov.name}\"]\n                        )\n                    else:\n                        categorical_intercept = posterior_means[f\"intercept_{cov.name}\"]\n                    z_transformed_correlation_estimate += categorical_intercept[\n                        category_indices\n                    ]\n\n        # Convert z-transformed score to correlation\n        correlation_estimate = np.tanh(z_transformed_correlation_estimate)\n\n        # Create a the predictions object and return\n        return NormativePredictions({\"correlation_estimate\": correlation_estimate})\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.adapt_fit","title":"<code>adapt_fit(covariate_to_adapt: str, new_category_names: npt.NDArray[np.str_], train_data: pd.DataFrame, *, pretrained_model_params: dict[str, Any] | None = None, save_directory: Path | None = None, progress_bar: bool = True) -&gt; None</code>","text":"<p>Using a previously fitted model, adapt the model to a new batch. This method enables adaptation of the model to data from a new batch/site by freezing all fitted parameters, and only estimating new parameters for the new batch/site category.</p> <p>Parameters:</p> Name Type Description Default <code>covariate_to_adapt</code> <code>str</code> <p>str Name of the categorical covariate representing the batch/site to which the model should be adapted. Note: This covariate must have been specified in the original model.</p> required <code>new_category_names</code> <code>NDArray[str_]</code> <p>list[str] Names of the new categories in the covariate_to_adapt representing the new batch/site labels (e.g. names of the new site). Note: These names must not have been present in the original fitted model.</p> required <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data for adaptation. It must include the variable of interest and all specified covariates. Note: The covariate_to_adapt column must only contain the new_category_names (no new data from previously trained batches).</p> required <code>pretrained_model_params</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None The model parameters from a previously fitted model to adapt. If None, the model parameters from the current instance will be used (assuming fitting was done).</p> <code>None</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the adapted model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def adapt_fit(\n    self,\n    covariate_to_adapt: str,\n    new_category_names: npt.NDArray[np.str_],\n    train_data: pd.DataFrame,\n    *,\n    pretrained_model_params: dict[str, Any] | None = None,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n) -&gt; None:\n    \"\"\"\n    Using a previously fitted model, adapt the model to a new batch.\n    This method enables adaptation of the model to data from a new\n    batch/site by freezing all fitted parameters, and only estimating\n    new parameters for the new batch/site category.\n\n    Args:\n        covariate_to_adapt: str\n            Name of the categorical covariate representing the batch/site\n            to which the model should be adapted.\n            Note: This covariate must have been specified in the original\n            model.\n        new_category_names: list[str]\n            Names of the new categories in the covariate_to_adapt representing\n            the new batch/site labels (e.g. names of the new site).\n            Note: These names must not have been present in the original\n            fitted model.\n        train_data: pd.DataFrame\n            DataFrame containing the training data for adaptation.\n            It must include the variable of interest and all specified covariates.\n            Note: The covariate_to_adapt column must only contain the\n            new_category_names (no new data from previously trained batches).\n        pretrained_model_params: dict[str, Any] | None\n            The model parameters from a previously fitted model to adapt.\n            If None, the model parameters from the current instance will be used\n            (assuming fitting was done).\n        save_directory: Path | None\n            A path to a directory to save the adapted model. If provided,\n            the fitted model will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # Locate the covariate to adapt\n    cov_to_adapt_index = [cov.name for cov in self.spec.covariates].index(\n        covariate_to_adapt,\n    )\n\n    # Extend the covariate categories to include the new categories\n    self.spec.covariates[cov_to_adapt_index].extend_categories(new_category_names)\n\n    # Extract the pre-trained model parameters\n    if pretrained_model_params is None:\n        if not hasattr(self, \"model_params\") or self.model_params is None:\n            err = (\n                \"No pretrained model parameters found. \"\n                \"Please provide pretrained_model_params or fit the model first.\"\n            )\n            raise ValueError(err)\n        pretrained_model_params = copy.deepcopy(self.model_params)\n\n    # Fit the adapted model\n    self.fit(\n        train_data,\n        save_directory=save_directory,\n        progress_bar=progress_bar,\n        adapt={\n            \"covariate_to_adapt\": covariate_to_adapt,\n            \"new_category_names\": new_category_names,\n            \"pretrained_model_params\": pretrained_model_params,\n        },\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.fit","title":"<code>fit(train_data: pd.DataFrame, *, save_directory: Path | None = None, progress_bar: bool = True, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Fit the normative model to the training data.</p> <p>This method implements the fitting logic for the normative model based on the provided training data and model specification.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data. It must include the variable of interest, their predicted moments, and all specified covariates.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None If provided, adapt a pre-trained model to a new covariate. Note: We recommended using the <code>adapt_fit</code> method, and not directly changing this argument, unless you know what you are doing.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    train_data: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Fit the normative model to the training data.\n\n    This method implements the fitting logic for the normative model\n    based on the provided training data and model specification.\n\n    Args:\n        train_data: pd.DataFrame\n            DataFrame containing the training data. It must include the variable\n            of interest, their predicted moments, and all specified covariates.\n        save_directory: Path | None\n            A path to a directory to save the model. If provided, the fitted model\n            will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n        adapt: dict[str, Any] | None\n            If provided, adapt a pre-trained model to a new covariate.\n            Note: We recommended using the `adapt_fit` method, and not directly\n            changing this argument, unless you know what you are doing.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # Extract the variables of interest\n    variables_of_interest = train_data[\n        [self.spec.variable_of_interest_1, self.spec.variable_of_interest_2]\n    ].to_numpy()\n\n    # A dictionary to hold the model parameters after fitting\n    if adapt is None:\n        self.model_params = {}\n        self.model_params[\"mean_vois\"] = variables_of_interest.mean(axis=0)\n        self.model_params[\"std_vois\"] = variables_of_interest.std(axis=0)\n        self.model_params[\"sample_size\"] = variables_of_interest.shape[0]\n        # Initialize parameter count\n        self.model_params[\"n_params\"] = 0\n    else:\n        # Update the pretrained model parameters\n        if not hasattr(self, \"model_params\") or self.model_params is None:\n            self.model_params = copy.deepcopy(adapt[\"pretrained_model_params\"])\n        self.model_params[\"sample_size\"] += variables_of_interest.shape[0]\n\n    # Data preparation\n    # Combination weights\n    combination_weights = np.ones(shape=(train_data.shape[0], 2))\n\n    # Data coordinates\n    combination_indices = np.arange(train_data.shape[0])\n    model_coords = self._build_model_coordinates(\n        observations=combination_indices,\n    )\n\n    # Fitting logic\n    with pm.Model(coords=model_coords) as self._model:\n        # Standardize the variable of interest, and store mean and std\n        # This is done to ensure that the model is not sensitive to\n        # the scale of the variable\n        standardized_vois = (\n            variables_of_interest - self.model_params[\"mean_vois\"]\n        ) / self.model_params[\"std_vois\"]\n        variables_of_interest_mu_estimate = train_data[\n            [\n                f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n            ]\n        ].to_numpy()\n        standardized_vois_mu_estimate = (\n            variables_of_interest_mu_estimate - self.model_params[\"mean_vois\"]\n        ) / self.model_params[\"std_vois\"]\n        variables_of_interest_std_estimate = train_data[\n            [\n                f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                f\"{self.spec.variable_of_interest_2}_std_estimate\",\n            ]\n        ].to_numpy()\n        standardized_vois_std_estimate = (\n            variables_of_interest_std_estimate / self.model_params[\"std_vois\"]\n        )\n\n        # A dictionary for precomputed bspline basis functions\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n        # A dictionary for factorized categories\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n        # Model the covariance between the variables of interest\n        z_transformed_correlation_effects = self._model_all_correlation_effects(\n            train_data,\n            spline_bases,\n            category_indices,\n            adapt=adapt,\n        )\n\n        # Combine all covariance effects\n        self._combine_all_correlation_effects(\n            z_transformed_correlation_effects=z_transformed_correlation_effects,\n            combination_indices=combination_indices,\n            combination_weights=combination_weights,\n            standardized_vois=standardized_vois,\n            standardized_vois_mu_estimate=standardized_vois_mu_estimate,\n            standardized_vois_std_estimate=standardized_vois_std_estimate,\n        )\n\n        # Fit the model using ADVI\n        self._fit_model_with_advi(progress_bar=progress_bar)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(Path(save_directory))\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.from_direct_model","title":"<code>from_direct_model(direct_model: DirectNormativeModel, variable_of_interest_1: str, variable_of_interest_2: str, influencing_covariance: list[str] | None = None, defaults_overwrite: dict[str, Any] | None = None) -&gt; CovarianceNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize the model from a direct model instance, and two variable names.</p> <p>Parameters:</p> Name Type Description Default <code>direct_model</code> <code>DirectNormativeModel</code> <p>DirectNormativeModel This model will be used to instantiate a similar covariance model.</p> required <code>variable_of_interest_1</code> <code>str</code> <p>str Name of the first target variable to model.</p> required <code>variable_of_interest_2</code> <code>str</code> <p>str Name of the second target variable to model.</p> required <code>influencing_covariance</code> <code>list[str] | None</code> <p>list[str] | None List of covariates that influence the covariance structure. If not provided, this will be copied from the direct model's <code>influencing_variance</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>CovarianceNormativeModel</code> <p>CovarianceNormativeModel An instance of CovarianceNormativeModel initialized with the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef from_direct_model(\n    cls,\n    direct_model: DirectNormativeModel,\n    variable_of_interest_1: str,\n    variable_of_interest_2: str,\n    influencing_covariance: list[str] | None = None,\n    defaults_overwrite: dict[str, Any] | None = None,\n) -&gt; CovarianceNormativeModel:\n    \"\"\"\n    Initialize the model from a direct model instance, and two variable names.\n\n    Args:\n        direct_model: DirectNormativeModel\n            This model will be used to instantiate a similar covariance model.\n        variable_of_interest_1: str\n            Name of the first target variable to model.\n        variable_of_interest_2: str\n            Name of the second target variable to model.\n        influencing_covariance: list[str] | None\n            List of covariates that influence the covariance structure. If not\n            provided, this will be copied from the direct model's\n            `influencing_variance`.\n\n    Returns:\n        CovarianceNormativeModel\n            An instance of CovarianceNormativeModel initialized with the provided\n            data.\n    \"\"\"\n    # Validity checks for input parameters\n    if not isinstance(direct_model, DirectNormativeModel):\n        err = \"direct_model must be an instance of DirectNormativeModel.\"\n        raise TypeError(err)\n    if not (\n        isinstance(variable_of_interest_1, str)\n        and isinstance(variable_of_interest_2, str)\n    ):\n        err = \"Variables of interest must be strings.\"\n        raise TypeError(err)\n\n    # Substitute influencing_covariance if not provided\n    if influencing_covariance is None:\n        influencing_covariance = direct_model.spec.influencing_variance\n\n    # Use the same setup as the direct model\n    model = cls(\n        spec=CovarianceModelSpec(\n            variable_of_interest_1=variable_of_interest_1,\n            variable_of_interest_2=variable_of_interest_2,\n            covariates=direct_model.spec.covariates,\n            influencing_covariance=influencing_covariance,\n        ),\n    )\n\n    # update defaults\n    model.defaults.update(direct_model.defaults)\n    model.defaults.update(defaults_overwrite or {})\n\n    return model\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.load_model","title":"<code>load_model(directory: Path, *, load_posterior: bool = False) -&gt; CovarianceNormativeModel</code>  <code>classmethod</code>","text":"<p>Load the model and its posterior from a directory. The model will be loaded from a subdirectory named 'saved_model'.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to the directory containing the model.</p> required <code>load_posterior</code> <code>bool</code> <p>bool (default=False) If True, load the model's posterior trace from the saved inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    directory: Path,\n    *,\n    load_posterior: bool = False,\n) -&gt; CovarianceNormativeModel:\n    \"\"\"\n    Load the model and its posterior from a directory.\n    The model will be loaded from a subdirectory named 'saved_model'.\n\n    Args:\n        directory: Path\n            Path to the directory containing the model.\n        load_posterior: bool (default=False)\n            If True, load the model's posterior trace from the saved inference data.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"saved_model\",\n    )\n\n    # Load the saved model dict\n    model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n    # Create an instance of the class\n    instance = cls(\n        spec=model_dict[\"spec\"],\n    )\n\n    # Set the attributes from the loaded model dictionary\n    instance.defaults.update(model_dict[\"defaults\"])\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n        if load_posterior:\n            instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n\n    return instance\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.predict","title":"<code>predict(test_covariates: pd.DataFrame, model_params: dict[str, Any] | None = None, predict_without: list[str] | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict correlation for new data (from covariates) using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates. Note: covariates listed in predict_without will be ignored and are hence not required.</p> required <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>predict_without</code> <code>list[str] | None</code> <p>list[str] | None Optional list of covariate names to ignore during prediction. This can be used to check the effect of removing certain covariates from the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted pairwise correlations for the variables of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    test_covariates: pd.DataFrame,\n    model_params: dict[str, Any] | None = None,\n    predict_without: list[str] | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict correlation for new data (from covariates) using the fitted model.\n\n    Args:\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n            Note: covariates listed in predict_without will be ignored and are\n            hence not required.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        predict_without: list[str] | None\n            Optional list of covariate names to ignore during prediction.\n            This can be used to check the effect of removing certain covariates\n            from the model.\n\n    Returns:\n        NormativePredictions: Object containing the predicted pairwise correlations\n            for the variables of interest.\n    \"\"\"\n    # Validate the new data\n    validation_columns = [\n        cov.name\n        for cov in self.spec.covariates\n        if cov.name not in (predict_without or [])\n    ]\n    utils.general.validate_dataframe(test_covariates, validation_columns)\n\n    # Parameters\n    model_params = model_params or self.model_params\n    if model_params is None:\n        err = \"No model parameters found. Please provide model_params.\"\n        raise ValueError(err)\n\n    # Posterior means\n    posterior_means = model_params[\"posterior_means\"]\n\n    # Calculate mean and variance effects\n    z_transformed_correlation_estimate = np.zeros(test_covariates.shape[0]) + float(\n        posterior_means[\"global_intercept_z\"],\n    )\n\n    for cov in self.spec.covariates:\n        if (cov.name in self.spec.influencing_covariance) and (\n            cov.name not in (predict_without or [])\n        ):\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"linear\":\n                    if cov.moments is None:\n                        err = (\n                            f\"Covariate '{cov.name}' is missing moments for\"\n                            \" standardization.\"\n                        )\n                        raise ValueError(err)\n                    z_transformed_correlation_estimate += (\n                        (\n                            cast(\n                                \"npt.NDArray[Any]\",\n                                test_covariates[cov.name].to_numpy(),\n                            )\n                            - cov.moments[0]\n                        )\n                        / cov.moments[1]\n                    ) * posterior_means[f\"linear_beta_{cov.name}\"]\n                elif cov.effect == \"spline\":\n                    spline_bases = cov.make_spline_bases(\n                        cast(\n                            \"npt.NDArray[Any]\",\n                            test_covariates[cov.name].to_numpy(),\n                        ),\n                    )\n                    spline_betas = posterior_means[f\"spline_betas_{cov.name}\"]\n                    z_transformed_correlation_estimate += (\n                        spline_bases @ spline_betas\n                    )\n            elif cov.cov_type == \"categorical\":\n                category_indices = cov.factorize_categories(\n                    cast(\"npt.NDArray[Any]\", test_covariates[cov.name].to_numpy()),\n                )\n                categorical_intercept = None\n                if cov.hierarchical:\n                    categorical_intercept = (\n                        posterior_means[f\"intercept_offset_{cov.name}\"]\n                        * posterior_means[f\"sigma_intercept_{cov.name}\"]\n                    )\n                else:\n                    categorical_intercept = posterior_means[f\"intercept_{cov.name}\"]\n                z_transformed_correlation_estimate += categorical_intercept[\n                    category_indices\n                ]\n\n    # Convert z-transformed score to correlation\n    correlation_estimate = np.tanh(z_transformed_correlation_estimate)\n\n    # Create a the predictions object and return\n    return NormativePredictions({\"correlation_estimate\": correlation_estimate})\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovarianceNormativeModel.save_model","title":"<code>save_model(directory: Path, *, save_posterior: bool = False) -&gt; None</code>","text":"<p>Save the fitted model and it's posterior to a directory. The model will be saved in a subdirectory named 'saved_model'. If this directory is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to a directory to save the model.</p> required <code>save_posterior</code> <code>bool</code> <p>bool (default=False) If True, save the model's posterior trace inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n    \"\"\"\n    Save the fitted model and it's posterior to a directory.\n    The model will be saved in a subdirectory named 'saved_model'.\n    If this directory is not empty, an error is raised.\n\n    Args:\n        directory: Path\n            Path to a directory to save the model.\n        save_posterior: bool (default=False)\n            If True, save the model's posterior trace inference data.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n    model_dict = {\n        \"spec\": self.spec,\n        \"defaults\": self.defaults,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n        if hasattr(self, \"model_inference_data\") and save_posterior:\n            self.model_inference_data.to_netcdf(\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n    joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovariateSpec","title":"<code>CovariateSpec</code>  <code>dataclass</code>","text":"<p>Specification of a single covariate and how it should be modeled.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>str Name of the covariate (e.g., 'age', 'site').</p> <code>cov_type</code> <code>CovariateType</code> <p>str Type of the covariate ('numerical' or 'categorical').</p> <code>effect</code> <code>NumericalEffect | None</code> <p>str For numerical covariates, how the effect is modeled ('linear' or 'spline').</p> <code>categories</code> <code>NDArray[str_] | None</code> <p>np.ndarray | None For categorical covariates, the category labels stored as a NumPy array.</p> <code>hierarchical</code> <code>bool | None</code> <p>bool For categorical covariates, whether to model with a hierarchical structure.</p> <code>spline_spec</code> <code>SplineSpec | None</code> <p>SplineSpec | None Optional SplineSpec instance for spline modeling; required if effect is 'spline'.</p> Validation <ul> <li>Numerical covariates must specify 'effect'.</li> <li>If 'effect' is 'spline', 'spline_spec' must be provided.</li> <li>Categorical covariates must specify 'hierarchical'.</li> <li>Categorical covariates cannot have 'effect' or 'spline_spec'.</li> <li>Categorical covariates must have categories listed.</li> </ul> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovariateSpec:\n    \"\"\"\n    Specification of a single covariate and how it should be modeled.\n\n    Attributes:\n        name: str\n            Name of the covariate (e.g., 'age', 'site').\n        cov_type: str\n            Type of the covariate ('numerical' or 'categorical').\n        effect: str\n            For numerical covariates, how the effect is modeled ('linear'\n            or 'spline').\n        categories: np.ndarray | None\n            For categorical covariates, the category labels stored as a NumPy array.\n        hierarchical: bool\n            For categorical covariates, whether to model with a\n            hierarchical structure.\n        spline_spec: SplineSpec | None\n            Optional SplineSpec instance for spline modeling;\n            required if effect is 'spline'.\n\n    Validation:\n        - Numerical covariates must specify 'effect'.\n        - If 'effect' is 'spline', 'spline_spec' must be provided.\n        - Categorical covariates must specify 'hierarchical'.\n        - Categorical covariates cannot have 'effect' or 'spline_spec'.\n        - Categorical covariates must have categories listed.\n    \"\"\"\n\n    name: str\n    cov_type: CovariateType  # \"categorical\" or \"numerical\"\n    effect: NumericalEffect | None = None  # Only if numerical\n    categories: npt.NDArray[np.str_] | None = None  # Only if categorical\n    hierarchical: bool | None = None  # Only if categorical\n    spline_spec: SplineSpec | None = None  # Only for spline modeling\n    moments: tuple[float, float] | None = None  # Only for linear effects\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the CovariateSpec instance.\n        \"\"\"\n        representation = f\"CovariateSpec(name={self.name}, cov_type={self.cov_type}\"\n        if self.cov_type == \"numerical\":\n            representation += f\", effect={self.effect}\"\n        elif self.cov_type == \"categorical\":\n            representation += f\", hierarchical={self.hierarchical}\"\n            if self.categories is not None:\n                representation += f\", n_categories={len(self.categories.tolist())}\"\n        representation += \")\"\n        return representation\n\n    # Validation checks for the covariate specification.\n    def validate_numerical(self) -&gt; None:\n        if self.effect not in {\"linear\", \"spline\"}:\n            err = (\n                f\"Numerical covariate '{self.name}' must specify effect as \"\n                \"'linear' or 'spline'.\"\n            )\n            raise ValueError(err)\n        if self.hierarchical is not None:\n            err = (\n                f\"Numerical covariate '{self.name}' should not specify 'hierarchical'.\"\n            )\n            raise ValueError(err)\n        if self.categories is not None:\n            err = f\"Numerical covariate '{self.name}' should not specify 'categories'.\"\n            raise ValueError(err)\n        if self.effect == \"spline\":\n            if self.spline_spec is None:\n                err = (\n                    f\"Numerical covariate '{self.name}' must have spline \"\n                    \"specification if effect is 'spline'.\"\n                )\n                raise ValueError(err)\n            if self.moments is not None:\n                err = (\n                    f\"Numerical covariate '{self.name}' should not specify \"\n                    \"moments if effect is 'spline'.\"\n                )\n                raise ValueError(err)\n        if self.effect == \"linear\":\n            if self.spline_spec is not None:\n                err = (\n                    f\"Numerical covariate '{self.name}' should not have spline \"\n                    \"specification unless effect is 'spline'.\"\n                )\n                raise ValueError(err)\n            if self.moments is None:\n                err = (\n                    f\"Numerical covariate '{self.name}' must specify moments \"\n                    \"(mean and standard deviation) for linear effects.\"\n                )\n                raise ValueError(err)\n\n    def validate_categorical(self) -&gt; None:\n        if self.effect is not None:\n            err = (\n                f\"Categorical covariate '{self.name}' should not have a \"\n                \"numerical effect type.\"\n            )\n            raise ValueError(err)\n        if self.spline_spec is not None:\n            err = (\n                f\"Categorical covariate '{self.name}' should not have spline \"\n                \"specification.\"\n            )\n            raise ValueError(err)\n        if self.hierarchical is None:\n            err = (\n                f\"Categorical covariate '{self.name}' must specify whether \"\n                \"it is hierarchical.\"\n            )\n            raise ValueError(err)\n        if self.categories is None:\n            err = f\"Categorical covariate '{self.name}' must specify categories.\"\n            raise ValueError(err)\n        if not isinstance(self.categories, np.ndarray):\n            err = (\n                f\"Categorical covariate '{self.name}' must specify categories \"\n                \"as a NumPy array.\"\n            )\n            raise TypeError(err)\n\n    def __post_init__(self) -&gt; None:\n        if self.cov_type == \"numerical\":\n            self.validate_numerical()\n        elif self.cov_type == \"categorical\":\n            self.validate_categorical()\n        else:\n            err = f\"Invalid covariate type '{self.cov_type}' for '{self.name}'.\"\n            raise ValueError(err)\n\n    def make_spline_bases(\n        self,\n        values: npt.NDArray[np.floating[Any]],\n        *,\n        include_intercept: bool = True,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Create B-spline basis expansion functions for a given covariate.\n\n        Args:\n            values (np.ndarray): The values to create the spline basis functions for.\n\n        Returns:\n            np.ndarray: The B-spline basis function expansion.\n        \"\"\"\n        if self.effect != \"spline\" or self.spline_spec is None:\n            err = f\"Covariate '{self.name}' is not a spline covariate.\"\n            raise ValueError(err)\n\n        # Create fixed set of knots if not already defined\n        if self.spline_spec.knots is None:\n            knots = np.linspace(\n                self.spline_spec.lower_bound,\n                self.spline_spec.upper_bound,\n                self.spline_spec.df - self.spline_spec.degree + 1,\n            )[1:-1].tolist()\n        else:\n            knots = self.spline_spec.knots\n\n        # Create B-spline basis functions\n        return np.array(\n            patsy.bs(  # pyright: ignore[reportAttributeAccessIssue]\n                values,\n                knots=knots,\n                df=self.spline_spec.df,\n                degree=self.spline_spec.degree,\n                lower_bound=self.spline_spec.lower_bound,\n                upper_bound=self.spline_spec.upper_bound,\n                include_intercept=include_intercept,\n            ),\n        )\n\n    def factorize_categories(\n        self,\n        values: npt.NDArray[np.str_],\n    ) -&gt; npt.NDArray[np.int_]:\n        \"\"\"\n        Factorize categorical covariate values into numerical indices.\n\n        Args:\n            values (np.ndarray): The values to factorize.\n\n        Returns:\n            np.ndarray: The factorized numerical indices for the categories.\n        \"\"\"\n        if self.cov_type != \"categorical\":\n            err = (\n                f\"Covariate '{self.name}' is not a categorical \"\n                \"covariate to be factorized.\"\n            )\n            raise ValueError(err)\n\n        # Create a mapping from category values to indices\n        if self.categories is None:  # to satisfy type checker\n            err = f\"Covariate '{self.name}' does not have categories defined.\"\n            raise ValueError(err)\n        category_mapping: dict[str, int] = {\n            category: idx for idx, category in enumerate(self.categories)\n        }\n        # Factorize the values using the mapping\n        return np.array([category_mapping[val] for val in values], dtype=int)\n\n    def extend_categories(\n        self,\n        new_categories: npt.NDArray[np.str_],\n    ) -&gt; None:\n        \"\"\"\n        Extend the categories of a categorical covariate with new categories.\n\n        Args:\n            new_categories (np.ndarray): The new categories to add.\n\n        Returns:\n            None\n        \"\"\"\n        if self.cov_type != \"categorical\":\n            err = f\"Covariate '{self.name}' is not a categorical covariate to extend.\"\n            raise ValueError(err)\n        if self.categories is None:  # to satisfy type checker\n            err = f\"Covariate '{self.name}' does not have categories defined.\"\n            raise ValueError(err)\n\n        # Make sure categories are unique and new\n        unique_new_categories = np.setdiff1d(new_categories, self.categories)\n        if unique_new_categories.size &lt; new_categories.size:\n            err = (\n                f\"Some new categories are already present in the \"\n                f\"covariate '{self.name}'.\"\n            )\n            raise ValueError(err)\n\n        # Extend the categories array\n        self.categories = np.concatenate((self.categories, unique_new_categories))\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovariateSpec.extend_categories","title":"<code>extend_categories(new_categories: npt.NDArray[np.str_]) -&gt; None</code>","text":"<p>Extend the categories of a categorical covariate with new categories.</p> <p>Parameters:</p> Name Type Description Default <code>new_categories</code> <code>ndarray</code> <p>The new categories to add.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def extend_categories(\n    self,\n    new_categories: npt.NDArray[np.str_],\n) -&gt; None:\n    \"\"\"\n    Extend the categories of a categorical covariate with new categories.\n\n    Args:\n        new_categories (np.ndarray): The new categories to add.\n\n    Returns:\n        None\n    \"\"\"\n    if self.cov_type != \"categorical\":\n        err = f\"Covariate '{self.name}' is not a categorical covariate to extend.\"\n        raise ValueError(err)\n    if self.categories is None:  # to satisfy type checker\n        err = f\"Covariate '{self.name}' does not have categories defined.\"\n        raise ValueError(err)\n\n    # Make sure categories are unique and new\n    unique_new_categories = np.setdiff1d(new_categories, self.categories)\n    if unique_new_categories.size &lt; new_categories.size:\n        err = (\n            f\"Some new categories are already present in the \"\n            f\"covariate '{self.name}'.\"\n        )\n        raise ValueError(err)\n\n    # Extend the categories array\n    self.categories = np.concatenate((self.categories, unique_new_categories))\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovariateSpec.factorize_categories","title":"<code>factorize_categories(values: npt.NDArray[np.str_]) -&gt; npt.NDArray[np.int_]</code>","text":"<p>Factorize categorical covariate values into numerical indices.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>The values to factorize.</p> required <p>Returns:</p> Type Description <code>NDArray[int_]</code> <p>np.ndarray: The factorized numerical indices for the categories.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def factorize_categories(\n    self,\n    values: npt.NDArray[np.str_],\n) -&gt; npt.NDArray[np.int_]:\n    \"\"\"\n    Factorize categorical covariate values into numerical indices.\n\n    Args:\n        values (np.ndarray): The values to factorize.\n\n    Returns:\n        np.ndarray: The factorized numerical indices for the categories.\n    \"\"\"\n    if self.cov_type != \"categorical\":\n        err = (\n            f\"Covariate '{self.name}' is not a categorical \"\n            \"covariate to be factorized.\"\n        )\n        raise ValueError(err)\n\n    # Create a mapping from category values to indices\n    if self.categories is None:  # to satisfy type checker\n        err = f\"Covariate '{self.name}' does not have categories defined.\"\n        raise ValueError(err)\n    category_mapping: dict[str, int] = {\n        category: idx for idx, category in enumerate(self.categories)\n    }\n    # Factorize the values using the mapping\n    return np.array([category_mapping[val] for val in values], dtype=int)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.CovariateSpec.make_spline_bases","title":"<code>make_spline_bases(values: npt.NDArray[np.floating[Any]], *, include_intercept: bool = True) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Create B-spline basis expansion functions for a given covariate.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>The values to create the spline basis functions for.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray: The B-spline basis function expansion.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def make_spline_bases(\n    self,\n    values: npt.NDArray[np.floating[Any]],\n    *,\n    include_intercept: bool = True,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Create B-spline basis expansion functions for a given covariate.\n\n    Args:\n        values (np.ndarray): The values to create the spline basis functions for.\n\n    Returns:\n        np.ndarray: The B-spline basis function expansion.\n    \"\"\"\n    if self.effect != \"spline\" or self.spline_spec is None:\n        err = f\"Covariate '{self.name}' is not a spline covariate.\"\n        raise ValueError(err)\n\n    # Create fixed set of knots if not already defined\n    if self.spline_spec.knots is None:\n        knots = np.linspace(\n            self.spline_spec.lower_bound,\n            self.spline_spec.upper_bound,\n            self.spline_spec.df - self.spline_spec.degree + 1,\n        )[1:-1].tolist()\n    else:\n        knots = self.spline_spec.knots\n\n    # Create B-spline basis functions\n    return np.array(\n        patsy.bs(  # pyright: ignore[reportAttributeAccessIssue]\n            values,\n            knots=knots,\n            df=self.spline_spec.df,\n            degree=self.spline_spec.degree,\n            lower_bound=self.spline_spec.lower_bound,\n            upper_bound=self.spline_spec.upper_bound,\n            include_intercept=include_intercept,\n        ),\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel","title":"<code>DirectNormativeModel</code>  <code>dataclass</code>","text":"<p>Direct normative model implementation.</p> <p>This class implements the direct normative modeling approach, which directly models the variable of interest using the specified covariates. It can be used to fit a model to data and predict normative centiles.</p> <p>Attributes:</p> Name Type Description <code>spec</code> <code>NormativeModelSpec</code> <p>NormativeModelSpec Specification of the normative model including variable of interest, covariates, and data source.</p> <code>defaults</code> <code>dict[str, Any]</code> <p>dict Default parameters for the model, including spline specifications, ADVI iterations, convergence tolerance, random seed, and Adam optimizer learning rates.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass DirectNormativeModel:\n    \"\"\"\n    Direct normative model implementation.\n\n    This class implements the direct normative modeling approach, which\n    directly models the variable of interest using the specified covariates.\n    It can be used to fit a model to data and predict normative centiles.\n\n    Attributes:\n        spec: NormativeModelSpec\n            Specification of the normative model including variable of interest,\n            covariates, and data source.\n        defaults: dict\n            Default parameters for the model, including spline specifications,\n            ADVI iterations, convergence tolerance, random seed, and Adam optimizer\n            learning rates.\n    \"\"\"\n\n    spec: NormativeModelSpec\n    defaults: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"spline_df\": DEFAULT_SPLINE_DF,\n            \"spline_degree\": DEFAULT_SPLINE_DEGREE,\n            \"spline_extrapolation_factor\": DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n            \"advi_iterations\": DEFAULT_ADVI_ITERATIONS,\n            \"advi_convergence_tolerance\": DEFAULT_ADVI_CONVERGENCE_TOLERANCE,\n            \"random_seed\": DEFAULT_RANDOM_SEED,\n            \"adam_learning_rate\": DEFAULT_ADAM_LEARNING_RATE,\n            \"adam_learning_rate_decay\": DEFAULT_ADAM_LEARNING_RATE_DECAY,\n        },\n    )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the DirectNormativeModel instance.\n        \"\"\"\n        return f\"DirectNormativeModel(spec={self.spec})\"\n\n    @staticmethod\n    def _validate_init_args(\n        model_type: ModelType,\n        variable_of_interest: str,\n        numerical_covariates: list[str],\n        categorical_covariates: list[str],\n        batch_covariates: list[str],\n        nonlinear_covariates: list[str],\n    ) -&gt; None:\n        # Validity checks for input parameters\n        if model_type not in {\"HBR\", \"BLR\"}:\n            err = f\"Invalid model type '{model_type}'. Must be 'HBR' or 'BLR'.\"\n            raise ValueError(err)\n        for list_name, covariate_list in [\n            (\"numerical\", numerical_covariates),\n            (\"categorical\", categorical_covariates),\n            (\"batch\", batch_covariates),\n            (\"nonlinear\", nonlinear_covariates),\n        ]:\n            if not all(isinstance(item, str) for item in covariate_list):\n                err = f\"All covariate names must be strings: {list_name} covariates.\"\n                raise TypeError(err)\n        if not isinstance(variable_of_interest, str):\n            err = \"Variable of interest must be a string.\"\n            raise TypeError(err)\n        if not all(col in categorical_covariates for col in batch_covariates):\n            err = \"All batch covariates must be included in categorical covariates.\"\n            raise ValueError(err)\n        if not all(col in numerical_covariates for col in nonlinear_covariates):\n            err = \"All nonlinear covariates must be included in numerical covariates.\"\n            raise ValueError(err)\n\n    @classmethod\n    def from_dataframe(\n        cls,\n        model_type: ModelType,\n        dataframe: pd.DataFrame,\n        variable_of_interest: str,\n        numerical_covariates: list[str] | None = None,\n        categorical_covariates: list[str] | None = None,\n        batch_covariates: list[str] | None = None,\n        nonlinear_covariates: list[str] | None = None,\n        influencing_mean: list[str] | None = None,\n        influencing_variance: list[str] | None = None,\n        spline_kwargs: dict[str, Any] | None = None,\n    ) -&gt; DirectNormativeModel:\n        \"\"\"\n        Initialize a normative model from a pandas DataFrame.\n\n        Args:\n            model_type: ModelType\n                Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n                Regression) or \"BLR\" (Bayesian Linear Regression).\n            dataframe: pd.DataFrame\n                DataFrame containing the data.\n            variable_of_interest: str\n                Name of the target variable to model.\n            numerical_covariates: list[str] | None\n                List of numerical covariate names.\n            categorical_covariates: list[str] | None\n                List of categorical covariate names.\n            batch_covariates: list[str] | None\n                List of batch covariate names which should also be included in\n                categorical_covariates.\n            nonlinear_covariates: list[str] | None\n                List of covariate names to be modeled as nonlinear effects.\n                These should also be included in numerical_covariates.\n            influencing_mean: list[str] | None\n                List of covariate names that influence the mean of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            influencing_variance: list[str] | None\n                List of covariate names that influence the variance of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            spline_kwargs: dict\n                Additional keyword arguments for spline specification, such as\n                `df`, `degree`, and `knots`. These are passed to the\n                `create_spline_spec` method to create spline specifications for\n                nonlinear covariates.\n\n        Returns:\n            DirectNormativeModel\n                An instance of DirectNormativeModel initialized with the provided data.\n        \"\"\"\n        # Set default values for optional parameters\n        numerical_covariates = numerical_covariates or []\n        categorical_covariates = categorical_covariates or []\n        batch_covariates = batch_covariates or []\n        nonlinear_covariates = nonlinear_covariates or []\n        influencing_mean = influencing_mean or []\n        influencing_variance = influencing_variance or []\n        spline_kwargs = spline_kwargs or {}\n\n        # Validity checks for input parameters\n        cls._validate_init_args(\n            model_type,\n            variable_of_interest,\n            numerical_covariates,\n            categorical_covariates,\n            batch_covariates,\n            nonlinear_covariates,\n        )\n        utils.general.validate_dataframe(\n            dataframe,\n            [variable_of_interest, *numerical_covariates, *categorical_covariates],\n        )\n\n        # Create an instance of the class\n        self = cls(\n            spec=NormativeModelSpec(\n                variable_of_interest=variable_of_interest,\n                covariates=[],\n                influencing_mean=influencing_mean,\n                influencing_variance=influencing_variance,\n            ),\n        )\n\n        # Populate the spline_kwargs with defaults if not provided\n        spline_kwargs[\"df\"] = spline_kwargs.get(\"df\", self.defaults[\"spline_df\"])\n        spline_kwargs[\"degree\"] = spline_kwargs.get(\n            \"degree\",\n            self.defaults[\"spline_degree\"],\n        )\n        spline_kwargs[\"extrapolation_factor\"] = spline_kwargs.get(\n            \"extrapolation_factor\",\n            self.defaults[\"spline_extrapolation_factor\"],\n        )\n\n        # Start building the model specification\n        # Add categorical covariates\n        for cov_name in categorical_covariates:\n            hierarchical = False\n            if cov_name in batch_covariates and model_type == \"HBR\":\n                hierarchical = True\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"categorical\",\n                    categories=dataframe[cov_name].unique(),\n                    hierarchical=hierarchical,\n                ),\n            )\n        for cov_name in numerical_covariates:\n            if cov_name not in nonlinear_covariates:\n                self.spec.covariates.append(\n                    CovariateSpec(\n                        name=cov_name,\n                        cov_type=\"numerical\",\n                        effect=\"linear\",\n                        moments=(\n                            dataframe[cov_name].mean(),\n                            dataframe[cov_name].std(),\n                        ),\n                    ),\n                )\n            else:\n                self.spec.covariates.append(\n                    CovariateSpec(\n                        name=cov_name,\n                        cov_type=\"numerical\",\n                        effect=\"spline\",\n                        spline_spec=SplineSpec.create_spline_spec(\n                            dataframe[cov_name],\n                            **spline_kwargs,\n                        ),\n                    ),\n                )\n        return self\n\n    def _validate_model(self) -&gt; None:\n        \"\"\"\n        Validate the model instance.\n\n        This method checks if the model instance is complete and valid.\n        It raises errors if any required fields are missing or if there are\n        inconsistencies in the model specification.\n        \"\"\"\n        if self.spec is None:\n            err = (\n                \"Model specification is not set. \"\n                \"Please initialize the model, e.g., with 'from_dataframe'.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.covariates) == 0:\n            err = (\n                \"No covariates specified in the model. \"\n                \"Please add covariates to the specification.\"\n            )\n            raise ValueError(err)\n        if (len(self.spec.influencing_mean) == 0) and (\n            len(self.spec.influencing_variance) == 0\n        ):\n            err = (\n                \"No covariates specified to influence the mean or \"\n                \"variance of the variable of interest.\"\n            )\n            raise ValueError(err)\n\n    def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n        \"\"\"\n        Save the fitted model and it's posterior to a directory.\n        The model will be saved in a subdirectory named 'saved_model'.\n        If this directory is not empty, an error is raised.\n\n        Args:\n            directory: Path\n                Path to a directory to save the model.\n            save_posterior: bool (default=False)\n                If True, save the model's posterior trace inference data.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n        model_dict = {\n            \"spec\": self.spec,\n            \"defaults\": self.defaults,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n            if hasattr(self, \"model_inference_data\") and save_posterior:\n                self.model_inference_data.to_netcdf(\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n        joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n\n    @classmethod\n    def load_model(\n        cls,\n        directory: Path,\n        *,\n        load_posterior: bool = False,\n    ) -&gt; DirectNormativeModel:\n        \"\"\"\n        Load the model and its posterior from a directory.\n        The model will be loaded from a subdirectory named 'saved_model'.\n\n        Args:\n            directory: Path\n                Path to the directory containing the model.\n            load_posterior: bool (default=False)\n                If True, load the model's posterior trace from the saved inference data.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"saved_model\",\n        )\n\n        # Load the saved model dict\n        model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n        # Create an instance of the class\n        instance = cls(\n            spec=model_dict[\"spec\"],\n        )\n\n        # Set the attributes from the loaded model dictionary\n        instance.defaults.update(model_dict[\"defaults\"])\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n            if load_posterior:\n                instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n\n        return instance\n\n    def _validate_dataframe_for_fitting(self, train_data: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Validate the training DataFrame for fitting.\n        \"\"\"\n        utils.general.validate_dataframe(\n            train_data,\n            (\n                [cov.name for cov in self.spec.covariates]\n                + [self.spec.variable_of_interest]\n            ),\n        )\n\n    def _build_model_coordinates(\n        self,\n        observations: npt.NDArray[np.integer[Any]],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Build the model coordinates for the training DataFrame.\n        \"\"\"\n        # Data coordinates\n        model_coords = {\"observations\": observations, \"scalar\": [0]}\n\n        # Additional coordinates for covariates\n        for cov in self.spec.covariates:\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"spline\":\n                    if cov.spline_spec is not None:  # to satisfy type checker\n                        model_coords[f\"{cov.name}_splines\"] = np.arange(\n                            cov.spline_spec.df,\n                        )\n                elif cov.effect == \"linear\":\n                    model_coords[f\"{cov.name}_linear\"] = np.arange(1)\n            elif cov.cov_type == \"categorical\":\n                model_coords[cov.name] = cov.categories\n            else:\n                err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                raise ValueError(err)\n        return model_coords\n\n    def _model_linear_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        sigma_prior: float = 10,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate on the mean estimate.\n        \"\"\"\n        # Linear effect\n        if adapt is None:  # Model fitting\n            linear_beta = pm.Normal(\n                f\"linear_beta_{cov.name}\",\n                mu=0,\n                sigma=sigma_prior,\n                size=1,\n                dims=(f\"{cov.name}_linear\",),\n            )\n            # Increment parameter count for linear effect\n            self.model_params[\"n_params\"] += 1\n        else:  # Freeze during adaptation/fine-tuning\n            linear_beta = pm.Deterministic(\n                f\"linear_beta_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"linear_beta_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_linear\",),\n            )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                (\n                    cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy())\n                    - cov.moments[0]\n                )\n                / cov.moments[1]\n                * linear_beta,\n            )\n\n    def _model_spline_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 10,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate on the mean estimate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            # ZeroSumNormal imposes a centering constraint ensuring identifiability\n            spline_betas = pm.ZeroSumNormal(\n                f\"spline_betas_{cov.name}\",\n                sigma=sigma_prior,\n                shape=spline_bases[cov.name].shape[1],\n                dims=(f\"{cov.name}_splines\",),\n            )\n            # Increment parameter count for spline effects\n            if cov.spline_spec is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n        else:  # Freeze during adaptation/fine-tuning\n            spline_betas = pm.Deterministic(\n                f\"spline_betas_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"spline_betas_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_splines\",),\n            )\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))\n\n    def _model_categorical_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 10,\n        hierarchical_sigma_prior: float = 1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate on the mean estimate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.HalfNormal(\n                    f\"sigma_intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(\"scalar\",),\n                )\n\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.ZeroSumNormal(\n                    f\"intercept_offset_{cov.name}\",\n                    sigma=hierarchical_sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n\n                # Increment parameter count for hierarchical intercept\n                self.model_params[\"n_params\"] += 1\n\n            else:\n                # Non-hierarchical (linear) categorical effect\n                categorical_intercept = pm.ZeroSumNormal(\n                    f\"intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n            # Increment parameter count for categorical effects\n            if cov.categories is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += len(cov.categories) - 1\n        elif cov.name != adapt[\"covariate_to_adapt\"]:\n            # Freeze during adaptation/fine-tuning\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.Deterministic(\n                    f\"sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n        else:  # Partial freezing (fit parameters for the new site only)\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                # Hyperpriors are fixed during adaptation\n                sigma_intercept_category = pm.Deterministic(\n                    f\"sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                pretrain_sigma_prior = adapt[\"pretrained_model_params\"][\n                    \"posterior_means\"\n                ][f\"variance_intercept_offset_{cov.name}\"].std()\n                new_categorical_intercept_offset = pm.Normal(\n                    f\"intercept_offset_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=pretrain_sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new offsets\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"intercept_offset_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept_offset,\n                            new_categorical_intercept_offset,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                # Non-hierarchical (linear) categorical effect\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                new_categorical_intercept = pm.Normal(\n                    f\"intercept_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new offsets\n                categorical_intercept = pm.Deterministic(\n                    f\"intercept_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept,\n                            new_categorical_intercept,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n            self.model_params[\"n_params\"] += new_category_count\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n\n    def _model_all_mean_effects(\n        self,\n        train_data: pd.DataFrame,\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; list[TensorVariable]:\n        \"\"\"\n        Model all covariate mean effects.\n        \"\"\"\n        mean_effects = []\n        # Model the global intercept\n        if adapt is None:  # Model fitting\n            global_intercept = pm.Normal(\n                \"global_intercept\",\n                mu=0,\n                sigma=5,\n                dims=(\"scalar\",),\n            )\n            # Increment parameter count for global intercept\n            self.model_params[\"n_params\"] += 1\n        else:  # Freeze during adaptation/fine-tuning\n            global_intercept = pm.Deterministic(\n                \"global_intercept\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        \"global_intercept\"\n                    ],\n                ),\n                dims=(\"scalar\",),\n            )\n        mean_effects.append(global_intercept)\n        # Model additional covariate effects on the mean\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_mean:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_mean_effect(\n                            train_data,\n                            cov,\n                            mean_effects,\n                            sigma_prior=5,\n                            adapt=adapt,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_mean_effect(\n                            train_data,\n                            cov,\n                            mean_effects,\n                            spline_bases,\n                            sigma_prior=5,\n                            adapt=adapt,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_mean_effect(\n                        train_data,\n                        cov,\n                        mean_effects,\n                        category_indices,\n                        sigma_prior=1,\n                        hierarchical_sigma_prior=5,\n                        adapt=adapt,\n                    )\n                else:\n                    err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    raise ValueError(err)\n        return mean_effects\n\n    def _model_linear_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        sigma_prior: float = 0.1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate on the variance estimate.\n        \"\"\"\n        # Linear effect\n        if adapt is None:  # Model fitting\n            linear_beta = pm.Normal(\n                f\"variance_linear_beta_{cov.name}\",\n                mu=0,\n                sigma=sigma_prior,\n                size=1,\n                dims=(f\"{cov.name}_linear\",),\n            )\n            # Increment parameter count for linear effect\n            self.model_params[\"n_params\"] += 1\n        else:  # Freeze during adaptation/fine-tuning\n            linear_beta = pm.Deterministic(\n                f\"variance_linear_beta_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"variance_linear_beta_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_linear\",),\n            )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                (\n                    cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy())\n                    - cov.moments[0]\n                )\n                / cov.moments[1]\n                * linear_beta,\n            )\n\n    def _model_spline_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 0.1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate on the variance estimate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            spline_betas = pm.ZeroSumNormal(\n                f\"variance_spline_betas_{cov.name}\",\n                sigma=sigma_prior,\n                shape=spline_bases[cov.name].shape[1],\n                dims=(f\"{cov.name}_splines\",),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n            # Increment parameter count for spline effects\n            if cov.spline_spec is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n        else:  # Freeze during adaptation/fine-tuning\n            spline_betas = pm.Deterministic(\n                f\"variance_spline_betas_{cov.name}\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        f\"variance_spline_betas_{cov.name}\"\n                    ],\n                ),\n                dims=(f\"{cov.name}_splines\",),\n            )\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))\n\n    def _model_categorical_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 0.1,\n        hierarchical_sigma_prior: float = 0.1,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate on the variance estimate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(\n                cast(\"npt.NDArray[Any]\", train_data[cov.name].to_numpy()),\n            ),\n        )\n        if adapt is None:  # Model fitting\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.HalfNormal(\n                    f\"variance_sigma_intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(\"scalar\",),\n                )\n\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.ZeroSumNormal(\n                    f\"variance_intercept_offset_{cov.name}\",\n                    sigma=hierarchical_sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n                categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n\n                # Increment parameter count for hierarchical intercept\n                self.model_params[\"n_params\"] += 1\n\n            else:\n                # Non-hierarchical (linear) categorical effect\n                categorical_intercept = pm.ZeroSumNormal(\n                    f\"variance_intercept_{cov.name}\",\n                    sigma=sigma_prior,\n                    dims=(cov.name,),\n                )\n                # Note ZeroSumNormal imposes a centering constraint\n                # (ensuring identifiability)\n            # Increment parameter count for categorical effects\n            if cov.categories is not None:  # to satisfy type checker\n                self.model_params[\"n_params\"] += len(cov.categories) - 1\n        elif cov.name != adapt[\"covariate_to_adapt\"]:\n            # Freeze during adaptation/fine-tuning\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                sigma_intercept_category = pm.Deterministic(\n                    f\"variance_sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"variance_intercept_offset_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n        else:  # Partial freezing (fit parameters for the new site only)\n            if cov.hierarchical:\n                # Hierarchical categorical effect\n                # Hyperpriors for category (Bayesian equivalent of random effects)\n                # Hyperpriors are fixed during adaptation\n                sigma_intercept_category = pm.Deterministic(\n                    f\"variance_sigma_intercept_{cov.name}\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_sigma_intercept_{cov.name}\"\n                        ],\n                    ),\n                    dims=(\"scalar\",),\n                )\n                # Hierarchical intercepts for each category (using reparameterized form)\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept_offset = pm.Deterministic(\n                    f\"variance_intercept_offset_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_intercept_offset_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                pretrain_sigma_prior = adapt[\"pretrained_model_params\"][\n                    \"posterior_means\"\n                ][f\"variance_intercept_offset_{cov.name}\"].std()\n                new_categorical_intercept_offset = pm.Normal(\n                    f\"variance_intercept_offset_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=pretrain_sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new offsets\n                categorical_intercept_offset = pm.Deterministic(\n                    f\"variance_intercept_offset_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept_offset,\n                            new_categorical_intercept_offset,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n                categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}\",\n                    (\n                        categorical_intercept_offset\n                        * pt.reshape(sigma_intercept_category, (1,))  # pyright: ignore[reportPrivateImportUsage]\n                    ),\n                    dims=(cov.name,),\n                )\n            else:\n                # Non-hierarchical (linear) categorical effect\n                # New categories get new parameters, old categories are fixed\n                # Freeze old category parameters during adaptation\n                fixed_categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}_fixed\",\n                    pt.as_tensor_variable(\n                        adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                            f\"variance_intercept_{cov.name}\"\n                        ],\n                    ),\n                )\n                # Create new parameters for new categories\n                new_category_count = len(adapt[\"new_category_names\"])\n                new_categorical_intercept = pm.Normal(\n                    f\"variance_intercept_{cov.name}_adapt\",\n                    mu=0,\n                    sigma=sigma_prior,\n                    size=new_category_count,\n                )\n                # Combine fixed and new offsets\n                categorical_intercept = pm.Deterministic(\n                    f\"variance_intercept_{cov.name}\",\n                    pt.concatenate(\n                        [\n                            fixed_categorical_intercept,\n                            new_categorical_intercept,\n                        ],\n                    ),\n                    dims=(cov.name,),\n                )\n            self.model_params[\"n_params\"] += new_category_count\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n\n    def _model_all_variance_effects(\n        self,\n        train_data: pd.DataFrame,\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; list[TensorVariable]:\n        \"\"\"\n        Model all covariate variance effects.\n        \"\"\"\n        variance_effects = []\n        # Model the global variance\n        if adapt is None:\n            global_variance_baseline = pm.Normal(\n                \"global_variance_baseline\",\n                mu=-0.0,\n                sigma=0.5,\n                dims=(\"scalar\",),\n            )\n            # Increment parameter count for global variance\n            self.model_params[\"n_params\"] += 1\n        else:\n            global_variance_baseline = pm.Deterministic(\n                \"global_variance_baseline\",\n                pt.as_tensor_variable(\n                    adapt[\"pretrained_model_params\"][\"posterior_means\"][\n                        \"global_variance_baseline\"\n                    ],\n                ),\n                dims=(\"scalar\",),\n            )\n        variance_effects.append(global_variance_baseline)\n        # Model additional covariate effects on the variance\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_variance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_variance_effect(\n                            train_data,\n                            cov,\n                            variance_effects,\n                            sigma_prior=0.1,\n                            adapt=adapt,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_variance_effect(\n                            train_data,\n                            cov,\n                            variance_effects,\n                            spline_bases,\n                            sigma_prior=0.1,\n                            adapt=adapt,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_variance_effect(\n                        train_data,\n                        cov,\n                        variance_effects,\n                        category_indices,\n                        sigma_prior=0.1,\n                        hierarchical_sigma_prior=0.1,\n                        adapt=adapt,\n                    )\n                else:\n                    err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    raise ValueError(err)\n        return variance_effects\n\n    def _combine_all_effects(\n        self,\n        mean_effects: list[TensorVariable],\n        variance_effects: list[TensorVariable],\n        standardized_voi: npt.NDArray[np.floating[Any]],\n    ) -&gt; None:\n        \"\"\"\n        Combine all effects to model the observed data likelihood.\n        \"\"\"\n        # Combine all mean and variance effects\n        mu_estimate = sum(mean_effects)\n        log_sigma_estimate = sum(variance_effects)\n        sigma_estimate = pt.exp(log_sigma_estimate)\n\n        effective_sample_size = self.model_params[\"sample_size\"]\n\n        # Model likelihood of the variable of interest\n        _likelihood = pm.Normal(\n            f\"likelihood_{self.spec.variable_of_interest}\",\n            mu=mu_estimate,\n            sigma=sigma_estimate,\n            observed=standardized_voi,\n            total_size=effective_sample_size,\n        )\n\n    def _fit_model_with_advi(self, *, progress_bar: bool = True) -&gt; None:\n        \"\"\"\n        Fit the model using Automatic Differentiation Variational Inference (ADVI).\n        \"\"\"\n        base_lr = self.defaults[\"adam_learning_rate\"]\n        decay = self.defaults[\"adam_learning_rate_decay\"]\n        lr = shared(base_lr)\n        optimizer = pm.adam(learning_rate=cast(\"float\", lr))\n\n        # Adaptive learning rate schedule callback\n        def update_learning_rate(_approx: Any, _loss: Any, iteration: int) -&gt; None:\n            lr.set_value(base_lr * (decay**iteration))\n\n        # Run automatic differential variational inference to fit the model\n        self._trace = pm.fit(\n            method=\"advi\",\n            n=self.defaults[\"advi_iterations\"],\n            random_seed=self.defaults[\"random_seed\"],  # For reproducibility\n            obj_optimizer=optimizer,\n            callbacks=[\n                update_learning_rate,\n                pm.callbacks.CheckParametersConvergence(\n                    tolerance=self.defaults[\"advi_convergence_tolerance\"],\n                    diff=\"relative\",\n                ),\n            ],\n            progressbar=progress_bar,\n        )\n\n        # Sample from the posterior distribution and store the results\n        self.model_inference_data = self._trace.sample(\n            2000,\n            random_seed=self.defaults[\"random_seed\"],\n        )\n\n        # Compute posterior means and standard deviations\n        posterior_means = self.model_inference_data.posterior.mean(\n            dim=(\"chain\", \"draw\"),\n        )\n        posterior_stds = self.model_inference_data.posterior.std(dim=(\"chain\", \"draw\"))\n\n        # Store posterior means and stds as a dictionary in model parameters\n        self.model_params[\"posterior_means\"] = {\n            x: posterior_means.data_vars[x].to_numpy()\n            for x in posterior_means.data_vars\n        }\n        self.model_params[\"posterior_stds\"] = {\n            x: posterior_stds.data_vars[x].to_numpy() for x in posterior_stds.data_vars\n        }\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the normative model to the training data.\n\n        This method implements the fitting logic for the normative model\n        based on the provided training data and model specification.\n\n        Args:\n            train_data: pd.DataFrame\n                DataFrame containing the training data. It must include the variable\n                of interest and all specified covariates.\n            save_directory: Path | None\n                A path to a directory to save the model. If provided, the fitted model\n                will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n            adapt: dict[str, Any] | None\n                If provided, adapt a pre-trained model to a new covariate.\n                Note: We recommended using the `adapt_fit` method, and not directly\n                changing this argument, unless you know what you are doing.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # Extract the variable of interest\n        variable_of_interest = train_data[self.spec.variable_of_interest].to_numpy()\n\n        # A dictionary to hold the model parameters after fitting\n        if adapt is None:\n            self.model_params = {}\n            self.model_params[\"mean_VOI\"] = variable_of_interest.mean()\n            self.model_params[\"std_VOI\"] = variable_of_interest.std()\n            self.model_params[\"sample_size\"] = variable_of_interest.shape[0]\n            # Initialize parameter count\n            self.model_params[\"n_params\"] = 0\n        else:\n            # Update the pretrained model parameters\n            if not hasattr(self, \"model_params\") or self.model_params is None:\n                self.model_params = copy.deepcopy(adapt[\"pretrained_model_params\"])\n            self.model_params[\"sample_size\"] += variable_of_interest.shape[0]\n\n        # Data preparation\n        model_coords = self._build_model_coordinates(\n            observations=np.arange(train_data.shape[0]),\n        )\n\n        # Fitting logic\n        with pm.Model(coords=model_coords) as self._model:\n            # Standardize the variable of interest, and store mean and std\n            # This ensures that the model is not sensitive to the scale of the variable\n            standardized_voi = (\n                variable_of_interest - self.model_params[\"mean_VOI\"]\n            ) / self.model_params[\"std_VOI\"]\n\n            # A dictionary for precomputed bspline basis functions\n            spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n            # A dictionary for factorized categories\n            category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n            # Model the mean of the variable of interest\n            mean_effects = self._model_all_mean_effects(\n                train_data,\n                spline_bases,\n                category_indices,\n                adapt=adapt,\n            )\n\n            # Model the variance of the variable of interest\n            variance_effects = self._model_all_variance_effects(\n                train_data,\n                spline_bases,\n                category_indices,\n                adapt=adapt,\n            )\n\n            # Combine all mean and variance effects\n            self._combine_all_effects(\n                mean_effects,\n                variance_effects,\n                standardized_voi,\n            )\n\n            # Fit the model using ADVI\n            self._fit_model_with_advi(progress_bar=progress_bar)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(Path(save_directory))\n\n    def adapt_fit(\n        self,\n        covariate_to_adapt: str,\n        new_category_names: npt.NDArray[np.str_],\n        train_data: pd.DataFrame,\n        *,\n        pretrained_model_params: dict[str, Any] | None = None,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Using a previously fitted model, adapt the model to a new batch.\n        This method enables adaptation of the model to data from a new\n        batch/site by freezing all fitted parameters, and only estimating\n        new parameters for the new batch/site category.\n\n        Args:\n            covariate_to_adapt: str\n                Name of the categorical covariate representing the batch/site\n                to which the model should be adapted.\n                Note: This covariate must have been specified in the original\n                model.\n            new_category_names: list[str]\n                Names of the new categories in the covariate_to_adapt representing\n                the new batch/site labels (e.g. names of the new site).\n                Note: These names must not have been present in the original\n                fitted model.\n            train_data: pd.DataFrame\n                DataFrame containing the training data for adaptation.\n                It must include the variable of interest and all specified covariates.\n                Note: The covariate_to_adapt column must only contain the\n                new_category_names (no new data from previously trained batches).\n            pretrained_model_params: dict[str, Any] | None\n                The model parameters from a previously fitted model to adapt.\n                If None, the model parameters from the current instance will be used\n                (assuming fitting was done).\n            save_directory: Path | None\n                A path to a directory to save the adapted model. If provided,\n                the fitted model will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # Locate the covariate to adapt\n        cov_to_adapt_index = [cov.name for cov in self.spec.covariates].index(\n            covariate_to_adapt,\n        )\n\n        # Extend the covariate categories to include the new categories\n        self.spec.covariates[cov_to_adapt_index].extend_categories(new_category_names)\n\n        # Extract the pre-trained model parameters\n        if pretrained_model_params is None:\n            if not self.model_params:\n                err = (\n                    \"No pretrained model parameters found. \"\n                    \"Please provide pretrained_model_params or fit the model first.\"\n                )\n                raise ValueError(err)\n            pretrained_model_params = copy.deepcopy(self.model_params)\n\n        # Fit the adapted model\n        self.fit(\n            train_data,\n            save_directory=save_directory,\n            progress_bar=progress_bar,\n            adapt={\n                \"covariate_to_adapt\": covariate_to_adapt,\n                \"new_category_names\": new_category_names,\n                \"pretrained_model_params\": pretrained_model_params,\n            },\n        )\n\n    def _predict_mu(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        predict_without: list[str],\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict the mean of the variable of interest.\n        \"\"\"\n        # Calculate mean effect\n        mu_estimate = np.full(\n            test_covariates.shape[0],\n            model_params[\"posterior_means\"][\"global_intercept\"].item(),\n        )\n\n        for cov in self.spec.covariates:\n            if (cov.name in self.spec.influencing_mean) and (\n                cov.name not in predict_without\n            ):\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is not None:  # to satisfy type checker\n                            mu_estimate += (\n                                (\n                                    cast(\n                                        \"npt.NDArray[Any]\",\n                                        test_covariates[cov.name].to_numpy(),\n                                    )\n                                    - cov.moments[0]\n                                )\n                                / cov.moments[1]\n                            ) * model_params[\"posterior_means\"][\n                                f\"linear_beta_{cov.name}\"\n                            ]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            cast(\n                                \"npt.NDArray[Any]\",\n                                test_covariates[cov.name].to_numpy(),\n                            ),\n                        )\n                        spline_betas = model_params[\"posterior_means\"][\n                            f\"spline_betas_{cov.name}\"\n                        ]\n                        mu_estimate += np.dot(spline_bases, spline_betas)\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        cast(\"npt.NDArray[Any]\", test_covariates[cov.name].to_numpy()),\n                    )\n                    if cov.hierarchical:\n                        categorical_intercept = (\n                            model_params[\"posterior_means\"][\n                                f\"intercept_offset_{cov.name}\"\n                            ]\n                            * model_params[\"posterior_means\"][\n                                f\"sigma_intercept_{cov.name}\"\n                            ]\n                        )\n                    else:\n                        categorical_intercept = model_params[\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ]\n                    mu_estimate += categorical_intercept[category_indices]\n\n        return np.array(\n            mu_estimate * model_params[\"std_VOI\"] + model_params[\"mean_VOI\"],\n        )\n\n    def _predict_std(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        predict_without: list[str],\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict the standard deviation of the variable of interest.\n        \"\"\"\n        # Calculate deviation effect\n        log_sigma_estimate = np.full(\n            test_covariates.shape[0],\n            model_params[\"posterior_means\"][\"global_variance_baseline\"].item(),\n        )\n\n        for cov in self.spec.covariates:\n            if (\n                cov.name in self.spec.influencing_variance\n                and cov.name not in predict_without\n            ):\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is not None:  # to satisfy type checker\n                            log_sigma_estimate += (\n                                (\n                                    cast(\n                                        \"npt.NDArray[Any]\",\n                                        test_covariates[cov.name].to_numpy(),\n                                    )\n                                    - cov.moments[0]\n                                )\n                                / cov.moments[1]\n                            ) * model_params[\"posterior_means\"][\n                                f\"variance_linear_beta_{cov.name}\"\n                            ]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            cast(\n                                \"npt.NDArray[Any]\",\n                                test_covariates[cov.name].to_numpy(),\n                            ),\n                        )\n                        variance_spline_betas = model_params[\"posterior_means\"][\n                            f\"variance_spline_betas_{cov.name}\"\n                        ]\n                        log_sigma_estimate += spline_bases @ variance_spline_betas\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        cast(\"npt.NDArray[Any]\", test_covariates[cov.name].to_numpy()),\n                    )\n                    if cov.hierarchical:\n                        categorical_variance_intercept = (\n                            model_params[\"posterior_means\"][\n                                f\"variance_intercept_offset_{cov.name}\"\n                            ]\n                            * model_params[\"posterior_means\"][\n                                f\"variance_sigma_intercept_{cov.name}\"\n                            ]\n                        )\n                    else:\n                        categorical_variance_intercept = model_params[\n                            \"posterior_means\"\n                        ][f\"variance_intercept_{cov.name}\"]\n                    log_sigma_estimate += categorical_variance_intercept[\n                        category_indices\n                    ]\n\n        return np.array(np.exp(log_sigma_estimate) * model_params[\"std_VOI\"])\n\n    def predict(\n        self,\n        test_covariates: pd.DataFrame,\n        *,\n        extended: bool = False,\n        model_params: dict[str, Any] | None = None,\n        predict_without: list[str] | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict normative moments (mean, std) for new data using the fitted model.\n\n        Args:\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n                Note: covariates listed in predict_without will be ignored and are\n                hence not required.\n            extended: bool\n                If True, return additional stats such as log-likelihood, centiles, etc.\n                Note that extended predictions require variable_of_interest to be\n                provided in the test_covariates DataFrame.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            predict_without: list[str] | None\n                Optional list of covariate names to ignore during prediction.\n                This can be used to check the effect of removing certain covariates\n                from the model.\n\n        Returns:\n            NormativePredictions: Object containing the predicted moments (mean, std)\n                for the variable of interest.\n        \"\"\"\n        # Validate the new data\n        validation_columns = [\n            cov.name\n            for cov in self.spec.covariates\n            if cov.name not in (predict_without or [])\n        ]\n        if extended:\n            validation_columns.append(self.spec.variable_of_interest)\n        utils.general.validate_dataframe(test_covariates, validation_columns)\n\n        # Parameters\n        model_params = model_params or self.model_params\n        if model_params is None:\n            err = \"No model parameters found. Please provide model_params.\"\n            raise ValueError(err)\n\n        # Calculate mean and variance effects and store in the predictions object\n        predictions = NormativePredictions(\n            {\n                \"mu_estimate\": self._predict_mu(\n                    test_covariates,\n                    model_params,\n                    (predict_without or []),\n                ),\n                \"std_estimate\": self._predict_std(\n                    test_covariates,\n                    model_params,\n                    (predict_without or []),\n                ),\n            },\n        )\n\n        # Check if extended predictions are requested\n        if extended:\n            # Add extended statistics to predictions (e.g. centiles, log loss, etc.)\n            predictions.extend_predictions(\n                variable_of_interest=(\n                    cast(\n                        \"npt.NDArray[Any]\",\n                        test_covariates[self.spec.variable_of_interest].to_numpy(),\n                    )\n                ),\n            )\n\n        return predictions\n\n    def evaluate(self, new_data: pd.DataFrame) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the model on new data and return predictions.\n\n        Args:\n            new_data: pd.DataFrame\n                DataFrame containing the new data to evaluate.\n                It must include all specified covariates and the variable of interest.\n\n        Returns:\n            NormativePredictions: Object containing the predictions and evaluation\n            metrics.\n        \"\"\"\n        # Run extended predictions\n        return self.predict(test_covariates=new_data).evaluate_predictions(\n            variable_of_interest=(\n                cast(\n                    \"npt.NDArray[Any]\",\n                    new_data[self.spec.variable_of_interest].to_numpy(),\n                )\n            ),\n            train_mean=self.model_params[\"mean_VOI\"],\n            train_std=self.model_params[\"std_VOI\"],\n            n_params=self.model_params[\"n_params\"],\n        )\n\n    def harmonize(\n        self,\n        data: pd.DataFrame,\n        covariates_to_harmonize: list[str],\n        *,\n        model_params: dict[str, Any] | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Harmonize the variable of interest in the data to remove effects of\n        certain covariates (e.g. batch).\n\n        Args:\n            data: pd.DataFrame\n                DataFrame containing the data to harmonize.\n                It must include all specified covariates and the variable of interest.\n            covariates_to_harmonize: list[str]\n                List of covariate names to harmonize.\n                The partial effects of these covariates will be removed from the\n                variable of interest, and the harmonized values will be returned.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n\n        Returns:\n            npt.NDArray[np.floating[Any]]: Array of harmonized values for the\n                variable of interest.\n        \"\"\"\n        # Validate the new data\n        validation_columns = [cov.name for cov in self.spec.covariates]\n        validation_columns.append(self.spec.variable_of_interest)\n        utils.general.validate_dataframe(data, validation_columns)\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Predict the mean and std with all covariates\n        full_predictions = self.predict(\n            test_covariates=data,\n            model_params=model_params,\n            predict_without=[],\n        )\n\n        # Predict the mean and std without the covariates to harmonize\n        reduced_predictions = self.predict(\n            test_covariates=data,\n            model_params=model_params,\n            predict_without=covariates_to_harmonize,\n        )\n\n        # First standardize the variable of interest based on the full model\n        voi_standardized = (\n            (cast(\"npt.NDArray[Any]\", data[self.spec.variable_of_interest].to_numpy()))\n            - full_predictions.predictions[\"mu_estimate\"]\n        ) / full_predictions.predictions[\"std_estimate\"]\n\n        # Then return the harmonized values based on the reduced model\n        return np.asarray(\n            (\n                voi_standardized * reduced_predictions.predictions[\"std_estimate\"]\n                + reduced_predictions.predictions[\"mu_estimate\"]\n            ),\n            dtype=np.float64,\n        )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.adapt_fit","title":"<code>adapt_fit(covariate_to_adapt: str, new_category_names: npt.NDArray[np.str_], train_data: pd.DataFrame, *, pretrained_model_params: dict[str, Any] | None = None, save_directory: Path | None = None, progress_bar: bool = True) -&gt; None</code>","text":"<p>Using a previously fitted model, adapt the model to a new batch. This method enables adaptation of the model to data from a new batch/site by freezing all fitted parameters, and only estimating new parameters for the new batch/site category.</p> <p>Parameters:</p> Name Type Description Default <code>covariate_to_adapt</code> <code>str</code> <p>str Name of the categorical covariate representing the batch/site to which the model should be adapted. Note: This covariate must have been specified in the original model.</p> required <code>new_category_names</code> <code>NDArray[str_]</code> <p>list[str] Names of the new categories in the covariate_to_adapt representing the new batch/site labels (e.g. names of the new site). Note: These names must not have been present in the original fitted model.</p> required <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data for adaptation. It must include the variable of interest and all specified covariates. Note: The covariate_to_adapt column must only contain the new_category_names (no new data from previously trained batches).</p> required <code>pretrained_model_params</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None The model parameters from a previously fitted model to adapt. If None, the model parameters from the current instance will be used (assuming fitting was done).</p> <code>None</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the adapted model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def adapt_fit(\n    self,\n    covariate_to_adapt: str,\n    new_category_names: npt.NDArray[np.str_],\n    train_data: pd.DataFrame,\n    *,\n    pretrained_model_params: dict[str, Any] | None = None,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n) -&gt; None:\n    \"\"\"\n    Using a previously fitted model, adapt the model to a new batch.\n    This method enables adaptation of the model to data from a new\n    batch/site by freezing all fitted parameters, and only estimating\n    new parameters for the new batch/site category.\n\n    Args:\n        covariate_to_adapt: str\n            Name of the categorical covariate representing the batch/site\n            to which the model should be adapted.\n            Note: This covariate must have been specified in the original\n            model.\n        new_category_names: list[str]\n            Names of the new categories in the covariate_to_adapt representing\n            the new batch/site labels (e.g. names of the new site).\n            Note: These names must not have been present in the original\n            fitted model.\n        train_data: pd.DataFrame\n            DataFrame containing the training data for adaptation.\n            It must include the variable of interest and all specified covariates.\n            Note: The covariate_to_adapt column must only contain the\n            new_category_names (no new data from previously trained batches).\n        pretrained_model_params: dict[str, Any] | None\n            The model parameters from a previously fitted model to adapt.\n            If None, the model parameters from the current instance will be used\n            (assuming fitting was done).\n        save_directory: Path | None\n            A path to a directory to save the adapted model. If provided,\n            the fitted model will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # Locate the covariate to adapt\n    cov_to_adapt_index = [cov.name for cov in self.spec.covariates].index(\n        covariate_to_adapt,\n    )\n\n    # Extend the covariate categories to include the new categories\n    self.spec.covariates[cov_to_adapt_index].extend_categories(new_category_names)\n\n    # Extract the pre-trained model parameters\n    if pretrained_model_params is None:\n        if not self.model_params:\n            err = (\n                \"No pretrained model parameters found. \"\n                \"Please provide pretrained_model_params or fit the model first.\"\n            )\n            raise ValueError(err)\n        pretrained_model_params = copy.deepcopy(self.model_params)\n\n    # Fit the adapted model\n    self.fit(\n        train_data,\n        save_directory=save_directory,\n        progress_bar=progress_bar,\n        adapt={\n            \"covariate_to_adapt\": covariate_to_adapt,\n            \"new_category_names\": new_category_names,\n            \"pretrained_model_params\": pretrained_model_params,\n        },\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.evaluate","title":"<code>evaluate(new_data: pd.DataFrame) -&gt; NormativePredictions</code>","text":"<p>Evaluate the model on new data and return predictions.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new data to evaluate. It must include all specified covariates and the variable of interest.</p> required <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predictions and evaluation</p> <code>NormativePredictions</code> <p>metrics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate(self, new_data: pd.DataFrame) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the model on new data and return predictions.\n\n    Args:\n        new_data: pd.DataFrame\n            DataFrame containing the new data to evaluate.\n            It must include all specified covariates and the variable of interest.\n\n    Returns:\n        NormativePredictions: Object containing the predictions and evaluation\n        metrics.\n    \"\"\"\n    # Run extended predictions\n    return self.predict(test_covariates=new_data).evaluate_predictions(\n        variable_of_interest=(\n            cast(\n                \"npt.NDArray[Any]\",\n                new_data[self.spec.variable_of_interest].to_numpy(),\n            )\n        ),\n        train_mean=self.model_params[\"mean_VOI\"],\n        train_std=self.model_params[\"std_VOI\"],\n        n_params=self.model_params[\"n_params\"],\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.fit","title":"<code>fit(train_data: pd.DataFrame, *, save_directory: Path | None = None, progress_bar: bool = True, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Fit the normative model to the training data.</p> <p>This method implements the fitting logic for the normative model based on the provided training data and model specification.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data. It must include the variable of interest and all specified covariates.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None If provided, adapt a pre-trained model to a new covariate. Note: We recommended using the <code>adapt_fit</code> method, and not directly changing this argument, unless you know what you are doing.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    train_data: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Fit the normative model to the training data.\n\n    This method implements the fitting logic for the normative model\n    based on the provided training data and model specification.\n\n    Args:\n        train_data: pd.DataFrame\n            DataFrame containing the training data. It must include the variable\n            of interest and all specified covariates.\n        save_directory: Path | None\n            A path to a directory to save the model. If provided, the fitted model\n            will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n        adapt: dict[str, Any] | None\n            If provided, adapt a pre-trained model to a new covariate.\n            Note: We recommended using the `adapt_fit` method, and not directly\n            changing this argument, unless you know what you are doing.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # Extract the variable of interest\n    variable_of_interest = train_data[self.spec.variable_of_interest].to_numpy()\n\n    # A dictionary to hold the model parameters after fitting\n    if adapt is None:\n        self.model_params = {}\n        self.model_params[\"mean_VOI\"] = variable_of_interest.mean()\n        self.model_params[\"std_VOI\"] = variable_of_interest.std()\n        self.model_params[\"sample_size\"] = variable_of_interest.shape[0]\n        # Initialize parameter count\n        self.model_params[\"n_params\"] = 0\n    else:\n        # Update the pretrained model parameters\n        if not hasattr(self, \"model_params\") or self.model_params is None:\n            self.model_params = copy.deepcopy(adapt[\"pretrained_model_params\"])\n        self.model_params[\"sample_size\"] += variable_of_interest.shape[0]\n\n    # Data preparation\n    model_coords = self._build_model_coordinates(\n        observations=np.arange(train_data.shape[0]),\n    )\n\n    # Fitting logic\n    with pm.Model(coords=model_coords) as self._model:\n        # Standardize the variable of interest, and store mean and std\n        # This ensures that the model is not sensitive to the scale of the variable\n        standardized_voi = (\n            variable_of_interest - self.model_params[\"mean_VOI\"]\n        ) / self.model_params[\"std_VOI\"]\n\n        # A dictionary for precomputed bspline basis functions\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n        # A dictionary for factorized categories\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n        # Model the mean of the variable of interest\n        mean_effects = self._model_all_mean_effects(\n            train_data,\n            spline_bases,\n            category_indices,\n            adapt=adapt,\n        )\n\n        # Model the variance of the variable of interest\n        variance_effects = self._model_all_variance_effects(\n            train_data,\n            spline_bases,\n            category_indices,\n            adapt=adapt,\n        )\n\n        # Combine all mean and variance effects\n        self._combine_all_effects(\n            mean_effects,\n            variance_effects,\n            standardized_voi,\n        )\n\n        # Fit the model using ADVI\n        self._fit_model_with_advi(progress_bar=progress_bar)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(Path(save_directory))\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.from_dataframe","title":"<code>from_dataframe(model_type: ModelType, dataframe: pd.DataFrame, variable_of_interest: str, numerical_covariates: list[str] | None = None, categorical_covariates: list[str] | None = None, batch_covariates: list[str] | None = None, nonlinear_covariates: list[str] | None = None, influencing_mean: list[str] | None = None, influencing_variance: list[str] | None = None, spline_kwargs: dict[str, Any] | None = None) -&gt; DirectNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize a normative model from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>ModelType Type of the model to create, either \"HBR\" (Hierarchical Bayesian Regression) or \"BLR\" (Bayesian Linear Regression).</p> required <code>dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the data.</p> required <code>variable_of_interest</code> <code>str</code> <p>str Name of the target variable to model.</p> required <code>numerical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of numerical covariate names.</p> <code>None</code> <code>categorical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of categorical covariate names.</p> <code>None</code> <code>batch_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of batch covariate names which should also be included in categorical_covariates.</p> <code>None</code> <code>nonlinear_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names to be modeled as nonlinear effects. These should also be included in numerical_covariates.</p> <code>None</code> <code>influencing_mean</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the mean of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>influencing_variance</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the variance of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>spline_kwargs</code> <code>dict[str, Any] | None</code> <p>dict Additional keyword arguments for spline specification, such as <code>df</code>, <code>degree</code>, and <code>knots</code>. These are passed to the <code>create_spline_spec</code> method to create spline specifications for nonlinear covariates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirectNormativeModel</code> <p>DirectNormativeModel An instance of DirectNormativeModel initialized with the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef from_dataframe(\n    cls,\n    model_type: ModelType,\n    dataframe: pd.DataFrame,\n    variable_of_interest: str,\n    numerical_covariates: list[str] | None = None,\n    categorical_covariates: list[str] | None = None,\n    batch_covariates: list[str] | None = None,\n    nonlinear_covariates: list[str] | None = None,\n    influencing_mean: list[str] | None = None,\n    influencing_variance: list[str] | None = None,\n    spline_kwargs: dict[str, Any] | None = None,\n) -&gt; DirectNormativeModel:\n    \"\"\"\n    Initialize a normative model from a pandas DataFrame.\n\n    Args:\n        model_type: ModelType\n            Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n            Regression) or \"BLR\" (Bayesian Linear Regression).\n        dataframe: pd.DataFrame\n            DataFrame containing the data.\n        variable_of_interest: str\n            Name of the target variable to model.\n        numerical_covariates: list[str] | None\n            List of numerical covariate names.\n        categorical_covariates: list[str] | None\n            List of categorical covariate names.\n        batch_covariates: list[str] | None\n            List of batch covariate names which should also be included in\n            categorical_covariates.\n        nonlinear_covariates: list[str] | None\n            List of covariate names to be modeled as nonlinear effects.\n            These should also be included in numerical_covariates.\n        influencing_mean: list[str] | None\n            List of covariate names that influence the mean of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        influencing_variance: list[str] | None\n            List of covariate names that influence the variance of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        spline_kwargs: dict\n            Additional keyword arguments for spline specification, such as\n            `df`, `degree`, and `knots`. These are passed to the\n            `create_spline_spec` method to create spline specifications for\n            nonlinear covariates.\n\n    Returns:\n        DirectNormativeModel\n            An instance of DirectNormativeModel initialized with the provided data.\n    \"\"\"\n    # Set default values for optional parameters\n    numerical_covariates = numerical_covariates or []\n    categorical_covariates = categorical_covariates or []\n    batch_covariates = batch_covariates or []\n    nonlinear_covariates = nonlinear_covariates or []\n    influencing_mean = influencing_mean or []\n    influencing_variance = influencing_variance or []\n    spline_kwargs = spline_kwargs or {}\n\n    # Validity checks for input parameters\n    cls._validate_init_args(\n        model_type,\n        variable_of_interest,\n        numerical_covariates,\n        categorical_covariates,\n        batch_covariates,\n        nonlinear_covariates,\n    )\n    utils.general.validate_dataframe(\n        dataframe,\n        [variable_of_interest, *numerical_covariates, *categorical_covariates],\n    )\n\n    # Create an instance of the class\n    self = cls(\n        spec=NormativeModelSpec(\n            variable_of_interest=variable_of_interest,\n            covariates=[],\n            influencing_mean=influencing_mean,\n            influencing_variance=influencing_variance,\n        ),\n    )\n\n    # Populate the spline_kwargs with defaults if not provided\n    spline_kwargs[\"df\"] = spline_kwargs.get(\"df\", self.defaults[\"spline_df\"])\n    spline_kwargs[\"degree\"] = spline_kwargs.get(\n        \"degree\",\n        self.defaults[\"spline_degree\"],\n    )\n    spline_kwargs[\"extrapolation_factor\"] = spline_kwargs.get(\n        \"extrapolation_factor\",\n        self.defaults[\"spline_extrapolation_factor\"],\n    )\n\n    # Start building the model specification\n    # Add categorical covariates\n    for cov_name in categorical_covariates:\n        hierarchical = False\n        if cov_name in batch_covariates and model_type == \"HBR\":\n            hierarchical = True\n        self.spec.covariates.append(\n            CovariateSpec(\n                name=cov_name,\n                cov_type=\"categorical\",\n                categories=dataframe[cov_name].unique(),\n                hierarchical=hierarchical,\n            ),\n        )\n    for cov_name in numerical_covariates:\n        if cov_name not in nonlinear_covariates:\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"numerical\",\n                    effect=\"linear\",\n                    moments=(\n                        dataframe[cov_name].mean(),\n                        dataframe[cov_name].std(),\n                    ),\n                ),\n            )\n        else:\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"numerical\",\n                    effect=\"spline\",\n                    spline_spec=SplineSpec.create_spline_spec(\n                        dataframe[cov_name],\n                        **spline_kwargs,\n                    ),\n                ),\n            )\n    return self\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.harmonize","title":"<code>harmonize(data: pd.DataFrame, covariates_to_harmonize: list[str], *, model_params: dict[str, Any] | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Harmonize the variable of interest in the data to remove effects of certain covariates (e.g. batch).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the data to harmonize. It must include all specified covariates and the variable of interest.</p> required <code>covariates_to_harmonize</code> <code>list[str]</code> <p>list[str] List of covariate names to harmonize. The partial effects of these covariates will be removed from the variable of interest, and the harmonized values will be returned.</p> required <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>npt.NDArray[np.floating[Any]]: Array of harmonized values for the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def harmonize(\n    self,\n    data: pd.DataFrame,\n    covariates_to_harmonize: list[str],\n    *,\n    model_params: dict[str, Any] | None = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Harmonize the variable of interest in the data to remove effects of\n    certain covariates (e.g. batch).\n\n    Args:\n        data: pd.DataFrame\n            DataFrame containing the data to harmonize.\n            It must include all specified covariates and the variable of interest.\n        covariates_to_harmonize: list[str]\n            List of covariate names to harmonize.\n            The partial effects of these covariates will be removed from the\n            variable of interest, and the harmonized values will be returned.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n\n    Returns:\n        npt.NDArray[np.floating[Any]]: Array of harmonized values for the\n            variable of interest.\n    \"\"\"\n    # Validate the new data\n    validation_columns = [cov.name for cov in self.spec.covariates]\n    validation_columns.append(self.spec.variable_of_interest)\n    utils.general.validate_dataframe(data, validation_columns)\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Predict the mean and std with all covariates\n    full_predictions = self.predict(\n        test_covariates=data,\n        model_params=model_params,\n        predict_without=[],\n    )\n\n    # Predict the mean and std without the covariates to harmonize\n    reduced_predictions = self.predict(\n        test_covariates=data,\n        model_params=model_params,\n        predict_without=covariates_to_harmonize,\n    )\n\n    # First standardize the variable of interest based on the full model\n    voi_standardized = (\n        (cast(\"npt.NDArray[Any]\", data[self.spec.variable_of_interest].to_numpy()))\n        - full_predictions.predictions[\"mu_estimate\"]\n    ) / full_predictions.predictions[\"std_estimate\"]\n\n    # Then return the harmonized values based on the reduced model\n    return np.asarray(\n        (\n            voi_standardized * reduced_predictions.predictions[\"std_estimate\"]\n            + reduced_predictions.predictions[\"mu_estimate\"]\n        ),\n        dtype=np.float64,\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.load_model","title":"<code>load_model(directory: Path, *, load_posterior: bool = False) -&gt; DirectNormativeModel</code>  <code>classmethod</code>","text":"<p>Load the model and its posterior from a directory. The model will be loaded from a subdirectory named 'saved_model'.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to the directory containing the model.</p> required <code>load_posterior</code> <code>bool</code> <p>bool (default=False) If True, load the model's posterior trace from the saved inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    directory: Path,\n    *,\n    load_posterior: bool = False,\n) -&gt; DirectNormativeModel:\n    \"\"\"\n    Load the model and its posterior from a directory.\n    The model will be loaded from a subdirectory named 'saved_model'.\n\n    Args:\n        directory: Path\n            Path to the directory containing the model.\n        load_posterior: bool (default=False)\n            If True, load the model's posterior trace from the saved inference data.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"saved_model\",\n    )\n\n    # Load the saved model dict\n    model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n    # Create an instance of the class\n    instance = cls(\n        spec=model_dict[\"spec\"],\n    )\n\n    # Set the attributes from the loaded model dictionary\n    instance.defaults.update(model_dict[\"defaults\"])\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n        if load_posterior:\n            instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n\n    return instance\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.predict","title":"<code>predict(test_covariates: pd.DataFrame, *, extended: bool = False, model_params: dict[str, Any] | None = None, predict_without: list[str] | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict normative moments (mean, std) for new data using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates. Note: covariates listed in predict_without will be ignored and are hence not required.</p> required <code>extended</code> <code>bool</code> <p>bool If True, return additional stats such as log-likelihood, centiles, etc. Note that extended predictions require variable_of_interest to be provided in the test_covariates DataFrame.</p> <code>False</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>predict_without</code> <code>list[str] | None</code> <p>list[str] | None Optional list of covariate names to ignore during prediction. This can be used to check the effect of removing certain covariates from the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted moments (mean, std) for the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    test_covariates: pd.DataFrame,\n    *,\n    extended: bool = False,\n    model_params: dict[str, Any] | None = None,\n    predict_without: list[str] | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict normative moments (mean, std) for new data using the fitted model.\n\n    Args:\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n            Note: covariates listed in predict_without will be ignored and are\n            hence not required.\n        extended: bool\n            If True, return additional stats such as log-likelihood, centiles, etc.\n            Note that extended predictions require variable_of_interest to be\n            provided in the test_covariates DataFrame.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        predict_without: list[str] | None\n            Optional list of covariate names to ignore during prediction.\n            This can be used to check the effect of removing certain covariates\n            from the model.\n\n    Returns:\n        NormativePredictions: Object containing the predicted moments (mean, std)\n            for the variable of interest.\n    \"\"\"\n    # Validate the new data\n    validation_columns = [\n        cov.name\n        for cov in self.spec.covariates\n        if cov.name not in (predict_without or [])\n    ]\n    if extended:\n        validation_columns.append(self.spec.variable_of_interest)\n    utils.general.validate_dataframe(test_covariates, validation_columns)\n\n    # Parameters\n    model_params = model_params or self.model_params\n    if model_params is None:\n        err = \"No model parameters found. Please provide model_params.\"\n        raise ValueError(err)\n\n    # Calculate mean and variance effects and store in the predictions object\n    predictions = NormativePredictions(\n        {\n            \"mu_estimate\": self._predict_mu(\n                test_covariates,\n                model_params,\n                (predict_without or []),\n            ),\n            \"std_estimate\": self._predict_std(\n                test_covariates,\n                model_params,\n                (predict_without or []),\n            ),\n        },\n    )\n\n    # Check if extended predictions are requested\n    if extended:\n        # Add extended statistics to predictions (e.g. centiles, log loss, etc.)\n        predictions.extend_predictions(\n            variable_of_interest=(\n                cast(\n                    \"npt.NDArray[Any]\",\n                    test_covariates[self.spec.variable_of_interest].to_numpy(),\n                )\n            ),\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.DirectNormativeModel.save_model","title":"<code>save_model(directory: Path, *, save_posterior: bool = False) -&gt; None</code>","text":"<p>Save the fitted model and it's posterior to a directory. The model will be saved in a subdirectory named 'saved_model'. If this directory is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to a directory to save the model.</p> required <code>save_posterior</code> <code>bool</code> <p>bool (default=False) If True, save the model's posterior trace inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n    \"\"\"\n    Save the fitted model and it's posterior to a directory.\n    The model will be saved in a subdirectory named 'saved_model'.\n    If this directory is not empty, an error is raised.\n\n    Args:\n        directory: Path\n            Path to a directory to save the model.\n        save_posterior: bool (default=False)\n            If True, save the model's posterior trace inference data.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n    model_dict = {\n        \"spec\": self.spec,\n        \"defaults\": self.defaults,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n        if hasattr(self, \"model_inference_data\") and save_posterior:\n            self.model_inference_data.to_netcdf(\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n    joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativeModelSpec","title":"<code>NormativeModelSpec</code>  <code>dataclass</code>","text":"<p>General specification of a normative model.</p> <p>Attributes:</p> Name Type Description <code>variable_of_interest</code> <code>str</code> <p>str Name of the target variable to model (e.g., \"thickness\").</p> <code>covariates</code> <code>list[CovariateSpec]</code> <p>list[CovariateSpec] Listing all model covariates and specifying how each covariate is modeled.</p> <code>influencing_mean</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the mean of the variable of interest.</p> <code>influencing_variance</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the variance of the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass NormativeModelSpec:\n    \"\"\"\n    General specification of a normative model.\n\n    Attributes:\n        variable_of_interest: str\n            Name of the target variable to model (e.g., \"thickness\").\n        covariates: list[CovariateSpec]\n            Listing all model covariates and specifying how each covariate is modeled.\n        influencing_mean: list[str]\n            List of covariate names that influence the mean of the variable of interest.\n        influencing_variance: list[str]\n            List of covariate names that influence the variance of the variable of\n            interest.\n    \"\"\"\n\n    variable_of_interest: str\n    covariates: list[CovariateSpec]\n    influencing_mean: list[str]\n    influencing_variance: list[str]\n\n    def __post_init__(self) -&gt; None:\n        if not isinstance(self.variable_of_interest, str):\n            err = \"variable_of_interest must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.covariates, list):\n            err = \"covariates must be a list of CovariateSpec instances.\"\n            raise TypeError(err)\n        if not all(isinstance(cov, CovariateSpec) for cov in self.covariates):\n            err = \"All items in covariates must be CovariateSpec instances.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_mean, list):\n            err = \"influencing_mean must be a list of covariate names.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_variance, list):\n            err = \"influencing_variance must be a list of covariate names.\"\n            raise TypeError(err)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativePredictions","title":"<code>NormativePredictions</code>  <code>dataclass</code>","text":"<p>Container for the results of model.predict() function.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>dict[str, NDArray[floating[Any]]]</code> <p>dict Dictionary containing the model's predictions, including - Predictions of mean (mu_estimate). - Predictions of standard deviation (std_estimate). - [Optional] The observed variable of interest (the name of which is   provided in the function argument). - [Optional] Additional evaluation metrics for the predictions.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass NormativePredictions:\n    \"\"\"\n    Container for the results of model.predict() function.\n\n    Attributes:\n        predictions: dict\n            Dictionary containing the model's predictions, including\n            - Predictions of mean (mu_estimate).\n            - Predictions of standard deviation (std_estimate).\n            - [Optional] The observed variable of interest (the name of which is\n              provided in the function argument).\n            - [Optional] Additional evaluation metrics for the predictions.\n    \"\"\"\n\n    predictions: dict[str, npt.NDArray[np.floating[Any]]]\n    evaluations: dict[str, npt.NDArray[np.floating[Any]] | float] = field(\n        default_factory=dict,\n    )\n\n    def extend_predictions(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n        *,\n        likelihood_censoring_quantile: float = 0.01,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Extend the NormativePredictions (predictions dictionary) with additional\n        statistics.\n\n        Args:\n            variable_of_interest: np.ndarray\n                The observed values for the variable(s) of interest.\n            likelihood_censoring_quantile: float (default=0.01)\n                Quantile below which log-likelihoods are censored for evaluation.\n                Note: by default, a censored log-likelihood is computed to avoid\n                extreme log-likelihood values for outliers. This affects several\n                resulting metrics that are based on log-likelihoods (e.g. MSLL).\n                If you want to compute the full (uncensored) log-likelihoods, set\n                `likelihood_censoring_quantile` to 0.\n\n        Returns:\n            NormativePredictions\n                Extended NormativePredictions with additional statistics.\n        \"\"\"\n        self.predictions[\"z-score\"] = (\n            variable_of_interest - self.predictions[\"mu_estimate\"]\n        ) / self.predictions[\"std_estimate\"]\n        self.predictions[\"log-likelihood\"] = (\n            utils.stats.compute_censored_log_likelihood(\n                variable_of_interest,\n                self.predictions[\"mu_estimate\"],\n                self.predictions[\"std_estimate\"],\n                censored_quantile=likelihood_censoring_quantile,\n            )\n        )\n        self.predictions[\"centiles\"] = utils.stats.compute_centiles_from_z_scores(\n            self.predictions[\"z-score\"],\n        )\n\n        self.predictions[\"variable_of_interest\"] = variable_of_interest\n\n        return self\n\n    def evaluate_predictions(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n        train_mean: npt.NDArray[np.floating[Any]],\n        train_std: npt.NDArray[np.floating[Any]],\n        n_params: int | None = None,\n        msll_censored_quantile: float = 0.01,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the predictions against the observed variable of interest.\n\n        This function computes a battery of evaluation metrics implemented\n        in `snm.utils.metrics`. Namely the evaluations include:\n            - Mean Absolute Error (MAE)\n            - Mean Squared Error (MSE)\n            - Root Mean Squared Error (RMSE)\n            - Mean Absolute Percentage Error (MAPE)\n            - R-squared\n            - Explained Variance Score\n            - Mean Standardized Log Loss (MSLL)\n\n        Args:\n            variable_of_interest: np.ndarray\n                The observed values for the variable(s) of interest.\n            train_mean: np.ndarray\n                Mean(s) of the variable(s) of interest from the training data.\n            train_std: np.ndarray\n                Standard deviation(s) of the variable(s) of interest from the training\n                data.\n            n_params: int\n                Number of free parameters in the model.\n            msll_censored_quantile: float (default=0.02)\n                Quantile below which log-likelihoods are censored for MSLL.\n\n        Returns:\n            NormativePredictions\n                Object containing the evaluation results.\n        \"\"\"\n        self.extend_predictions(\n            variable_of_interest,\n            likelihood_censoring_quantile=msll_censored_quantile,\n        )\n        # Mean Absolute Error (MAE)\n        self.evaluations[\"MAE\"] = utils.metrics.compute_mae(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Squared Error (MSE)\n        self.evaluations[\"MSE\"] = utils.metrics.compute_mse(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Root Mean Squared Error (RMSE)\n        self.evaluations[\"RMSE\"] = utils.metrics.compute_rmse(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Absolute Percentage Error (MAPE)\n        self.evaluations[\"MAPE\"] = utils.metrics.compute_mape(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # R-squared\n        self.evaluations[\"R-squared\"] = utils.metrics.compute_r2(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Explained Variance Score\n        self.evaluations[\"Explained Variance\"] = utils.metrics.compute_expv(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Standardized Log Loss (MSLL)\n        self.evaluations[\"MSLL\"] = utils.metrics.compute_msll(\n            model_log_likelihoods=self.predictions[\"log-likelihood\"],\n            baseline_log_likelihoods=utils.stats.compute_censored_log_likelihood(\n                self.predictions[\"variable_of_interest\"],\n                train_mean,\n                train_std,\n                censored_quantile=msll_censored_quantile,\n            ),\n        )\n        _ = n_params  # keep for future use (e.g. information criteria calculations)\n\n        return self\n\n    def to_array(self, keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Return prediction results as a list of NumPy arrays.\n\n        Args:\n            keys: list[str]\n                Optional list of keys to return.\n                Defaults to [\"mu_estimate\", \"std_estimate\"].\n\n        Returns:\n            list[np.ndarray]\n                NumPy arrays for the requested predictions\n        \"\"\"\n        keys = keys or [\"mu_estimate\", \"std_estimate\"]\n        return np.array([self.predictions[key] for key in keys])\n\n    def to_dataframe(\n        self,\n        index: pd.Index[Any] | list[Any] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return prediction results as a DataFrame.\n\n        Args:\n            index: pd.Index | list | None\n                Optional index for the DataFrame (defaults to None)\n\n        Returns:\n            pd.DataFrame\n                DataFrame containing the predictions\n        \"\"\"\n        predictions = self.predictions.copy()\n        # Flatten the predictions dictionary if multiple queries are predicted\n        for key in predictions:\n            if predictions[key].ndim &gt; 1:\n                if predictions[key].shape[1] == 1:\n                    predictions[key] = predictions[key].flatten()\n                else:\n                    for i in range(predictions[key].shape[1]):\n                        predictions[f\"{key}_{i + 1}\"] = predictions[key][:, i]\n                    # delete the key\n                    del predictions[key]\n\n        # Make a new DataFrame for the predictions dictionary\n        return pd.DataFrame(predictions, index=index)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativePredictions.evaluate_predictions","title":"<code>evaluate_predictions(variable_of_interest: npt.NDArray[np.floating[Any]], train_mean: npt.NDArray[np.floating[Any]], train_std: npt.NDArray[np.floating[Any]], n_params: int | None = None, msll_censored_quantile: float = 0.01) -&gt; NormativePredictions</code>","text":"<p>Evaluate the predictions against the observed variable of interest.</p> <p>This function computes a battery of evaluation metrics implemented in <code>snm.utils.metrics</code>. Namely the evaluations include:     - Mean Absolute Error (MAE)     - Mean Squared Error (MSE)     - Root Mean Squared Error (RMSE)     - Mean Absolute Percentage Error (MAPE)     - R-squared     - Explained Variance Score     - Mean Standardized Log Loss (MSLL)</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The observed values for the variable(s) of interest.</p> required <code>train_mean</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean(s) of the variable(s) of interest from the training data.</p> required <code>train_std</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Standard deviation(s) of the variable(s) of interest from the training data.</p> required <code>n_params</code> <code>int | None</code> <p>int Number of free parameters in the model.</p> <code>None</code> <code>msll_censored_quantile</code> <code>float</code> <p>float (default=0.02) Quantile below which log-likelihoods are censored for MSLL.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>NormativePredictions Object containing the evaluation results.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate_predictions(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n    train_mean: npt.NDArray[np.floating[Any]],\n    train_std: npt.NDArray[np.floating[Any]],\n    n_params: int | None = None,\n    msll_censored_quantile: float = 0.01,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the predictions against the observed variable of interest.\n\n    This function computes a battery of evaluation metrics implemented\n    in `snm.utils.metrics`. Namely the evaluations include:\n        - Mean Absolute Error (MAE)\n        - Mean Squared Error (MSE)\n        - Root Mean Squared Error (RMSE)\n        - Mean Absolute Percentage Error (MAPE)\n        - R-squared\n        - Explained Variance Score\n        - Mean Standardized Log Loss (MSLL)\n\n    Args:\n        variable_of_interest: np.ndarray\n            The observed values for the variable(s) of interest.\n        train_mean: np.ndarray\n            Mean(s) of the variable(s) of interest from the training data.\n        train_std: np.ndarray\n            Standard deviation(s) of the variable(s) of interest from the training\n            data.\n        n_params: int\n            Number of free parameters in the model.\n        msll_censored_quantile: float (default=0.02)\n            Quantile below which log-likelihoods are censored for MSLL.\n\n    Returns:\n        NormativePredictions\n            Object containing the evaluation results.\n    \"\"\"\n    self.extend_predictions(\n        variable_of_interest,\n        likelihood_censoring_quantile=msll_censored_quantile,\n    )\n    # Mean Absolute Error (MAE)\n    self.evaluations[\"MAE\"] = utils.metrics.compute_mae(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Squared Error (MSE)\n    self.evaluations[\"MSE\"] = utils.metrics.compute_mse(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Root Mean Squared Error (RMSE)\n    self.evaluations[\"RMSE\"] = utils.metrics.compute_rmse(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Absolute Percentage Error (MAPE)\n    self.evaluations[\"MAPE\"] = utils.metrics.compute_mape(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # R-squared\n    self.evaluations[\"R-squared\"] = utils.metrics.compute_r2(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Explained Variance Score\n    self.evaluations[\"Explained Variance\"] = utils.metrics.compute_expv(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Standardized Log Loss (MSLL)\n    self.evaluations[\"MSLL\"] = utils.metrics.compute_msll(\n        model_log_likelihoods=self.predictions[\"log-likelihood\"],\n        baseline_log_likelihoods=utils.stats.compute_censored_log_likelihood(\n            self.predictions[\"variable_of_interest\"],\n            train_mean,\n            train_std,\n            censored_quantile=msll_censored_quantile,\n        ),\n    )\n    _ = n_params  # keep for future use (e.g. information criteria calculations)\n\n    return self\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativePredictions.extend_predictions","title":"<code>extend_predictions(variable_of_interest: npt.NDArray[np.floating[Any]], *, likelihood_censoring_quantile: float = 0.01) -&gt; NormativePredictions</code>","text":"<p>Extend the NormativePredictions (predictions dictionary) with additional statistics.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The observed values for the variable(s) of interest.</p> required <code>likelihood_censoring_quantile</code> <code>float</code> <p>float (default=0.01) Quantile below which log-likelihoods are censored for evaluation. Note: by default, a censored log-likelihood is computed to avoid extreme log-likelihood values for outliers. This affects several resulting metrics that are based on log-likelihoods (e.g. MSLL). If you want to compute the full (uncensored) log-likelihoods, set <code>likelihood_censoring_quantile</code> to 0.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>NormativePredictions Extended NormativePredictions with additional statistics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def extend_predictions(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n    *,\n    likelihood_censoring_quantile: float = 0.01,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Extend the NormativePredictions (predictions dictionary) with additional\n    statistics.\n\n    Args:\n        variable_of_interest: np.ndarray\n            The observed values for the variable(s) of interest.\n        likelihood_censoring_quantile: float (default=0.01)\n            Quantile below which log-likelihoods are censored for evaluation.\n            Note: by default, a censored log-likelihood is computed to avoid\n            extreme log-likelihood values for outliers. This affects several\n            resulting metrics that are based on log-likelihoods (e.g. MSLL).\n            If you want to compute the full (uncensored) log-likelihoods, set\n            `likelihood_censoring_quantile` to 0.\n\n    Returns:\n        NormativePredictions\n            Extended NormativePredictions with additional statistics.\n    \"\"\"\n    self.predictions[\"z-score\"] = (\n        variable_of_interest - self.predictions[\"mu_estimate\"]\n    ) / self.predictions[\"std_estimate\"]\n    self.predictions[\"log-likelihood\"] = (\n        utils.stats.compute_censored_log_likelihood(\n            variable_of_interest,\n            self.predictions[\"mu_estimate\"],\n            self.predictions[\"std_estimate\"],\n            censored_quantile=likelihood_censoring_quantile,\n        )\n    )\n    self.predictions[\"centiles\"] = utils.stats.compute_centiles_from_z_scores(\n        self.predictions[\"z-score\"],\n    )\n\n    self.predictions[\"variable_of_interest\"] = variable_of_interest\n\n    return self\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativePredictions.to_array","title":"<code>to_array(keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Return prediction results as a list of NumPy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str] | None</code> <p>list[str] Optional list of keys to return. Defaults to [\"mu_estimate\", \"std_estimate\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>list[np.ndarray] NumPy arrays for the requested predictions</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def to_array(self, keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Return prediction results as a list of NumPy arrays.\n\n    Args:\n        keys: list[str]\n            Optional list of keys to return.\n            Defaults to [\"mu_estimate\", \"std_estimate\"].\n\n    Returns:\n        list[np.ndarray]\n            NumPy arrays for the requested predictions\n    \"\"\"\n    keys = keys or [\"mu_estimate\", \"std_estimate\"]\n    return np.array([self.predictions[key] for key in keys])\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.NormativePredictions.to_dataframe","title":"<code>to_dataframe(index: pd.Index[Any] | list[Any] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Return prediction results as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index[Any] | list[Any] | None</code> <p>pd.Index | list | None Optional index for the DataFrame (defaults to None)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the predictions</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def to_dataframe(\n    self,\n    index: pd.Index[Any] | list[Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return prediction results as a DataFrame.\n\n    Args:\n        index: pd.Index | list | None\n            Optional index for the DataFrame (defaults to None)\n\n    Returns:\n        pd.DataFrame\n            DataFrame containing the predictions\n    \"\"\"\n    predictions = self.predictions.copy()\n    # Flatten the predictions dictionary if multiple queries are predicted\n    for key in predictions:\n        if predictions[key].ndim &gt; 1:\n            if predictions[key].shape[1] == 1:\n                predictions[key] = predictions[key].flatten()\n            else:\n                for i in range(predictions[key].shape[1]):\n                    predictions[f\"{key}_{i + 1}\"] = predictions[key][:, i]\n                # delete the key\n                del predictions[key]\n\n    # Make a new DataFrame for the predictions dictionary\n    return pd.DataFrame(predictions, index=index)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel","title":"<code>SpectralNormativeModel</code>  <code>dataclass</code>","text":"<p>Spectral normative model implementation.</p> <p>This class implements the spectral normative modeling approach, which utilizes a base direct model to generalize normative modeling to any arbitrary variable of interest reconstructed from a graph spectral embedding. It can be used to fit a normative model to high-dimensional data and predict normative centiles for arbitrary variables of interest.</p> <p>Attributes:</p> Name Type Description <code>eigenmode_basis</code> <code>EigenmodeBasis</code> <p>utils.gsp.EigenmodeBasis The eigenmode basis used for spectral normative modeling. This should be an instance of utils.gsp.EigenmodeBasis.</p> <code>base_model</code> <code>DirectNormativeModel</code> <p>DirectNormativeModel The base (direct) normative model used for spectral normative modeling.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass SpectralNormativeModel:\n    \"\"\"\n    Spectral normative model implementation.\n\n    This class implements the spectral normative modeling approach, which\n    utilizes a base direct model to generalize normative modeling to any\n    arbitrary variable of interest reconstructed from a graph spectral\n    embedding. It can be used to fit a normative model to high-dimensional\n    data and predict normative centiles for arbitrary variables of interest.\n\n    Attributes:\n        eigenmode_basis: utils.gsp.EigenmodeBasis\n            The eigenmode basis used for spectral normative modeling. This should be an\n            instance of utils.gsp.EigenmodeBasis.\n        base_model: DirectNormativeModel\n            The base (direct) normative model used for spectral normative modeling.\n    \"\"\"\n\n    eigenmode_basis: utils.gsp.EigenmodeBasis\n    base_model: DirectNormativeModel\n\n    @classmethod\n    def build_from_dataframe(\n        cls,\n        eigenmode_basis: utils.gsp.EigenmodeBasis,\n        model_type: ModelType,\n        covariates_dataframe: pd.DataFrame,\n        numerical_covariates: list[str] | None = None,\n        categorical_covariates: list[str] | None = None,\n        batch_covariates: list[str] | None = None,\n        nonlinear_covariates: list[str] | None = None,\n        influencing_mean: list[str] | None = None,\n        influencing_variance: list[str] | None = None,\n        spline_kwargs: dict[str, Any] | None = None,\n    ) -&gt; SpectralNormativeModel:\n        \"\"\"\n        Initialize SNM with an eigenmode basis and a base direct model built from a\n        pandas DataFrame containing all covariates.\n\n        This uses the from_dataframe method of the DirectNormativeModel class\n        to populate the direct model specification of SNM. Given that SNM does not\n        require a fixed variable of interest, this method assigns a dummy name\n        to the variable_of_interest parameter of the DirectNormativeModel. As such,\n        the provided dataframe should not contain a column with \"dummy_VOI\" as name.\n\n        Essentially, the provided dataframe should contain all covariates as columns.\n\n        Args:\n            eigenmode_basis: utils.gsp.EigenmodeBasis\n                The eigenmode basis to be used for spectral normative modeling.\n            model_type: ModelType\n                Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n                Regression) or \"BLR\" (Bayesian Linear Regression).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the data for all covariates and all samples.\n            numerical_covariates: list[str] | None\n                List of numerical covariate names.\n            categorical_covariates: list[str] | None\n                List of categorical covariate names.\n            batch_covariates: list[str] | None\n                List of batch covariate names which should also be included in\n                categorical_covariates.\n            nonlinear_covariates: list[str] | None\n                List of covariate names to be modeled as nonlinear effects.\n                These should also be included in numerical_covariates.\n            influencing_mean: list[str] | None\n                List of covariate names that influence the mean of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            influencing_variance: list[str] | None\n                List of covariate names that influence the variance of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            spline_kwargs: dict\n                Additional keyword arguments for spline specification, such as\n                `df`, `degree`, and `knots`. These are passed to the\n                `create_spline_spec` method to create spline specifications for\n                nonlinear covariates.\n\n        Returns:\n            SpectralNormativeModel\n                An instance of SpectralNormativeModel with base model specs initialized\n                based on the provided data.\n        \"\"\"\n        # Add a dummy variable of interest to the covariates_dataframe\n        covariates_dataframe = covariates_dataframe.copy()\n        covariates_dataframe[\"dummy_VOI\"] = 0.0  # Dummy variable of interest\n        # Specify the base model from the dataframe\n        return cls(\n            eigenmode_basis=eigenmode_basis,\n            base_model=DirectNormativeModel.from_dataframe(\n                model_type=model_type,\n                dataframe=covariates_dataframe,\n                variable_of_interest=\"dummy_VOI\",  # Dummy variable of interest\n                numerical_covariates=(numerical_covariates or []),\n                categorical_covariates=(categorical_covariates or []),\n                batch_covariates=(batch_covariates or []),\n                nonlinear_covariates=(nonlinear_covariates or []),\n                influencing_mean=(influencing_mean or []),\n                influencing_variance=(influencing_variance or []),\n                spline_kwargs=(spline_kwargs or {}),\n            ),\n        )\n\n    def save_model(self, directory: Path) -&gt; None:\n        \"\"\"\n        Save the fitted spectral normative model to the specified directory.\n\n        Args:\n            directory: Path\n                Directory to save the fitted model. A subdirectory named\n                \"spectral_normative_model\" will be created within this directory.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(\n            directory,\n            \"spectral_normative_model\",\n        )\n\n        # Save the eigenmode basis separately\n        self.eigenmode_basis.save(str(saved_model_dir / \"eigenmode_basis.joblib\"))\n\n        # Save the model\n        model_dict = {\n            \"spec\": self.base_model.spec,\n            \"defaults\": self.base_model.defaults,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n        joblib.dump(model_dict, saved_model_dir / \"spectral_model_dict.joblib\")\n\n    @classmethod\n    def load_model(\n        cls,\n        directory: Path,\n        mmap_mode: MmapMode | None = \"r\",\n    ) -&gt; SpectralNormativeModel:\n        \"\"\"\n        Load a spectral normative model instance from the specified save directory.\n\n        Args:\n            directory: Path\n                Directory to load the fitted model from. A subdirectory named\n                \"spectral_normative_model\" will be searched within this directory.\n            mmap_mode: MmapMode | None\n                Memory mapping mode for joblib (default: \"r\").\n                You can set this to None to disable memory-mapping.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"spectral_normative_model\",\n        )\n\n        # Check if the pickled joblib file exists in this directory\n        for filename in [\"spectral_model_dict.joblib\", \"eigenmode_basis.joblib\"]:\n            if not (saved_model_dir / filename).exists():\n                err = f\"Model Load Error: Required file '{filename}' does not exist.\"\n                raise FileNotFoundError(err)\n\n        # Load the pickled model dictionary\n        model_dict = joblib.load(saved_model_dir / \"spectral_model_dict.joblib\")\n\n        # Load the eigenmode basis\n        eigenmode_basis = utils.gsp.EigenmodeBasis.load(\n            str(saved_model_dir / \"eigenmode_basis.joblib\"),\n            mmap_mode=mmap_mode,\n        )\n\n        # Create an instance of the class\n        instance = cls(\n            eigenmode_basis=eigenmode_basis,\n            base_model=DirectNormativeModel(\n                spec=model_dict[\"spec\"],\n                defaults=model_dict[\"defaults\"],\n            ),\n        )\n\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n\n        return instance\n\n    def _validate_fit_input(\n        self,\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        n_modes: int,\n    ) -&gt; None:\n        \"\"\"\n        Internal method to validate input data for fitting the spectral normative model.\n        \"\"\"\n        # Validate the input data\n        if not isinstance(spectral_coeff_train_data, np.ndarray):\n            err = \"spectral_coeff_train_data must be a numpy array.\"\n            raise TypeError(err)\n        if spectral_coeff_train_data.shape[1] &lt; n_modes:\n            err = (\n                f\"spectral_coeff_train_data must have at least\"\n                f\" {n_modes} columns (n_modes).\"\n            )\n            raise ValueError(err)\n        if self.eigenmode_basis.n_modes &lt; n_modes:\n            err = (\n                f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n                f\" modes, while {n_modes} were requested.\"\n            )\n            raise ValueError(err)\n\n    def identify_sparse_covariance_structure(\n        self,\n        data: npt.NDArray[np.floating[Any]],\n        sparsity_threshold: float = 1,\n    ) -&gt; npt.NDArray[np.integer[Any]]:\n        \"\"\"\n        Identify the sparse cross-basis covariance structure in the phenotype.\n        This method analyzes the phenotype's spectral coefficients to determine the\n        covariance pairs that need to be modeled.\n\n        Note: if the batches become too small, this estimate can become less stable\n        in which case it is recommended to provide the sparse covariance structure\n        to the model instead.\n\n        Args:\n            data: np.ndarray\n                Spectral coefficients of training data representing the phenotype in\n                the graph frequency domain\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            sparsity_threshold: float\n                Number of strongest correlations to keep (proportional to the number\n                of modes). Defaults to 1, meaning that the number of sparse covariance\n                pairs will be equal to the number of modes. If set to a lower value,\n                fewer covariance pairs will be retained.\n\n        Returns:\n            np.ndarray:\n                A (N, 2) array: the rows and columns of the\n                identified sparse covariance structure.\n        \"\"\"\n        # Start with correlation structure across the whole sample\n        correlations = np.corrcoef(data.T)\n\n        # Remove self-correlations\n        np.fill_diagonal(correlations, 0)\n\n        # Extract the upper triangle of the correlation matrix\n        upper_triangle_indices = np.triu_indices(correlations.shape[0], k=1)\n\n        # Determine the number of correlations to keep\n        n_correlations_to_keep = int(\n            sparsity_threshold * correlations.shape[0],\n        )\n\n        # Find the cutoff value for the top correlations\n        if n_correlations_to_keep &lt; len(upper_triangle_indices[0]):\n            cutoff_value = np.partition(\n                np.abs(correlations[upper_triangle_indices]),\n                -n_correlations_to_keep,\n            )[-n_correlations_to_keep]\n        else:\n            cutoff_value = 0\n            # Warn the user if they are keeping all correlations\n            logger.warning(\n                \"Sparsity threshold is high, keeping all correlations.\",\n            )\n\n        # Now compute the sparsity structure based on the resulting matrix\n        rows, cols = np.where(np.abs(correlations) &gt; cutoff_value)\n\n        # Remove redundant and duplicate pairs\n        rows_lim = rows[rows &lt; cols]\n        cols_lim = cols[rows &lt; cols]\n\n        return np.array([rows_lim, cols_lim]).T\n\n    @staticmethod\n    def _is_valid_covariance_structure(\n        covariance_structure: npt.NDArray[np.integer[Any]] | float,\n    ) -&gt; bool:\n        \"\"\"\n        Verify the validity of the sparse covariance structure.\n        \"\"\"\n        # Check it's a 2D array with two columns\n        expected_ndims = 2\n        expected_ncols = 2\n        if not (\n            isinstance(covariance_structure, np.ndarray)\n            and covariance_structure.ndim == expected_ndims\n            and covariance_structure.shape[1] == expected_ncols\n        ):\n            return False\n        return np.issubdtype(covariance_structure.dtype, np.integer)\n\n    def fit_single_direct(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        return_model_params: bool = True,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Fit a direct normative model for a single spectral eigenmode.\n        This method fits the base direct model to the provided variable of interest\n        and covariates dataframe, allowing for the model to be trained on a specific\n        eigenmode of the spectral embedding.\n\n        Args:\n            variable_of_interest: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a single eigenmode.\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n            return_model_params: bool\n                If True, return the fitted model parameters.\n            adapt: dict[str, Any] | None\n                Adaptation parameters from a previously fitted model. If provided,\n                the model will be adapted using these parameters during fitting.\n\n        Returns:\n            dict:\n                If `return_model_params` is True, return the fitted model parameters\n                in a dictionary.\n        \"\"\"\n        # Prepare the data for fitting\n        train_data = covariates_dataframe.copy()\n        # Add the mode loading as the variable of interest\n        train_data[\"VOI\"] = variable_of_interest\n\n        # Instantiate a direct normative model from the base model\n        direct_model = DirectNormativeModel(\n            spec=NormativeModelSpec(\n                variable_of_interest=\"VOI\",  # Use the added VOI column\n                covariates=self.base_model.spec.covariates,\n                influencing_mean=self.base_model.spec.influencing_mean,\n                influencing_variance=self.base_model.spec.influencing_variance,\n            ),\n            defaults=self.base_model.defaults,\n        )\n\n        # Fit the model silently\n        with utils.general.suppress_output():\n            direct_model.fit(\n                train_data=train_data,\n                save_directory=save_directory,\n                progress_bar=False,\n                adapt=adapt,\n            )\n\n        # Return the fitted model parameters if requested\n        if return_model_params:\n            return direct_model.model_params\n\n        # If not returning model parameters, return None\n        return None\n\n    def fit_single_covariance(\n        self,\n        variable_of_interest_1: npt.NDArray[np.floating[Any]],\n        variable_of_interest_2: npt.NDArray[np.floating[Any]],\n        direct_model_params_1: dict[str, Any],\n        direct_model_params_2: dict[str, Any],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        return_model_params: bool = True,\n        defaults_overwrite: dict[str, Any] | None = None,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Fit a covariance normative model between a single pair of eigenmodes.\n        This method fits a covariance model to the provided pair of variables\n        and covariates dataframe, considering the direct model fits for each\n        eigenmode, while allowing for the cross-eigenmode covariance to vary\n        normatively.\n\n        Args:\n            variable_of_interest_1: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a single eigenmode.\n            variable_of_interest_2: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a second eigenmode.\n            direct_model_params_1: dict\n                The parameters of the direct model fitted to the first eigenmode.\n            direct_model_params_2: dict\n                The parameters of the direct model fitted to the second eigenmode.\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n            return_model_params: bool\n                If True, return the fitted model parameters.\n            defaults_overwrite: dict (default={})\n                Dictionary of default values to overwrite in the model fitting process.\n            adapt: dict[str, Any] | None = None\n                Adaptation parameters from a previously fitted model. If provided,\n                the model will be adapted using these parameters during fitting.\n\n        Returns:\n            dict:\n                If `return_model_params` is True, return the fitted model parameters\n                in a dictionary.\n        \"\"\"\n        # Prepare the data for fitting\n        train_data = covariates_dataframe.copy()\n        # Add the respective mode loadings as the variables of interest\n        train_data[\"VOI_1\"] = variable_of_interest_1\n        train_data[\"VOI_2\"] = variable_of_interest_2\n        train_data[[\"VOI_1_mu_estimate\", \"VOI_1_std_estimate\"]] = (\n            self.base_model.predict(\n                train_data,\n                model_params=direct_model_params_1,\n            )\n            .to_array()\n            .T\n        )  # Add the direct model predictions\n        train_data[[\"VOI_2_mu_estimate\", \"VOI_2_std_estimate\"]] = (\n            self.base_model.predict(\n                train_data,\n                model_params=direct_model_params_2,\n            )\n            .to_array()\n            .T\n        )  # Add the direct model predictions\n\n        # Instantiate a covariance normative model from the base model\n        covariance_model = CovarianceNormativeModel.from_direct_model(\n            self.base_model,\n            variable_of_interest_1=\"VOI_1\",\n            variable_of_interest_2=\"VOI_2\",\n            defaults_overwrite=(defaults_overwrite or {}),\n        )\n\n        # Fit the model silently\n        with utils.general.suppress_output():\n            covariance_model.fit(\n                train_data=train_data,\n                save_directory=save_directory,\n                progress_bar=False,\n                adapt=adapt,\n            )\n\n        # Return the fitted model parameters if requested\n        if return_model_params:\n            return covariance_model.model_params\n\n        # If not returning model parameters, return None\n        return None\n\n    def fit_all_direct(\n        self,\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_modes: int = -1,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the direct models for all specified eigenmodes.\n\n        Args:\n            spectral_coeff_train_data: np.ndarray\n                Spectral coefficients of training data\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_modes: int (default=-1)\n                Number of eigenmodes to fit the model to. If -1, all modes are\n                used. If a positive integer, only the first n_modes are used.\n                Note that the spectral_coeff_train_data and the eigenmode basis should\n                have at least n_modes columns/eigenvectors.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n            adapt: dict[str, Any] | None\n                Adaptation parameters from a previously fitted model. If provided,\n                the model will be adapted using these parameters during fitting.\n        \"\"\"\n        # Setup the save directory if needed\n        if save_directory is not None:\n            save_directory = Path(save_directory)\n\n        # Evaluate the number of modes to fit\n        if n_modes == -1:\n            n_modes = self.eigenmode_basis.n_modes\n\n        # Fit the base direct model for each eigenmode using parallel processing\n        tasks = (\n            joblib.delayed(self.fit_single_direct)(\n                variable_of_interest=spectral_coeff_train_data[:, i],\n                covariates_dataframe=covariates_dataframe,\n                save_directory=(\n                    utils.general.ensure_dir(\n                        save_directory\n                        / \"spectral_normative_model\"\n                        / \"direct_models\"\n                        / f\"mode_{i + 1}\",\n                    )\n                    if save_directory is not None and save_separate\n                    else None\n                ),\n                adapt=(\n                    None\n                    if adapt is None\n                    else {\n                        \"covariate_to_adapt\": adapt[\"covariate_to_adapt\"],\n                        \"new_category_names\": adapt[\"new_category_names\"],\n                        \"pretrained_model_params\": adapt[\"pretrained_model_params\"][\n                            \"direct_model_params\"\n                        ][i],\n                    }\n                ),\n            )\n            for i in range(n_modes)\n        )\n        self.direct_model_params = list(\n            utils.parallel.ParallelTqdm(\n                n_jobs=n_jobs,\n                total_tasks=n_modes,\n                desc=\"Fitting direct models\",\n            )(tasks),  # pyright: ignore[reportCallIssue]\n        )\n\n    def identify_covariance_structure(\n        self,\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        n_modes: int,\n        covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Identify and set the sparse covariance structure for the spectral normative\n        model based on the provided training data and covariance structure input.\n\n        Args:\n            spectral_coeff_train_data: np.ndarray\n                Spectral coefficients of training data\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n            n_modes: int\n                Number of eigenmodes to consider.\n            covariance_structure: np.ndarray | float\n                Sparse covariance structure to use for the model fitting. If a\n                (2, n_pairs) array of row and column indices are provided, the model\n                will use this structure. If float, the model will estimate the\n                covariance structure based on the training data and the float value\n                will be used as the sparsity threshold for the number of covariance\n                pairs to keep proportional to the number of modes. Defaults to 0.5,\n                meaning that the number of modeled sparse covariance pairs will be\n                half the number of modes.\n            adapt: dict[str, Any] | None\n                Adaptation parameters from a previously fitted model. If provided,\n                the sparse covariance structure from the pretrained model parameters\n                will be used instead of estimating a new one.\n        \"\"\"\n        if adapt is not None:\n            covariance_structure = adapt[\"pretrained_model_params\"][\n                \"sparse_covariance_structure\"\n            ]\n\n        # Identify sparse covariance structure if a float value is given\n        if isinstance(covariance_structure, float):\n            # Use trained models to compute z-scores\n            spectral_train_z_scores = np.array(\n                [\n                    self.base_model.predict(\n                        test_covariates=covariates_dataframe,\n                        model_params=self.direct_model_params[x],\n                    )\n                    .extend_predictions(\n                        variable_of_interest=spectral_coeff_train_data[:, x],\n                    )\n                    .predictions[\"z-score\"]\n                    for x in range(n_modes)\n                ],\n            ).T\n\n            self.sparse_covariance_structure = (\n                self.identify_sparse_covariance_structure(\n                    spectral_train_z_scores,\n                    covariance_structure,\n                )\n            )\n        else:\n            self.sparse_covariance_structure = np.array(covariance_structure)\n\n        # Verify that the covariance structure is valid\n        if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n            err = \"Invalid sparse covariance structure.\"\n            raise ValueError(err)\n\n    def fit_all_covariance(\n        self,\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the direct models for all specified eigenmodes.\n\n        Args:\n            spectral_coeff_train_data: np.ndarray\n                Spectral coefficients of training data\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n            adapt: dict[str, Any] | None\n                Adaptation parameters from a previously fitted model. If provided,\n                the model will be adapted using these parameters during fitting.\n        \"\"\"\n        # Setup the save directory if needed\n        if save_directory is not None:\n            save_directory = Path(save_directory)\n\n        # Fit the base covariance models for selected eigenmode pairs in parallel\n        tasks = (\n            joblib.delayed(self.fit_single_covariance)(\n                variable_of_interest_1=spectral_coeff_train_data[\n                    :,\n                    self.sparse_covariance_structure[i, 0],\n                ],\n                variable_of_interest_2=spectral_coeff_train_data[\n                    :,\n                    self.sparse_covariance_structure[i, 1],\n                ],\n                direct_model_params_1=self.direct_model_params[\n                    self.sparse_covariance_structure[i, 0]\n                ],\n                direct_model_params_2=self.direct_model_params[\n                    self.sparse_covariance_structure[i, 1]\n                ],\n                covariates_dataframe=covariates_dataframe,\n                save_directory=(\n                    utils.general.ensure_dir(\n                        save_directory\n                        / \"spectral_normative_model\"\n                        / \"covariance_models\"\n                        / (\n                            f\"mode_{self.sparse_covariance_structure[i, 0] + 1},\"\n                            f\"mode_{self.sparse_covariance_structure[i, 1] + 1}\"\n                        ),\n                    )\n                    if save_directory is not None and save_separate\n                    else None\n                ),\n                adapt=(\n                    None\n                    if adapt is None\n                    else {\n                        \"covariate_to_adapt\": adapt[\"covariate_to_adapt\"],\n                        \"new_category_names\": adapt[\"new_category_names\"],\n                        \"pretrained_model_params\": adapt[\"pretrained_model_params\"][\n                            \"covariance_model_params\"\n                        ][i],\n                    }\n                ),\n            )\n            for i in range(self.sparse_covariance_structure.shape[0])\n        )\n        self.covariance_model_params = utils.parallel.ParallelTqdm(\n            n_jobs=n_jobs,\n            total_tasks=self.sparse_covariance_structure.shape[0],\n            desc=\"Fitting covariance models\",\n        )(tasks)  # pyright: ignore[reportCallIssue]\n\n    def fit(\n        self,\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_modes: int = -1,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n        covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5,\n        adapt: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the spectral normative model to the provided spectral coefficient\n        training data.\n\n        Args:\n            spectral_coeff_train_data: np.ndarray\n                Spectral coefficients of training data\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_modes: int (default=-1)\n                Number of eigenmodes to fit the model to. If -1, all modes are\n                used. If a positive integer, only the first n_modes are used.\n                Note that the spectral_coeff_train_data and the eigenmode basis should\n                have at least n_modes columns/eigenvectors.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n            covariance_structure: np.ndarray | float\n                Sparse covariance structure to use for the model fitting. If a\n                (2, n_pairs) array of row and column indices are provided, the model\n                will use this structure. If float, the model will estimate the\n                covariance structure based on the training data and the float value\n                will be used as the sparsity threshold for the number of covariance\n                pairs to keep proportional to the number of modes. Defaults to 0.5,\n                meaning that the number of modeled sparse covariance pairs will be\n                half the number of modes.\n                Note: If using a small number of nodes, it is advisable to increase\n                the sparsity threshold to ensure a stable estimation of the covariance\n                structure. In contrast, when using a large number of nodes, a lower\n                sparsity threshold should be used to ensure sparse modeling of the\n                covariance structure.\n            adapt: dict[str, Any] | None (default=None)\n                If provided, adapt a pre-trained model to a new covariate.\n                Note: We recommended using the `adapt_fit` method, and not directly\n                changing this argument, unless you know what you are doing.\n        \"\"\"\n        logger.info(\"Starting SNM model fitting:\")\n        # Evaluate the number of modes to fit\n        if n_modes == -1:\n            n_modes = self.eigenmode_basis.n_modes\n        # Validate the input data\n        if not isinstance(spectral_coeff_train_data, np.ndarray):\n            err = \"spectral_coeff_train_data must be a numpy array.\"\n            raise TypeError(err)\n        if spectral_coeff_train_data.shape[1] &lt; n_modes:\n            err = (\n                f\"spectral_coeff_train_data must have at least {n_modes}\"\n                \" columns (n_modes).\"\n            )\n            raise ValueError(err)\n        if self.eigenmode_basis.n_modes &lt; n_modes:\n            err = (\n                f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n                f\" modes, while {n_modes} were requested.\"\n            )\n            raise ValueError(err)\n\n        # Setup the save directory if needed\n        if save_directory is not None:\n            # Prepare the save directory\n            save_directory = Path(save_directory)\n            utils.general.prepare_save_directory(\n                save_directory,\n                \"spectral_normative_model\",\n            )\n\n        logger.info(\"Step 1; direct models for each eigenmode (%s modes)\", n_modes)\n\n        self.fit_all_direct(\n            spectral_coeff_train_data=spectral_coeff_train_data,\n            covariates_dataframe=covariates_dataframe,\n            n_modes=n_modes,\n            n_jobs=n_jobs,\n            save_directory=save_directory,\n            save_separate=save_separate,\n            adapt=adapt,\n        )\n\n        logger.info(\"Step 2; identify sparse covariance structure\")\n\n        self.identify_covariance_structure(\n            spectral_coeff_train_data=spectral_coeff_train_data,\n            covariates_dataframe=covariates_dataframe,\n            n_modes=n_modes,\n            covariance_structure=covariance_structure,\n            adapt=adapt,\n        )\n\n        # Verify that the covariance structure is valid\n        if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n            err = \"Invalid sparse covariance structure.\"\n            raise ValueError(err)\n\n        # Model cross basis sparse covariance structure\n        logger.info(\n            \"Step 3; cross-eigenmode dependency modeling (%s pairs)\",\n            self.sparse_covariance_structure.shape[0],\n        )\n\n        self.fit_all_covariance(\n            spectral_coeff_train_data=spectral_coeff_train_data,\n            covariates_dataframe=covariates_dataframe,\n            n_jobs=n_jobs,\n            save_directory=save_directory,\n            save_separate=save_separate,\n            adapt=adapt,\n        )\n\n        # Save SNM model parameters\n        sample_size = spectral_coeff_train_data.shape[0]\n        if adapt is not None:\n            sample_size += adapt[\"pretrained_model_params\"][\"sample_size\"]\n        self.model_params = {\n            \"n_modes\": n_modes,\n            \"sample_size\": sample_size,\n            \"direct_model_params\": self.direct_model_params,\n            \"sparse_covariance_structure\": self.sparse_covariance_structure,\n            \"covariance_model_params\": self.covariance_model_params,\n        }\n        if (self.direct_model_params[0] is not None) and (\n            \"n_params\" in self.direct_model_params[0]\n        ):\n            self.model_params[\"n_params\"] = self.direct_model_params[0][\"n_params\"]\n        else:\n            err = \"Direct model parameters are not valid.\"\n            raise ValueError(err)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(save_directory)\n\n    def adapt_fit(\n        self,\n        covariate_to_adapt: str,\n        new_category_names: npt.NDArray[np.str_],\n        spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        pretrained_model_params: dict[str, Any] | None = None,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Using a previously fitted spectral normative model, adapt to a new\n        batch.\n        This method enables adaptation (fine-tuning) of the model to data\n        from a new batch/site by freezing all fitted parameters, and only\n        estimating new parameters for the new batch/site category.\n\n        Args:\n            covariate_to_adapt: str\n                Name of the categorical covariate representing the batch/site\n                to which the model should be adapted.\n                Note: This covariate must have been specified in the original\n                model.\n            new_category_names: list[str]\n                Names of the new categories in the covariate_to_adapt representing\n                the new batch/site labels (e.g. names of the new site).\n                Note: These names must not have been present in the original\n                fitted model.\n            spectral_coeff_train_data: np.ndarray\n                Spectral coefficients of training data\n                :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n                as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n                Note: The covariate_to_adapt column must only contain the\n                new_category_names (no new data from previously trained batches).\n            pretrained_model_params: dict[str, Any] | None\n                The model parameters from a previously fitted model to adapt.\n                If None, the model parameters from the current instance will be used\n                (assuming fitting was done).\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                A path to a directory to save the adapted model. If provided,\n                the fitted model will be saved to this path.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n        \"\"\"\n        # Locate the covariate to adapt\n        cov_to_adapt_index = [\n            cov.name for cov in self.base_model.spec.covariates\n        ].index(covariate_to_adapt)\n\n        # Extend the covariate categories to include the new categories\n        self.base_model.spec.covariates[cov_to_adapt_index].extend_categories(\n            new_category_names,\n        )\n\n        # Extract the pre-trained model parameters\n        if pretrained_model_params is None:\n            if not hasattr(self, \"model_params\") or self.model_params is None:\n                err = (\n                    \"No pretrained model parameters found. \"\n                    \"Please provide pretrained_model_params or fit the model first.\"\n                )\n                raise ValueError(err)\n            pretrained_model_params = copy.deepcopy(self.model_params)\n\n        # Fit the adapted model\n        self.fit(\n            spectral_coeff_train_data,\n            covariates_dataframe,\n            n_modes=pretrained_model_params[\"n_modes\"],\n            n_jobs=n_jobs,\n            save_directory=save_directory,\n            save_separate=save_separate,\n            covariance_structure=pretrained_model_params[\"sparse_covariance_structure\"],\n            adapt={\n                \"covariate_to_adapt\": covariate_to_adapt,\n                \"new_category_names\": new_category_names,\n                \"pretrained_model_params\": pretrained_model_params,\n            },\n        )\n\n    def _predict_from_spectral_estimates(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        eigenmode_mu_estimates: npt.NDArray[np.floating[Any]],\n        eigenmode_std_estimates: npt.NDArray[np.floating[Any]],\n        rho_estimates: npt.NDArray[np.floating[Any]],\n        model_params: dict[str, Any],\n        n_modes: int,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Internal method to predict only the mean and sigma for new data using the fitted\n        spectral moments.\n        \"\"\"\n        # Constrain mu and std estimates to the number of modes\n        eigenmode_mu_estimates = eigenmode_mu_estimates[:, :n_modes]\n        eigenmode_std_estimates = eigenmode_std_estimates[:, :n_modes]\n\n        # Prepare the predictions\n        predictions_dict = {}\n        predictions_dict[\"mu_estimate\"] = eigenmode_mu_estimates @ encoded_query\n\n        # Load sparse covariance structure\n        row_indices = model_params[\"sparse_covariance_structure\"][:, 0]\n        col_indices = model_params[\"sparse_covariance_structure\"][:, 1]\n\n        # Select indices that are both within n_modes\n        corr_index_valid = (row_indices &lt; n_modes) &amp; (col_indices &lt; n_modes)\n\n        # Mask valid values\n        valid_rho_estimates = rho_estimates[:, corr_index_valid]\n        valid_row_indices = row_indices[corr_index_valid]\n        valid_col_indices = col_indices[corr_index_valid]\n\n        # number of dimensions\n        n_samples = eigenmode_mu_estimates.shape[0]\n        n_queries = encoded_query.shape[1]\n\n        # Pre-compute squared encoded query\n        # to have shape of (n_modes, n_queries)\n        # Avoids recomputing for every sample\n        encoded_query_squared = encoded_query**2\n\n        # Empty matrix to fill in loop\n        sample_query_stds = np.empty((n_samples, n_queries), dtype=rho_estimates.dtype)\n\n        # Implement the equivalent of sparse matrix multiplications methodically\n        # to avoid large memory consumption and optimize speed\n        # sparse multiplication requires (n_samples, n_modes, n_queries) memory\n        for sample_idx in range(n_samples):\n            # Build weighted mode stds on the fly \u2014 small memory (n_modes, n_queries)\n            weighted_mode_stds_sample = (\n                eigenmode_std_estimates[sample_idx, :, None] * encoded_query\n            )\n\n            # diagonal term (within mode variances)\n            diagonal_term = (\n                eigenmode_std_estimates[sample_idx] ** 2\n            ) @ encoded_query_squared\n            # off-diagonal term (cross-mode covariances)\n            # weighted stds for valid pairs (n_edges, n_queries)\n            weighted_mode_stds_sample_row = weighted_mode_stds_sample[valid_row_indices]\n            weighted_mode_stds_sample_col = weighted_mode_stds_sample[valid_col_indices]\n            off_diagonal_term = 2 * (\n                valid_rho_estimates[sample_idx, :, None]\n                * (weighted_mode_stds_sample_row * weighted_mode_stds_sample_col)\n            ).sum(axis=0)\n            # avoid negative values due to numerical issues\n            # if it happens, ignore off-diagonal term\n            try:\n                sample_query_stds[sample_idx] = np.sqrt(\n                    diagonal_term + off_diagonal_term,\n                )\n            except RuntimeWarning:\n                sample_query_stds[sample_idx] = np.sqrt(diagonal_term)\n\n        predictions_dict[\"std_estimate\"] = sample_query_stds\n\n        # Create a the predictions object\n        return NormativePredictions(predictions=predictions_dict)\n\n    @staticmethod\n    def _predict_single_mode_estimates(\n        direct_model_spec: NormativeModelSpec,\n        direct_model_defaults: dict[str, Any],\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        predict_without: list[str] | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict single mode estimates for new data using the fitted\n        spectral normative model.\n        \"\"\"\n        # Instantiate a direct normative model from the base model\n        direct_model = DirectNormativeModel(\n            spec=NormativeModelSpec(\n                variable_of_interest=\"VOI\",  # Use the added VOI column\n                covariates=direct_model_spec.covariates,\n                influencing_mean=direct_model_spec.influencing_mean,\n                influencing_variance=direct_model_spec.influencing_variance,\n            ),\n            defaults=direct_model_defaults,\n        )\n        return (\n            direct_model.predict(\n                test_covariates,\n                model_params=model_params,\n                predict_without=predict_without,\n            )\n            .to_array([\"mu_estimate\", \"std_estimate\"])\n            .T\n        )\n\n    def _predict_all_mode_estimates(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        n_modes: int,\n        n_jobs: int = -1,\n        predict_without: list[str] | None = None,\n    ) -&gt; tuple[\n        npt.NDArray[np.floating[Any]],\n        npt.NDArray[np.floating[Any]],\n    ]:\n        \"\"\"\n        Internal method to predict all direct estimates for new data using the fitted\n        spectral normative model.\n        \"\"\"\n        # direct normative predictions for each eigenmode\n        tasks = (\n            joblib.delayed(self._predict_single_mode_estimates)(\n                self.base_model.spec,\n                self.base_model.defaults,\n                test_covariates,\n                model_params=direct_model_params,\n                predict_without=predict_without,\n            )\n            for direct_model_params in model_params[\"direct_model_params\"][:n_modes]\n        )\n\n        results = list(\n            utils.parallel.ParallelTqdm(\n                n_jobs=n_jobs,\n                total_tasks=n_modes,\n                desc=\"Computing direct eigenmode estimates\",\n            )(tasks),  # pyright: ignore[reportCallIssue]\n        )\n\n        # Unpack results, estimates have a shape of (n_samples, n_modes)\n        eigenmode_mu_estimates, eigenmode_std_estimates = np.array(results).T\n\n        return eigenmode_mu_estimates, eigenmode_std_estimates\n\n    @staticmethod\n    def _predict_single_covariance_estimates(\n        covariance_model_spec: CovarianceModelSpec,\n        covariance_model_defaults: dict[str, Any],\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        predict_without: list[str] | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict single covariance estimates for new data using the\n        fitted spectral normative model.\n        \"\"\"\n        # create a dummy covariance model\n        covariance_model = CovarianceNormativeModel(\n            spec=CovarianceModelSpec(\n                variable_of_interest_1=\"VOI_1\",\n                variable_of_interest_2=\"VOI_2\",\n                covariates=covariance_model_spec.covariates,\n                influencing_covariance=covariance_model_spec.influencing_covariance,\n            ),\n            defaults=covariance_model_defaults,\n        )\n        return (\n            covariance_model.predict(\n                test_covariates,\n                model_params=model_params,\n                predict_without=predict_without,\n            )\n            .to_array([\"correlation_estimate\"])\n            .T\n        )\n\n    def _predict_all_covariance_estimates(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        n_modes: int,\n        n_jobs: int = -1,\n        predict_without: list[str] | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict all covariance estimates for new data using the\n        fitted spectral normative model.\n        \"\"\"\n        # create a dummy covariance model\n        covariance_model = CovarianceNormativeModel.from_direct_model(\n            self.base_model,\n            variable_of_interest_1=\"dummy_VOI_1\",  # Dummy variable of interest\n            variable_of_interest_2=\"dummy_VOI_2\",  # Dummy variable of interest\n        )\n\n        # Check sparse covariance structure\n        row_indices = model_params[\"sparse_covariance_structure\"][:, 0]\n        col_indices = model_params[\"sparse_covariance_structure\"][:, 1]\n        # Select indices that are within n_modes\n        corr_index_valid = (row_indices &lt; n_modes) &amp; (col_indices &lt; n_modes)\n\n        # cross-mode dependence structure for valid pairs\n        tasks = (\n            joblib.delayed(self._predict_single_covariance_estimates)(\n                covariance_model.spec,\n                covariance_model.defaults,\n                test_covariates,\n                model_params=covariance_model_params,\n                predict_without=predict_without,\n            )\n            for i, covariance_model_params in enumerate(\n                model_params[\"covariance_model_params\"],\n            )\n            if corr_index_valid[i]\n        )\n\n        results = list(\n            utils.parallel.ParallelTqdm(\n                n_jobs=n_jobs,\n                total_tasks=np.sum(corr_index_valid),\n                desc=\"Computing cross-mode dependence estimates\",\n            )(tasks),  # pyright: ignore[reportCallIssue]\n        )\n\n        # Unpack results, (n_samples, n_valid_covariance_pairs)\n        valid_rho_estimates = np.array(results).T[0]\n\n        # Now fill in the full set of rho estimates with NaNs for the invalid pairs\n        rho_estimates = np.full(\n            (\n                test_covariates.shape[0],\n                model_params[\"sparse_covariance_structure\"].shape[0],\n            ),\n            np.nan,\n        )\n        rho_estimates[:, corr_index_valid] = valid_rho_estimates\n        # final estimates have a shape of (n_samples, n_covariance_pairs)\n\n        return rho_estimates\n\n    def compute_spectral_predictions(\n        self,\n        test_covariates: pd.DataFrame,\n        *,\n        model_params: dict[str, Any] | None = None,\n        n_modes: int | None = None,\n        n_jobs: int = -1,\n        predict_without: list[str] | None = None,\n    ) -&gt; dict[str, npt.NDArray[np.floating[Any]]]:\n        \"\"\"\n        Predict normative moments (mean, std) of the eigenmode basis for new data\n        using the fitted spectral normative model.\n\n        This function requires a dataframe of covariates (test_covariates) to compute\n        a set of spectral predictions that can subsequently be combined to efficiently\n        estimate normative predictions for any query(ies).\n\n        Args:\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n                Note: covariates listed in predict_without will be ignored and are\n                hence not required.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the number of modes from model_params will be used.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to utilize. If -1, all available CPU cores are\n                used. If 1, no parallelization is used.\n            predict_without: list[str] | None\n                Optional list of covariate names to ignore during prediction.\n                This can be used to check the effect of removing certain covariates\n                from the model.\n\n        Returns:\n            dict:\n                A dictionary containing:\n                - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n                - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n                - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n        \"\"\"\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(model_params[\"n_modes\"])\n\n        if self.base_model.spec is None:\n            err = \"The base model is not specified. Cannot predict new data.\"\n            raise ValueError(err)\n\n        # Validate the covariate data\n        validation_columns = [\n            cov.name\n            for cov in self.base_model.spec.covariates\n            if cov.name not in (predict_without or [])\n        ]\n        utils.general.validate_dataframe(test_covariates, validation_columns)\n\n        # direct normative predictions for each eigenmode\n        (\n            eigenmode_mu_estimates,\n            eigenmode_std_estimates,\n        ) = self._predict_all_mode_estimates(\n            test_covariates,\n            model_params,\n            n_modes,\n            n_jobs=n_jobs,\n            predict_without=predict_without,\n        )  # estimates have a shape of (n_samples, n_modes)\n\n        # cross-mode dependence structure\n        rho_estimates = self._predict_all_covariance_estimates(\n            test_covariates,\n            model_params,\n            n_modes,\n            n_jobs=n_jobs,\n            predict_without=predict_without,\n        )  # estimates have a shape of (n_samples, n_covariance_pairs)\n\n        return {\n            \"eigenmode_mu_estimates\": eigenmode_mu_estimates,\n            \"eigenmode_std_estimates\": eigenmode_std_estimates,\n            \"rho_estimates\": rho_estimates,\n        }\n\n    def _validate_spectral_predictions(\n        self,\n        spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]],\n    ) -&gt; None:\n        \"\"\"\n        Internal method to validate the spectral predictions dictionary.\n        \"\"\"\n        required_keys = [\n            \"eigenmode_mu_estimates\",\n            \"eigenmode_std_estimates\",\n            \"rho_estimates\",\n        ]\n        if not all(key in spectral_predictions for key in required_keys):\n            err = (\n                \"spectral_predictions must contain 'eigenmode_mu_estimates',\"\n                \" 'eigenmode_std_estimates', and 'rho_estimates'.\"\n            )\n            raise ValueError(err)\n\n    def predict(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        *,\n        spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None,\n        test_covariates: pd.DataFrame | None = None,\n        extended: bool = False,\n        model_params: dict[str, Any] | None = None,\n        spectral_coeff_test_data: npt.NDArray[np.floating[Any]] | None = None,\n        n_modes: int | None = None,\n        predict_without: list[str] | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict normative moments (mean, std) for new data using the fitted spectral\n        normative model.\n        Spectral normative modeling can estimate the normative distribution of any\n        variable of interest defined as a spatial query encoded in the latent low-pass\n        graph spectral space.\n\n        As such, the predict method requires:\n            - The encoded query(ies) defining the variable(s) of interest.\n\n        In addition, the method requires either:\n            - A dataframe of covariates (test_covariates) to be used for inference\n              of a set of spectral predictions that will subsequently be combined\n              to yield the normative predictions for the encoded query(ies).\n            OR\n            - A dictionary of precomputed spectral predictions (spectral_predictions)\n              to be used for efficiently predicting the encoded query(ies).\n\n        The precomputed spectral predictions can be obtained using the\n        'compute_spectral_predictions' function. This is particularly useful when\n        predicting multiple queries or when the same covariate set is used for\n        multiple predictions, as it avoids redundant computations.\n\n        Args:\n            encoded_query: np.ndarray\n                Encoded query data defining the normative variable of interest.\n                Can be provided as:\n                - shape = (n_modes) for a single query vector\n                - shape = (n_modes, n_queries) for multiple queries predicted at once\n            spectral_predictions: dict | None\n                Optional dictionary of precomputed spectral predictions to use for\n                the prediction. If not provided, test_covariates must be provided\n                instead to compute the spectral predictions.\n                The dictionary should contain:\n                - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n                - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n                - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n                This can be obtained using the 'compute_spectral_predictions' method.\n            test_covariates: pd.DataFrame | None\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n                Note: covariates listed in predict_without will be ignored and are\n                hence not required.\n            extended: bool (default: False)\n                If True, return additional stats such as log-likelihood, centiles, etc.\n                Note that extended predictions require spectral_coeff_test_data to be\n                provided in addition to the covariates.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            spectral_coeff_test_data: np.ndarray | None\n                Optional spectral coefficient of test data for the phenotype being\n                modeled :math:`(T_{test} \\\\Psi_{(k)}) \\\\in R^{N_{test} \\\\times k}`\n                (only required for extended predictions).\n                Expects a numpy array (n_samples, n_modes)\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the number of modes from model_params will be used.\n            predict_without: list[str] | None\n                Optional list of covariate names to ignore during prediction.\n                This can be used to check the effect of removing certain covariates\n                from the model.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the predicted moments (mean, std) for\n                the variable of interest defined by the encoded query.\n        \"\"\"\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(model_params[\"n_modes\"])\n\n        if self.base_model.spec is None:\n            err = \"The base model is not specified. Cannot predict new data.\"\n            raise ValueError(err)\n\n        if spectral_predictions is None:\n            if test_covariates is None:\n                err = \"Either test_covariates or spectral_predictions must be provided.\"\n                raise ValueError(err)\n\n            # Compute spectral predictions if not provided\n            spectral_predictions = self.compute_spectral_predictions(\n                test_covariates=test_covariates,\n                model_params=model_params,\n                n_modes=n_modes,\n                predict_without=predict_without,\n            )\n        elif test_covariates is not None:\n            logger.warning(\n                \"Both test_covariates and spectral_predictions are provided.\"\n                \" Ignoring test_covariates and using spectral_predictions.\",\n            )\n            if predict_without is not None:\n                logger.warning(\n                    \"predict_without is ignored when spectral_predictions\"\n                    \" are provided directly.\",\n                )\n\n        # Unpack spectral predictions\n        self._validate_spectral_predictions(spectral_predictions)\n        eigenmode_mu_estimates = spectral_predictions[\"eigenmode_mu_estimates\"]\n        eigenmode_std_estimates = spectral_predictions[\"eigenmode_std_estimates\"]\n        rho_estimates = spectral_predictions[\"rho_estimates\"]\n\n        # Reformat encoded queries (for efficiency)\n        encoded_query = np.asarray(encoded_query[:n_modes])\n        encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n        # Compute the predictions\n        predictions = self._predict_from_spectral_estimates(\n            encoded_query=encoded_query,\n            eigenmode_mu_estimates=eigenmode_mu_estimates,\n            eigenmode_std_estimates=eigenmode_std_estimates,\n            rho_estimates=rho_estimates,\n            model_params=model_params,\n            n_modes=n_modes,\n        )\n\n        # Check if extended predictions are requested\n        if extended:\n            if spectral_coeff_test_data is None:\n                err = (\n                    \"Extended predictions require spectral_coeff_test_data\"\n                    \" to be provided.\"\n                )\n                raise ValueError(err)\n            # Add extended statistics to predictions (e.g. centiles, log-loss, etc.)\n            predictions.extend_predictions(\n                variable_of_interest=spectral_coeff_test_data @ encoded_query,\n            )\n\n        return predictions\n\n    def evaluate(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        spectral_coeff_test_data: npt.NDArray[np.floating[Any]],\n        *,\n        spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None,\n        test_covariates: pd.DataFrame | None = None,\n        query_train_moments: npt.NDArray[np.floating[Any]] | None = None,\n        model_params: dict[str, Any] | None = None,\n        n_modes: int | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the model on new data and return predictions along with evaluation\n        metrics.\n\n        Args:\n            encoded_query: np.ndarray\n                Encoded query data defining the normative variable of interest.\n                Can be provided as:\n                - shape = (n_modes) for a single query vector\n                - shape = (n_modes, n_queries) for multiple queries predicted at once\n            spectral_coeff_test_data: np.ndarray | None\n                Spectral coefficient of test data for the phenotype being modeled\n                :math:`(T_{test} \\\\Psi_{(k)}) \\\\in R^{N_{test} \\\\times k}`\n                (only required for extended predictions).\n                Expects a numpy array (n_samples, n_modes)\n            spectral_predictions: dict | None\n                Optional dictionary of precomputed spectral predictions to use for\n                the evaluation. If not provided, test_covariates must be provided\n                instead to compute the spectral predictions.\n                The dictionary should contain:\n                - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n                - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n                - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n                This can be obtained using the 'compute_spectral_predictions' method.\n            test_covariates: pd.DataFrame | None\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n                Note: This is only required if spectral_predictions was not provided.\n            query_train_moments: np.ndarray | None\n                A (2, n_queries) array containing the query moments (mean, std) directly\n                measured in the training data. While optional, providing these moments\n                is strongly recommended for accurate evaluation of the model's MSLL.\n                If not provided, the model will use the test data moments as an\n                approximation, which may lead to overestimating MSLL. This is made\n                optional to allow evaluating MSLL when the training data is not\n                accessible (e.g. using a pre-trained model).\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the stored number of modes from model.fit() will be used.\n\n        Returns:\n            NormativePredictions:\n                Object containing the predicted moments (mean, std) for\n                the variable of interest defined by the encoded query, along with\n                evaluation metrics.\n        \"\"\"\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(self.model_params[\"n_modes\"])\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Reformat encoded queries (for efficiency)\n        encoded_query = np.asarray(encoded_query[:n_modes])\n        encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n        # Run extended predictions\n        predictions = self.predict(\n            encoded_query=encoded_query,\n            spectral_predictions=spectral_predictions,\n            test_covariates=test_covariates,\n            extended=True,\n            model_params=model_params,\n            spectral_coeff_test_data=spectral_coeff_test_data,\n            n_modes=n_modes,\n        )\n        if query_train_moments is None:\n            logger.warning(\n                \"Query moments not provided. Using test data moments as an\"\n                \" approximation, which may lead to overestimating MSLL.\",\n            )\n            query_train_moments = np.array(\n                [\n                    np.mean(spectral_coeff_test_data @ encoded_query, axis=0),\n                    np.std(spectral_coeff_test_data @ encoded_query, axis=0, ddof=1),\n                ],\n            )\n        return predictions.evaluate_predictions(\n            variable_of_interest=spectral_coeff_test_data @ encoded_query,\n            train_mean=query_train_moments[0],\n            train_std=query_train_moments[1],\n            n_params=model_params[\"n_params\"],\n        )\n\n    def harmonize(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        spectral_coeff_data: npt.NDArray[np.floating[Any]],\n        *,\n        covariates_to_harmonize: list[str] | None = None,\n        covariates_dataframe: pd.DataFrame | None = None,\n        spectral_predictions_full: dict[str, npt.NDArray[np.floating[Any]]]\n        | None = None,\n        spectral_predictions_partial: dict[str, npt.NDArray[np.floating[Any]]]\n        | None = None,\n        model_params: dict[str, Any] | None = None,\n        n_modes: int | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Harmonize the variables of interest in the data to remove effects of\n        certain covariates (e.g. batch). This method uses the spectral normative model\n        to harmonize one or several variables of interest defined by the encoded query.\n\n        The harmonization method can be used in two ways:\n        - By providing a dataframe of covariates (covariates_dataframe) to compute the\n          necessary spectral predictions for both the full model (all covariates)\n          and the partial model (excluding covariates to harmonize). In this format,\n          you should also provide the covariates_to_harmonize (list of covariate names).\n        - By providing precomputed spectral predictions for both the full and partial\n          models (spectral_predictions_full and spectral_predictions_partial). In this\n          format, the partial spectral predictions should have been computed by\n          excluding the covariates to harmonize using the predict_without parameter in\n          the compute_spectral_predictions method.\n          Note: In the latter, the method will not use the covariates_to_harmonize list.\n\n        Args:\n            encoded_query: np.ndarray\n                Encoded query data defining the normative variable of interest.\n                Can be provided as:\n                - shape = (n_modes) for a single query vector\n                - shape = (n_modes, n_queries) for multiple queries predicted at once\n            spectral_coeff_data: np.ndarray | None\n                Spectral coefficient of the the phenotype being modeled\n                :math:`(T \\\\Psi_{(k)}) \\\\in R^{N_{p} \\\\times k}`.\n                Expects a numpy array (n_samples, n_modes)\n            covariates_to_harmonize: list[str] | None\n                List of covariate names to harmonize.\n                The partial effects of these covariates will be removed from the\n                variable of interest, and the harmonized values will be returned.\n                Note: This is only required if spectral_predictions_full and\n                spectral_predictions_partial were not provided.\n            covariates_dataframe: pd.DataFrame | None\n                DataFrame containing covariate information for the data to harmonize.\n                This must include all specified covariates. The dataframe is expected\n                to have all covariates as columns and samples as rows.\n                Note: This is only required if spectral_predictions_full and\n                spectral_predictions_partial were not provided. Alternatively,\n                if any of the aforementioned spectral predictions were previously\n                computed, then they could be passed to this method to avoid\n                recomputation.\n            spectral_predictions_full: dict | None\n                Optional dictionary of precomputed spectral predictions to use for\n                the harmonization. If not provided, covariates_dataframe must be\n                provided instead to compute the spectral predictions.\n                These predictions use all set of covariates.\n                The dictionary should contain:\n                - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n                - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n                - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n                This can be obtained using the 'compute_spectral_predictions' method.\n            spectral_predictions_partial: dict | None\n                Optional dictionary of precomputed spectral predictions to use for\n                the harmonization. If not provided, covariates_dataframe must be\n                provided instead to compute the partial spectral predictions.\n                These predictions use all set of covariates except those to harmonize.\n                The covariates to harmonize need to be partialed out using the\n                predict_without parameter.\n                The dictionary should contain:\n                - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n                - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n                - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n                This can be obtained using the 'compute_spectral_predictions' method.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the stored number of modes from model.fit() will be used.\n\n        Returns:\n            npt.NDArray[np.floating[Any]]: Array of harmonized values for the\n                variable of interest.\n        \"\"\"\n        if (spectral_predictions_full is None) or (\n            spectral_predictions_partial is None\n        ):\n            if (covariates_dataframe is None) or (covariates_to_harmonize is None):\n                err = (\n                    \"Either [covariates_dataframe and covariates_to_harmonize] or \"\n                    \"both spectral_predictions_full \"\n                    \"and spectral_predictions_partial must be provided.\"\n                )\n                raise ValueError(err)\n            # Validate the new data\n            validation_columns = [cov.name for cov in self.base_model.spec.covariates]\n            utils.general.validate_dataframe(covariates_dataframe, validation_columns)\n\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(self.model_params[\"n_modes\"])\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Reformat encoded queries (for efficiency)\n        encoded_query = np.asarray(encoded_query[:n_modes])\n        encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n        # Predict the mean and std with all covariates\n        full_predictions = self.predict(\n            encoded_query=encoded_query,\n            spectral_predictions=spectral_predictions_full,\n            test_covariates=covariates_dataframe,\n            model_params=model_params,\n            n_modes=n_modes,\n            predict_without=[],\n        )\n\n        # Predict the mean and std without the covariates to harmonize\n        reduced_predictions = self.predict(\n            encoded_query=encoded_query,\n            spectral_predictions=spectral_predictions_partial,\n            test_covariates=covariates_dataframe,\n            model_params=model_params,\n            n_modes=n_modes,\n            predict_without=covariates_to_harmonize,\n        )\n\n        # Reconstruct observed phenotype for query from spectral coefficients\n        observed_phenotype = spectral_coeff_data @ encoded_query[:n_modes]\n\n        # First standardize the variable of interest based on the full model\n        vois_standardized = (\n            observed_phenotype - full_predictions.predictions[\"mu_estimate\"]\n        ) / full_predictions.predictions[\"std_estimate\"]\n\n        # Then return the harmonized values based on the reduced model\n        return np.asarray(\n            (\n                vois_standardized * reduced_predictions.predictions[\"std_estimate\"]\n                + reduced_predictions.predictions[\"mu_estimate\"]\n            ),\n            dtype=np.float64,\n        )\n\n    def reduce_model(\n        self,\n        n_modes: int,\n        *,\n        inplace: bool = False,\n    ) -&gt; SpectralNormativeModel:\n        \"\"\"\n        Create a reduced spectral normative model using only the first n_modes.\n\n        Args:\n            n_modes: int\n                Number of modes to retain in the reduced model. Must be less than or\n                equal to the current number of modes considered by the model.\n            inplace: bool (default: False)\n                If True, modify the current model instance to reduce its modes. If\n                False, return a new SpectralNormativeModel instance with the reduced\n                modes.\n        Returns:\n            SpectralNormativeModel\n                A new SpectralNormativeModel instance with reduced number of modes.\n        \"\"\"\n        if (n_modes &gt; self.eigenmode_basis.n_modes) or (\n            hasattr(self, \"model_params\") and n_modes &gt; self.model_params[\"n_modes\"]\n        ):\n            available_modes = self.eigenmode_basis.n_modes\n            if hasattr(self, \"model_params\"):\n                available_modes = min(available_modes, self.model_params[\"n_modes\"])\n            err = f\"Cannot reduce to {n_modes} modes, only {available_modes} available.\"\n            raise ValueError(err)\n\n        # Create a reduced eigenbasis\n        reduced_eigenbasis = self.eigenmode_basis.reduce(n_modes)\n\n        if inplace:\n            return_model = self\n            return_model.eigenmode_basis = reduced_eigenbasis\n        else:\n            return_model = SpectralNormativeModel(\n                base_model=self.base_model,\n                eigenmode_basis=reduced_eigenbasis,\n            )\n            return_model.model_params = self.model_params  # Copy model parameters\n\n        # Update model parameters to reflect reduced modes\n        if hasattr(self, \"model_params\"):\n            new_model_params: dict[str, Any] = {}\n            new_model_params[\"n_modes\"] = n_modes\n            new_model_params[\"sample_size\"] = self.model_params[\"sample_size\"]\n            new_model_params[\"direct_model_params\"] = self.model_params[\n                \"direct_model_params\"\n            ][:n_modes]\n            valid_cov_indices = np.where(\n                (\n                    return_model.model_params[\"sparse_covariance_structure\"][:, 0]\n                    &lt; n_modes\n                )\n                &amp; (\n                    return_model.model_params[\"sparse_covariance_structure\"][:, 1]\n                    &lt; n_modes\n                ),\n            )[0]\n            new_model_params[\"sparse_covariance_structure\"] = return_model.model_params[\n                \"sparse_covariance_structure\"\n            ][valid_cov_indices]\n            new_model_params[\"covariance_model_params\"] = [\n                return_model.model_params[\"covariance_model_params\"][i]\n                for i in valid_cov_indices\n            ]\n            new_model_params[\"n_params\"] = self.model_params.get(\"n_params\", None)\n            return_model.model_params = new_model_params\n\n        return return_model\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.adapt_fit","title":"<code>adapt_fit(covariate_to_adapt: str, new_category_names: npt.NDArray[np.str_], spectral_coeff_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, pretrained_model_params: dict[str, Any] | None = None, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False) -&gt; None</code>","text":"<p>Using a previously fitted spectral normative model, adapt to a new batch. This method enables adaptation (fine-tuning) of the model to data from a new batch/site by freezing all fitted parameters, and only estimating new parameters for the new batch/site category.</p> <p>Parameters:</p> Name Type Description Default <code>covariate_to_adapt</code> <code>str</code> <p>str Name of the categorical covariate representing the batch/site to which the model should be adapted. Note: This covariate must have been specified in the original model.</p> required <code>new_category_names</code> <code>NDArray[str_]</code> <p>list[str] Names of the new categories in the covariate_to_adapt representing the new batch/site labels (e.g. names of the new site). Note: These names must not have been present in the original fitted model.</p> required <code>spectral_coeff_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification. Note: The covariate_to_adapt column must only contain the new_category_names (no new data from previously trained batches).</p> required <code>pretrained_model_params</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None The model parameters from a previously fitted model to adapt. If None, the model parameters from the current instance will be used (assuming fitting was done).</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the adapted model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def adapt_fit(\n    self,\n    covariate_to_adapt: str,\n    new_category_names: npt.NDArray[np.str_],\n    spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    pretrained_model_params: dict[str, Any] | None = None,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Using a previously fitted spectral normative model, adapt to a new\n    batch.\n    This method enables adaptation (fine-tuning) of the model to data\n    from a new batch/site by freezing all fitted parameters, and only\n    estimating new parameters for the new batch/site category.\n\n    Args:\n        covariate_to_adapt: str\n            Name of the categorical covariate representing the batch/site\n            to which the model should be adapted.\n            Note: This covariate must have been specified in the original\n            model.\n        new_category_names: list[str]\n            Names of the new categories in the covariate_to_adapt representing\n            the new batch/site labels (e.g. names of the new site).\n            Note: These names must not have been present in the original\n            fitted model.\n        spectral_coeff_train_data: np.ndarray\n            Spectral coefficients of training data\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n            Note: The covariate_to_adapt column must only contain the\n            new_category_names (no new data from previously trained batches).\n        pretrained_model_params: dict[str, Any] | None\n            The model parameters from a previously fitted model to adapt.\n            If None, the model parameters from the current instance will be used\n            (assuming fitting was done).\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            A path to a directory to save the adapted model. If provided,\n            the fitted model will be saved to this path.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n    \"\"\"\n    # Locate the covariate to adapt\n    cov_to_adapt_index = [\n        cov.name for cov in self.base_model.spec.covariates\n    ].index(covariate_to_adapt)\n\n    # Extend the covariate categories to include the new categories\n    self.base_model.spec.covariates[cov_to_adapt_index].extend_categories(\n        new_category_names,\n    )\n\n    # Extract the pre-trained model parameters\n    if pretrained_model_params is None:\n        if not hasattr(self, \"model_params\") or self.model_params is None:\n            err = (\n                \"No pretrained model parameters found. \"\n                \"Please provide pretrained_model_params or fit the model first.\"\n            )\n            raise ValueError(err)\n        pretrained_model_params = copy.deepcopy(self.model_params)\n\n    # Fit the adapted model\n    self.fit(\n        spectral_coeff_train_data,\n        covariates_dataframe,\n        n_modes=pretrained_model_params[\"n_modes\"],\n        n_jobs=n_jobs,\n        save_directory=save_directory,\n        save_separate=save_separate,\n        covariance_structure=pretrained_model_params[\"sparse_covariance_structure\"],\n        adapt={\n            \"covariate_to_adapt\": covariate_to_adapt,\n            \"new_category_names\": new_category_names,\n            \"pretrained_model_params\": pretrained_model_params,\n        },\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.build_from_dataframe","title":"<code>build_from_dataframe(eigenmode_basis: utils.gsp.EigenmodeBasis, model_type: ModelType, covariates_dataframe: pd.DataFrame, numerical_covariates: list[str] | None = None, categorical_covariates: list[str] | None = None, batch_covariates: list[str] | None = None, nonlinear_covariates: list[str] | None = None, influencing_mean: list[str] | None = None, influencing_variance: list[str] | None = None, spline_kwargs: dict[str, Any] | None = None) -&gt; SpectralNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize SNM with an eigenmode basis and a base direct model built from a pandas DataFrame containing all covariates.</p> <p>This uses the from_dataframe method of the DirectNormativeModel class to populate the direct model specification of SNM. Given that SNM does not require a fixed variable of interest, this method assigns a dummy name to the variable_of_interest parameter of the DirectNormativeModel. As such, the provided dataframe should not contain a column with \"dummy_VOI\" as name.</p> <p>Essentially, the provided dataframe should contain all covariates as columns.</p> <p>Parameters:</p> Name Type Description Default <code>eigenmode_basis</code> <code>EigenmodeBasis</code> <p>utils.gsp.EigenmodeBasis The eigenmode basis to be used for spectral normative modeling.</p> required <code>model_type</code> <code>ModelType</code> <p>ModelType Type of the model to create, either \"HBR\" (Hierarchical Bayesian Regression) or \"BLR\" (Bayesian Linear Regression).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the data for all covariates and all samples.</p> required <code>numerical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of numerical covariate names.</p> <code>None</code> <code>categorical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of categorical covariate names.</p> <code>None</code> <code>batch_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of batch covariate names which should also be included in categorical_covariates.</p> <code>None</code> <code>nonlinear_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names to be modeled as nonlinear effects. These should also be included in numerical_covariates.</p> <code>None</code> <code>influencing_mean</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the mean of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>influencing_variance</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the variance of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>spline_kwargs</code> <code>dict[str, Any] | None</code> <p>dict Additional keyword arguments for spline specification, such as <code>df</code>, <code>degree</code>, and <code>knots</code>. These are passed to the <code>create_spline_spec</code> method to create spline specifications for nonlinear covariates.</p> <code>None</code> <p>Returns:</p> Type Description <code>SpectralNormativeModel</code> <p>SpectralNormativeModel An instance of SpectralNormativeModel with base model specs initialized based on the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef build_from_dataframe(\n    cls,\n    eigenmode_basis: utils.gsp.EigenmodeBasis,\n    model_type: ModelType,\n    covariates_dataframe: pd.DataFrame,\n    numerical_covariates: list[str] | None = None,\n    categorical_covariates: list[str] | None = None,\n    batch_covariates: list[str] | None = None,\n    nonlinear_covariates: list[str] | None = None,\n    influencing_mean: list[str] | None = None,\n    influencing_variance: list[str] | None = None,\n    spline_kwargs: dict[str, Any] | None = None,\n) -&gt; SpectralNormativeModel:\n    \"\"\"\n    Initialize SNM with an eigenmode basis and a base direct model built from a\n    pandas DataFrame containing all covariates.\n\n    This uses the from_dataframe method of the DirectNormativeModel class\n    to populate the direct model specification of SNM. Given that SNM does not\n    require a fixed variable of interest, this method assigns a dummy name\n    to the variable_of_interest parameter of the DirectNormativeModel. As such,\n    the provided dataframe should not contain a column with \"dummy_VOI\" as name.\n\n    Essentially, the provided dataframe should contain all covariates as columns.\n\n    Args:\n        eigenmode_basis: utils.gsp.EigenmodeBasis\n            The eigenmode basis to be used for spectral normative modeling.\n        model_type: ModelType\n            Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n            Regression) or \"BLR\" (Bayesian Linear Regression).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the data for all covariates and all samples.\n        numerical_covariates: list[str] | None\n            List of numerical covariate names.\n        categorical_covariates: list[str] | None\n            List of categorical covariate names.\n        batch_covariates: list[str] | None\n            List of batch covariate names which should also be included in\n            categorical_covariates.\n        nonlinear_covariates: list[str] | None\n            List of covariate names to be modeled as nonlinear effects.\n            These should also be included in numerical_covariates.\n        influencing_mean: list[str] | None\n            List of covariate names that influence the mean of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        influencing_variance: list[str] | None\n            List of covariate names that influence the variance of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        spline_kwargs: dict\n            Additional keyword arguments for spline specification, such as\n            `df`, `degree`, and `knots`. These are passed to the\n            `create_spline_spec` method to create spline specifications for\n            nonlinear covariates.\n\n    Returns:\n        SpectralNormativeModel\n            An instance of SpectralNormativeModel with base model specs initialized\n            based on the provided data.\n    \"\"\"\n    # Add a dummy variable of interest to the covariates_dataframe\n    covariates_dataframe = covariates_dataframe.copy()\n    covariates_dataframe[\"dummy_VOI\"] = 0.0  # Dummy variable of interest\n    # Specify the base model from the dataframe\n    return cls(\n        eigenmode_basis=eigenmode_basis,\n        base_model=DirectNormativeModel.from_dataframe(\n            model_type=model_type,\n            dataframe=covariates_dataframe,\n            variable_of_interest=\"dummy_VOI\",  # Dummy variable of interest\n            numerical_covariates=(numerical_covariates or []),\n            categorical_covariates=(categorical_covariates or []),\n            batch_covariates=(batch_covariates or []),\n            nonlinear_covariates=(nonlinear_covariates or []),\n            influencing_mean=(influencing_mean or []),\n            influencing_variance=(influencing_variance or []),\n            spline_kwargs=(spline_kwargs or {}),\n        ),\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.compute_spectral_predictions","title":"<code>compute_spectral_predictions(test_covariates: pd.DataFrame, *, model_params: dict[str, Any] | None = None, n_modes: int | None = None, n_jobs: int = -1, predict_without: list[str] | None = None) -&gt; dict[str, npt.NDArray[np.floating[Any]]]</code>","text":"<p>Predict normative moments (mean, std) of the eigenmode basis for new data using the fitted spectral normative model.</p> <p>This function requires a dataframe of covariates (test_covariates) to compute a set of spectral predictions that can subsequently be combined to efficiently estimate normative predictions for any query(ies).</p> <p>Parameters:</p> Name Type Description Default <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates. Note: covariates listed in predict_without will be ignored and are hence not required.</p> required <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the number of modes from model_params will be used.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to utilize. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>predict_without</code> <code>list[str] | None</code> <p>list[str] | None Optional list of covariate names to ignore during prediction. This can be used to check the effect of removing certain covariates from the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, NDArray[floating[Any]]]</code> <p>A dictionary containing: - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes) - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes) - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def compute_spectral_predictions(\n    self,\n    test_covariates: pd.DataFrame,\n    *,\n    model_params: dict[str, Any] | None = None,\n    n_modes: int | None = None,\n    n_jobs: int = -1,\n    predict_without: list[str] | None = None,\n) -&gt; dict[str, npt.NDArray[np.floating[Any]]]:\n    \"\"\"\n    Predict normative moments (mean, std) of the eigenmode basis for new data\n    using the fitted spectral normative model.\n\n    This function requires a dataframe of covariates (test_covariates) to compute\n    a set of spectral predictions that can subsequently be combined to efficiently\n    estimate normative predictions for any query(ies).\n\n    Args:\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n            Note: covariates listed in predict_without will be ignored and are\n            hence not required.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the number of modes from model_params will be used.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to utilize. If -1, all available CPU cores are\n            used. If 1, no parallelization is used.\n        predict_without: list[str] | None\n            Optional list of covariate names to ignore during prediction.\n            This can be used to check the effect of removing certain covariates\n            from the model.\n\n    Returns:\n        dict:\n            A dictionary containing:\n            - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n            - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n            - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n    \"\"\"\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(model_params[\"n_modes\"])\n\n    if self.base_model.spec is None:\n        err = \"The base model is not specified. Cannot predict new data.\"\n        raise ValueError(err)\n\n    # Validate the covariate data\n    validation_columns = [\n        cov.name\n        for cov in self.base_model.spec.covariates\n        if cov.name not in (predict_without or [])\n    ]\n    utils.general.validate_dataframe(test_covariates, validation_columns)\n\n    # direct normative predictions for each eigenmode\n    (\n        eigenmode_mu_estimates,\n        eigenmode_std_estimates,\n    ) = self._predict_all_mode_estimates(\n        test_covariates,\n        model_params,\n        n_modes,\n        n_jobs=n_jobs,\n        predict_without=predict_without,\n    )  # estimates have a shape of (n_samples, n_modes)\n\n    # cross-mode dependence structure\n    rho_estimates = self._predict_all_covariance_estimates(\n        test_covariates,\n        model_params,\n        n_modes,\n        n_jobs=n_jobs,\n        predict_without=predict_without,\n    )  # estimates have a shape of (n_samples, n_covariance_pairs)\n\n    return {\n        \"eigenmode_mu_estimates\": eigenmode_mu_estimates,\n        \"eigenmode_std_estimates\": eigenmode_std_estimates,\n        \"rho_estimates\": rho_estimates,\n    }\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.evaluate","title":"<code>evaluate(encoded_query: npt.NDArray[np.floating[Any]], spectral_coeff_test_data: npt.NDArray[np.floating[Any]], *, spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None, test_covariates: pd.DataFrame | None = None, query_train_moments: npt.NDArray[np.floating[Any]] | None = None, model_params: dict[str, Any] | None = None, n_modes: int | None = None) -&gt; NormativePredictions</code>","text":"<p>Evaluate the model on new data and return predictions along with evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_query</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded query data defining the normative variable of interest. Can be provided as: - shape = (n_modes) for a single query vector - shape = (n_modes, n_queries) for multiple queries predicted at once</p> required <code>spectral_coeff_test_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray | None Spectral coefficient of test data for the phenotype being modeled :math:<code>(T_{test} \\Psi_{(k)}) \\in R^{N_{test} \\times k}</code> (only required for extended predictions). Expects a numpy array (n_samples, n_modes)</p> required <code>spectral_predictions</code> <code>dict[str, NDArray[floating[Any]]] | None</code> <p>dict | None Optional dictionary of precomputed spectral predictions to use for the evaluation. If not provided, test_covariates must be provided instead to compute the spectral predictions. The dictionary should contain: - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes) - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes) - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs) This can be obtained using the 'compute_spectral_predictions' method.</p> <code>None</code> <code>test_covariates</code> <code>DataFrame | None</code> <p>pd.DataFrame | None DataFrame containing the new covariate data to predict. This must include all specified covariates. Note: This is only required if spectral_predictions was not provided.</p> <code>None</code> <code>query_train_moments</code> <code>NDArray[floating[Any]] | None</code> <p>np.ndarray | None A (2, n_queries) array containing the query moments (mean, std) directly measured in the training data. While optional, providing these moments is strongly recommended for accurate evaluation of the model's MSLL. If not provided, the model will use the test data moments as an approximation, which may lead to overestimating MSLL. This is made optional to allow evaluating MSLL when the training data is not accessible (e.g. using a pre-trained model).</p> <code>None</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the stored number of modes from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted moments (mean, std) for the variable of interest defined by the encoded query, along with evaluation metrics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate(\n    self,\n    encoded_query: npt.NDArray[np.floating[Any]],\n    spectral_coeff_test_data: npt.NDArray[np.floating[Any]],\n    *,\n    spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None,\n    test_covariates: pd.DataFrame | None = None,\n    query_train_moments: npt.NDArray[np.floating[Any]] | None = None,\n    model_params: dict[str, Any] | None = None,\n    n_modes: int | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the model on new data and return predictions along with evaluation\n    metrics.\n\n    Args:\n        encoded_query: np.ndarray\n            Encoded query data defining the normative variable of interest.\n            Can be provided as:\n            - shape = (n_modes) for a single query vector\n            - shape = (n_modes, n_queries) for multiple queries predicted at once\n        spectral_coeff_test_data: np.ndarray | None\n            Spectral coefficient of test data for the phenotype being modeled\n            :math:`(T_{test} \\\\Psi_{(k)}) \\\\in R^{N_{test} \\\\times k}`\n            (only required for extended predictions).\n            Expects a numpy array (n_samples, n_modes)\n        spectral_predictions: dict | None\n            Optional dictionary of precomputed spectral predictions to use for\n            the evaluation. If not provided, test_covariates must be provided\n            instead to compute the spectral predictions.\n            The dictionary should contain:\n            - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n            - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n            - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n            This can be obtained using the 'compute_spectral_predictions' method.\n        test_covariates: pd.DataFrame | None\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n            Note: This is only required if spectral_predictions was not provided.\n        query_train_moments: np.ndarray | None\n            A (2, n_queries) array containing the query moments (mean, std) directly\n            measured in the training data. While optional, providing these moments\n            is strongly recommended for accurate evaluation of the model's MSLL.\n            If not provided, the model will use the test data moments as an\n            approximation, which may lead to overestimating MSLL. This is made\n            optional to allow evaluating MSLL when the training data is not\n            accessible (e.g. using a pre-trained model).\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the stored number of modes from model.fit() will be used.\n\n    Returns:\n        NormativePredictions:\n            Object containing the predicted moments (mean, std) for\n            the variable of interest defined by the encoded query, along with\n            evaluation metrics.\n    \"\"\"\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(self.model_params[\"n_modes\"])\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Reformat encoded queries (for efficiency)\n    encoded_query = np.asarray(encoded_query[:n_modes])\n    encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n    # Run extended predictions\n    predictions = self.predict(\n        encoded_query=encoded_query,\n        spectral_predictions=spectral_predictions,\n        test_covariates=test_covariates,\n        extended=True,\n        model_params=model_params,\n        spectral_coeff_test_data=spectral_coeff_test_data,\n        n_modes=n_modes,\n    )\n    if query_train_moments is None:\n        logger.warning(\n            \"Query moments not provided. Using test data moments as an\"\n            \" approximation, which may lead to overestimating MSLL.\",\n        )\n        query_train_moments = np.array(\n            [\n                np.mean(spectral_coeff_test_data @ encoded_query, axis=0),\n                np.std(spectral_coeff_test_data @ encoded_query, axis=0, ddof=1),\n            ],\n        )\n    return predictions.evaluate_predictions(\n        variable_of_interest=spectral_coeff_test_data @ encoded_query,\n        train_mean=query_train_moments[0],\n        train_std=query_train_moments[1],\n        n_params=model_params[\"n_params\"],\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.fit","title":"<code>fit(spectral_coeff_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_modes: int = -1, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False, covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Fit the spectral normative model to the provided spectral coefficient training data.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_coeff_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_modes</code> <code>int</code> <p>int (default=-1) Number of eigenmodes to fit the model to. If -1, all modes are used. If a positive integer, only the first n_modes are used. Note that the spectral_coeff_train_data and the eigenmode basis should have at least n_modes columns/eigenvectors.</p> <code>-1</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> <code>covariance_structure</code> <code>NDArray[floating[Any]] | float</code> <p>np.ndarray | float Sparse covariance structure to use for the model fitting. If a (2, n_pairs) array of row and column indices are provided, the model will use this structure. If float, the model will estimate the covariance structure based on the training data and the float value will be used as the sparsity threshold for the number of covariance pairs to keep proportional to the number of modes. Defaults to 0.5, meaning that the number of modeled sparse covariance pairs will be half the number of modes. Note: If using a small number of nodes, it is advisable to increase the sparsity threshold to ensure a stable estimation of the covariance structure. In contrast, when using a large number of nodes, a lower sparsity threshold should be used to ensure sparse modeling of the covariance structure.</p> <code>0.5</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None (default=None) If provided, adapt a pre-trained model to a new covariate. Note: We recommended using the <code>adapt_fit</code> method, and not directly changing this argument, unless you know what you are doing.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_modes: int = -1,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n    covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Fit the spectral normative model to the provided spectral coefficient\n    training data.\n\n    Args:\n        spectral_coeff_train_data: np.ndarray\n            Spectral coefficients of training data\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_modes: int (default=-1)\n            Number of eigenmodes to fit the model to. If -1, all modes are\n            used. If a positive integer, only the first n_modes are used.\n            Note that the spectral_coeff_train_data and the eigenmode basis should\n            have at least n_modes columns/eigenvectors.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n        covariance_structure: np.ndarray | float\n            Sparse covariance structure to use for the model fitting. If a\n            (2, n_pairs) array of row and column indices are provided, the model\n            will use this structure. If float, the model will estimate the\n            covariance structure based on the training data and the float value\n            will be used as the sparsity threshold for the number of covariance\n            pairs to keep proportional to the number of modes. Defaults to 0.5,\n            meaning that the number of modeled sparse covariance pairs will be\n            half the number of modes.\n            Note: If using a small number of nodes, it is advisable to increase\n            the sparsity threshold to ensure a stable estimation of the covariance\n            structure. In contrast, when using a large number of nodes, a lower\n            sparsity threshold should be used to ensure sparse modeling of the\n            covariance structure.\n        adapt: dict[str, Any] | None (default=None)\n            If provided, adapt a pre-trained model to a new covariate.\n            Note: We recommended using the `adapt_fit` method, and not directly\n            changing this argument, unless you know what you are doing.\n    \"\"\"\n    logger.info(\"Starting SNM model fitting:\")\n    # Evaluate the number of modes to fit\n    if n_modes == -1:\n        n_modes = self.eigenmode_basis.n_modes\n    # Validate the input data\n    if not isinstance(spectral_coeff_train_data, np.ndarray):\n        err = \"spectral_coeff_train_data must be a numpy array.\"\n        raise TypeError(err)\n    if spectral_coeff_train_data.shape[1] &lt; n_modes:\n        err = (\n            f\"spectral_coeff_train_data must have at least {n_modes}\"\n            \" columns (n_modes).\"\n        )\n        raise ValueError(err)\n    if self.eigenmode_basis.n_modes &lt; n_modes:\n        err = (\n            f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n            f\" modes, while {n_modes} were requested.\"\n        )\n        raise ValueError(err)\n\n    # Setup the save directory if needed\n    if save_directory is not None:\n        # Prepare the save directory\n        save_directory = Path(save_directory)\n        utils.general.prepare_save_directory(\n            save_directory,\n            \"spectral_normative_model\",\n        )\n\n    logger.info(\"Step 1; direct models for each eigenmode (%s modes)\", n_modes)\n\n    self.fit_all_direct(\n        spectral_coeff_train_data=spectral_coeff_train_data,\n        covariates_dataframe=covariates_dataframe,\n        n_modes=n_modes,\n        n_jobs=n_jobs,\n        save_directory=save_directory,\n        save_separate=save_separate,\n        adapt=adapt,\n    )\n\n    logger.info(\"Step 2; identify sparse covariance structure\")\n\n    self.identify_covariance_structure(\n        spectral_coeff_train_data=spectral_coeff_train_data,\n        covariates_dataframe=covariates_dataframe,\n        n_modes=n_modes,\n        covariance_structure=covariance_structure,\n        adapt=adapt,\n    )\n\n    # Verify that the covariance structure is valid\n    if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n        err = \"Invalid sparse covariance structure.\"\n        raise ValueError(err)\n\n    # Model cross basis sparse covariance structure\n    logger.info(\n        \"Step 3; cross-eigenmode dependency modeling (%s pairs)\",\n        self.sparse_covariance_structure.shape[0],\n    )\n\n    self.fit_all_covariance(\n        spectral_coeff_train_data=spectral_coeff_train_data,\n        covariates_dataframe=covariates_dataframe,\n        n_jobs=n_jobs,\n        save_directory=save_directory,\n        save_separate=save_separate,\n        adapt=adapt,\n    )\n\n    # Save SNM model parameters\n    sample_size = spectral_coeff_train_data.shape[0]\n    if adapt is not None:\n        sample_size += adapt[\"pretrained_model_params\"][\"sample_size\"]\n    self.model_params = {\n        \"n_modes\": n_modes,\n        \"sample_size\": sample_size,\n        \"direct_model_params\": self.direct_model_params,\n        \"sparse_covariance_structure\": self.sparse_covariance_structure,\n        \"covariance_model_params\": self.covariance_model_params,\n    }\n    if (self.direct_model_params[0] is not None) and (\n        \"n_params\" in self.direct_model_params[0]\n    ):\n        self.model_params[\"n_params\"] = self.direct_model_params[0][\"n_params\"]\n    else:\n        err = \"Direct model parameters are not valid.\"\n        raise ValueError(err)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(save_directory)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.fit_all_covariance","title":"<code>fit_all_covariance(spectral_coeff_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Fit the direct models for all specified eigenmodes.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_coeff_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None Adaptation parameters from a previously fitted model. If provided, the model will be adapted using these parameters during fitting.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_all_covariance(\n    self,\n    spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Fit the direct models for all specified eigenmodes.\n\n    Args:\n        spectral_coeff_train_data: np.ndarray\n            Spectral coefficients of training data\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n        adapt: dict[str, Any] | None\n            Adaptation parameters from a previously fitted model. If provided,\n            the model will be adapted using these parameters during fitting.\n    \"\"\"\n    # Setup the save directory if needed\n    if save_directory is not None:\n        save_directory = Path(save_directory)\n\n    # Fit the base covariance models for selected eigenmode pairs in parallel\n    tasks = (\n        joblib.delayed(self.fit_single_covariance)(\n            variable_of_interest_1=spectral_coeff_train_data[\n                :,\n                self.sparse_covariance_structure[i, 0],\n            ],\n            variable_of_interest_2=spectral_coeff_train_data[\n                :,\n                self.sparse_covariance_structure[i, 1],\n            ],\n            direct_model_params_1=self.direct_model_params[\n                self.sparse_covariance_structure[i, 0]\n            ],\n            direct_model_params_2=self.direct_model_params[\n                self.sparse_covariance_structure[i, 1]\n            ],\n            covariates_dataframe=covariates_dataframe,\n            save_directory=(\n                utils.general.ensure_dir(\n                    save_directory\n                    / \"spectral_normative_model\"\n                    / \"covariance_models\"\n                    / (\n                        f\"mode_{self.sparse_covariance_structure[i, 0] + 1},\"\n                        f\"mode_{self.sparse_covariance_structure[i, 1] + 1}\"\n                    ),\n                )\n                if save_directory is not None and save_separate\n                else None\n            ),\n            adapt=(\n                None\n                if adapt is None\n                else {\n                    \"covariate_to_adapt\": adapt[\"covariate_to_adapt\"],\n                    \"new_category_names\": adapt[\"new_category_names\"],\n                    \"pretrained_model_params\": adapt[\"pretrained_model_params\"][\n                        \"covariance_model_params\"\n                    ][i],\n                }\n            ),\n        )\n        for i in range(self.sparse_covariance_structure.shape[0])\n    )\n    self.covariance_model_params = utils.parallel.ParallelTqdm(\n        n_jobs=n_jobs,\n        total_tasks=self.sparse_covariance_structure.shape[0],\n        desc=\"Fitting covariance models\",\n    )(tasks)  # pyright: ignore[reportCallIssue]\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.fit_all_direct","title":"<code>fit_all_direct(spectral_coeff_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_modes: int = -1, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Fit the direct models for all specified eigenmodes.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_coeff_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_modes</code> <code>int</code> <p>int (default=-1) Number of eigenmodes to fit the model to. If -1, all modes are used. If a positive integer, only the first n_modes are used. Note that the spectral_coeff_train_data and the eigenmode basis should have at least n_modes columns/eigenvectors.</p> <code>-1</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None Adaptation parameters from a previously fitted model. If provided, the model will be adapted using these parameters during fitting.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_all_direct(\n    self,\n    spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_modes: int = -1,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Fit the direct models for all specified eigenmodes.\n\n    Args:\n        spectral_coeff_train_data: np.ndarray\n            Spectral coefficients of training data\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_modes: int (default=-1)\n            Number of eigenmodes to fit the model to. If -1, all modes are\n            used. If a positive integer, only the first n_modes are used.\n            Note that the spectral_coeff_train_data and the eigenmode basis should\n            have at least n_modes columns/eigenvectors.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n        adapt: dict[str, Any] | None\n            Adaptation parameters from a previously fitted model. If provided,\n            the model will be adapted using these parameters during fitting.\n    \"\"\"\n    # Setup the save directory if needed\n    if save_directory is not None:\n        save_directory = Path(save_directory)\n\n    # Evaluate the number of modes to fit\n    if n_modes == -1:\n        n_modes = self.eigenmode_basis.n_modes\n\n    # Fit the base direct model for each eigenmode using parallel processing\n    tasks = (\n        joblib.delayed(self.fit_single_direct)(\n            variable_of_interest=spectral_coeff_train_data[:, i],\n            covariates_dataframe=covariates_dataframe,\n            save_directory=(\n                utils.general.ensure_dir(\n                    save_directory\n                    / \"spectral_normative_model\"\n                    / \"direct_models\"\n                    / f\"mode_{i + 1}\",\n                )\n                if save_directory is not None and save_separate\n                else None\n            ),\n            adapt=(\n                None\n                if adapt is None\n                else {\n                    \"covariate_to_adapt\": adapt[\"covariate_to_adapt\"],\n                    \"new_category_names\": adapt[\"new_category_names\"],\n                    \"pretrained_model_params\": adapt[\"pretrained_model_params\"][\n                        \"direct_model_params\"\n                    ][i],\n                }\n            ),\n        )\n        for i in range(n_modes)\n    )\n    self.direct_model_params = list(\n        utils.parallel.ParallelTqdm(\n            n_jobs=n_jobs,\n            total_tasks=n_modes,\n            desc=\"Fitting direct models\",\n        )(tasks),  # pyright: ignore[reportCallIssue]\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.fit_single_covariance","title":"<code>fit_single_covariance(variable_of_interest_1: npt.NDArray[np.floating[Any]], variable_of_interest_2: npt.NDArray[np.floating[Any]], direct_model_params_1: dict[str, Any], direct_model_params_2: dict[str, Any], covariates_dataframe: pd.DataFrame, *, save_directory: Path | None = None, return_model_params: bool = True, defaults_overwrite: dict[str, Any] | None = None, adapt: dict[str, Any] | None = None) -&gt; dict[str, Any] | None</code>","text":"<p>Fit a covariance normative model between a single pair of eigenmodes. This method fits a covariance model to the provided pair of variables and covariates dataframe, considering the direct model fits for each eigenmode, while allowing for the cross-eigenmode covariance to vary normatively.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest_1</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a single eigenmode.</p> required <code>variable_of_interest_2</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a second eigenmode.</p> required <code>direct_model_params_1</code> <code>dict[str, Any]</code> <p>dict The parameters of the direct model fitted to the first eigenmode.</p> required <code>direct_model_params_2</code> <code>dict[str, Any]</code> <p>dict The parameters of the direct model fitted to the second eigenmode.</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved.</p> <code>None</code> <code>return_model_params</code> <code>bool</code> <p>bool If True, return the fitted model parameters.</p> <code>True</code> <code>defaults_overwrite</code> <code>dict[str, Any] | None</code> <p>dict (default={}) Dictionary of default values to overwrite in the model fitting process.</p> <code>None</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None = None Adaptation parameters from a previously fitted model. If provided, the model will be adapted using these parameters during fitting.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any] | None</code> <p>If <code>return_model_params</code> is True, return the fitted model parameters in a dictionary.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_single_covariance(\n    self,\n    variable_of_interest_1: npt.NDArray[np.floating[Any]],\n    variable_of_interest_2: npt.NDArray[np.floating[Any]],\n    direct_model_params_1: dict[str, Any],\n    direct_model_params_2: dict[str, Any],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    return_model_params: bool = True,\n    defaults_overwrite: dict[str, Any] | None = None,\n    adapt: dict[str, Any] | None = None,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Fit a covariance normative model between a single pair of eigenmodes.\n    This method fits a covariance model to the provided pair of variables\n    and covariates dataframe, considering the direct model fits for each\n    eigenmode, while allowing for the cross-eigenmode covariance to vary\n    normatively.\n\n    Args:\n        variable_of_interest_1: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a single eigenmode.\n        variable_of_interest_2: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a second eigenmode.\n        direct_model_params_1: dict\n            The parameters of the direct model fitted to the first eigenmode.\n        direct_model_params_2: dict\n            The parameters of the direct model fitted to the second eigenmode.\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n        return_model_params: bool\n            If True, return the fitted model parameters.\n        defaults_overwrite: dict (default={})\n            Dictionary of default values to overwrite in the model fitting process.\n        adapt: dict[str, Any] | None = None\n            Adaptation parameters from a previously fitted model. If provided,\n            the model will be adapted using these parameters during fitting.\n\n    Returns:\n        dict:\n            If `return_model_params` is True, return the fitted model parameters\n            in a dictionary.\n    \"\"\"\n    # Prepare the data for fitting\n    train_data = covariates_dataframe.copy()\n    # Add the respective mode loadings as the variables of interest\n    train_data[\"VOI_1\"] = variable_of_interest_1\n    train_data[\"VOI_2\"] = variable_of_interest_2\n    train_data[[\"VOI_1_mu_estimate\", \"VOI_1_std_estimate\"]] = (\n        self.base_model.predict(\n            train_data,\n            model_params=direct_model_params_1,\n        )\n        .to_array()\n        .T\n    )  # Add the direct model predictions\n    train_data[[\"VOI_2_mu_estimate\", \"VOI_2_std_estimate\"]] = (\n        self.base_model.predict(\n            train_data,\n            model_params=direct_model_params_2,\n        )\n        .to_array()\n        .T\n    )  # Add the direct model predictions\n\n    # Instantiate a covariance normative model from the base model\n    covariance_model = CovarianceNormativeModel.from_direct_model(\n        self.base_model,\n        variable_of_interest_1=\"VOI_1\",\n        variable_of_interest_2=\"VOI_2\",\n        defaults_overwrite=(defaults_overwrite or {}),\n    )\n\n    # Fit the model silently\n    with utils.general.suppress_output():\n        covariance_model.fit(\n            train_data=train_data,\n            save_directory=save_directory,\n            progress_bar=False,\n            adapt=adapt,\n        )\n\n    # Return the fitted model parameters if requested\n    if return_model_params:\n        return covariance_model.model_params\n\n    # If not returning model parameters, return None\n    return None\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.fit_single_direct","title":"<code>fit_single_direct(variable_of_interest: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, save_directory: Path | None = None, return_model_params: bool = True, adapt: dict[str, Any] | None = None) -&gt; dict[str, Any] | None</code>","text":"<p>Fit a direct normative model for a single spectral eigenmode. This method fits the base direct model to the provided variable of interest and covariates dataframe, allowing for the model to be trained on a specific eigenmode of the spectral embedding.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a single eigenmode.</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved.</p> <code>None</code> <code>return_model_params</code> <code>bool</code> <p>bool If True, return the fitted model parameters.</p> <code>True</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None Adaptation parameters from a previously fitted model. If provided, the model will be adapted using these parameters during fitting.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any] | None</code> <p>If <code>return_model_params</code> is True, return the fitted model parameters in a dictionary.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_single_direct(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    return_model_params: bool = True,\n    adapt: dict[str, Any] | None = None,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Fit a direct normative model for a single spectral eigenmode.\n    This method fits the base direct model to the provided variable of interest\n    and covariates dataframe, allowing for the model to be trained on a specific\n    eigenmode of the spectral embedding.\n\n    Args:\n        variable_of_interest: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a single eigenmode.\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n        return_model_params: bool\n            If True, return the fitted model parameters.\n        adapt: dict[str, Any] | None\n            Adaptation parameters from a previously fitted model. If provided,\n            the model will be adapted using these parameters during fitting.\n\n    Returns:\n        dict:\n            If `return_model_params` is True, return the fitted model parameters\n            in a dictionary.\n    \"\"\"\n    # Prepare the data for fitting\n    train_data = covariates_dataframe.copy()\n    # Add the mode loading as the variable of interest\n    train_data[\"VOI\"] = variable_of_interest\n\n    # Instantiate a direct normative model from the base model\n    direct_model = DirectNormativeModel(\n        spec=NormativeModelSpec(\n            variable_of_interest=\"VOI\",  # Use the added VOI column\n            covariates=self.base_model.spec.covariates,\n            influencing_mean=self.base_model.spec.influencing_mean,\n            influencing_variance=self.base_model.spec.influencing_variance,\n        ),\n        defaults=self.base_model.defaults,\n    )\n\n    # Fit the model silently\n    with utils.general.suppress_output():\n        direct_model.fit(\n            train_data=train_data,\n            save_directory=save_directory,\n            progress_bar=False,\n            adapt=adapt,\n        )\n\n    # Return the fitted model parameters if requested\n    if return_model_params:\n        return direct_model.model_params\n\n    # If not returning model parameters, return None\n    return None\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.harmonize","title":"<code>harmonize(encoded_query: npt.NDArray[np.floating[Any]], spectral_coeff_data: npt.NDArray[np.floating[Any]], *, covariates_to_harmonize: list[str] | None = None, covariates_dataframe: pd.DataFrame | None = None, spectral_predictions_full: dict[str, npt.NDArray[np.floating[Any]]] | None = None, spectral_predictions_partial: dict[str, npt.NDArray[np.floating[Any]]] | None = None, model_params: dict[str, Any] | None = None, n_modes: int | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Harmonize the variables of interest in the data to remove effects of certain covariates (e.g. batch). This method uses the spectral normative model to harmonize one or several variables of interest defined by the encoded query.</p> <p>The harmonization method can be used in two ways: - By providing a dataframe of covariates (covariates_dataframe) to compute the   necessary spectral predictions for both the full model (all covariates)   and the partial model (excluding covariates to harmonize). In this format,   you should also provide the covariates_to_harmonize (list of covariate names). - By providing precomputed spectral predictions for both the full and partial   models (spectral_predictions_full and spectral_predictions_partial). In this   format, the partial spectral predictions should have been computed by   excluding the covariates to harmonize using the predict_without parameter in   the compute_spectral_predictions method.   Note: In the latter, the method will not use the covariates_to_harmonize list.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_query</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded query data defining the normative variable of interest. Can be provided as: - shape = (n_modes) for a single query vector - shape = (n_modes, n_queries) for multiple queries predicted at once</p> required <code>spectral_coeff_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray | None Spectral coefficient of the the phenotype being modeled :math:<code>(T \\Psi_{(k)}) \\in R^{N_{p} \\times k}</code>. Expects a numpy array (n_samples, n_modes)</p> required <code>covariates_to_harmonize</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names to harmonize. The partial effects of these covariates will be removed from the variable of interest, and the harmonized values will be returned. Note: This is only required if spectral_predictions_full and spectral_predictions_partial were not provided.</p> <code>None</code> <code>covariates_dataframe</code> <code>DataFrame | None</code> <p>pd.DataFrame | None DataFrame containing covariate information for the data to harmonize. This must include all specified covariates. The dataframe is expected to have all covariates as columns and samples as rows. Note: This is only required if spectral_predictions_full and spectral_predictions_partial were not provided. Alternatively, if any of the aforementioned spectral predictions were previously computed, then they could be passed to this method to avoid recomputation.</p> <code>None</code> <code>spectral_predictions_full</code> <code>dict[str, NDArray[floating[Any]]] | None</code> <p>dict | None Optional dictionary of precomputed spectral predictions to use for the harmonization. If not provided, covariates_dataframe must be provided instead to compute the spectral predictions. These predictions use all set of covariates. The dictionary should contain: - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes) - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes) - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs) This can be obtained using the 'compute_spectral_predictions' method.</p> <code>None</code> <code>spectral_predictions_partial</code> <code>dict[str, NDArray[floating[Any]]] | None</code> <p>dict | None Optional dictionary of precomputed spectral predictions to use for the harmonization. If not provided, covariates_dataframe must be provided instead to compute the partial spectral predictions. These predictions use all set of covariates except those to harmonize. The covariates to harmonize need to be partialed out using the predict_without parameter. The dictionary should contain: - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes) - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes) - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs) This can be obtained using the 'compute_spectral_predictions' method.</p> <code>None</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the stored number of modes from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>npt.NDArray[np.floating[Any]]: Array of harmonized values for the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def harmonize(\n    self,\n    encoded_query: npt.NDArray[np.floating[Any]],\n    spectral_coeff_data: npt.NDArray[np.floating[Any]],\n    *,\n    covariates_to_harmonize: list[str] | None = None,\n    covariates_dataframe: pd.DataFrame | None = None,\n    spectral_predictions_full: dict[str, npt.NDArray[np.floating[Any]]]\n    | None = None,\n    spectral_predictions_partial: dict[str, npt.NDArray[np.floating[Any]]]\n    | None = None,\n    model_params: dict[str, Any] | None = None,\n    n_modes: int | None = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Harmonize the variables of interest in the data to remove effects of\n    certain covariates (e.g. batch). This method uses the spectral normative model\n    to harmonize one or several variables of interest defined by the encoded query.\n\n    The harmonization method can be used in two ways:\n    - By providing a dataframe of covariates (covariates_dataframe) to compute the\n      necessary spectral predictions for both the full model (all covariates)\n      and the partial model (excluding covariates to harmonize). In this format,\n      you should also provide the covariates_to_harmonize (list of covariate names).\n    - By providing precomputed spectral predictions for both the full and partial\n      models (spectral_predictions_full and spectral_predictions_partial). In this\n      format, the partial spectral predictions should have been computed by\n      excluding the covariates to harmonize using the predict_without parameter in\n      the compute_spectral_predictions method.\n      Note: In the latter, the method will not use the covariates_to_harmonize list.\n\n    Args:\n        encoded_query: np.ndarray\n            Encoded query data defining the normative variable of interest.\n            Can be provided as:\n            - shape = (n_modes) for a single query vector\n            - shape = (n_modes, n_queries) for multiple queries predicted at once\n        spectral_coeff_data: np.ndarray | None\n            Spectral coefficient of the the phenotype being modeled\n            :math:`(T \\\\Psi_{(k)}) \\\\in R^{N_{p} \\\\times k}`.\n            Expects a numpy array (n_samples, n_modes)\n        covariates_to_harmonize: list[str] | None\n            List of covariate names to harmonize.\n            The partial effects of these covariates will be removed from the\n            variable of interest, and the harmonized values will be returned.\n            Note: This is only required if spectral_predictions_full and\n            spectral_predictions_partial were not provided.\n        covariates_dataframe: pd.DataFrame | None\n            DataFrame containing covariate information for the data to harmonize.\n            This must include all specified covariates. The dataframe is expected\n            to have all covariates as columns and samples as rows.\n            Note: This is only required if spectral_predictions_full and\n            spectral_predictions_partial were not provided. Alternatively,\n            if any of the aforementioned spectral predictions were previously\n            computed, then they could be passed to this method to avoid\n            recomputation.\n        spectral_predictions_full: dict | None\n            Optional dictionary of precomputed spectral predictions to use for\n            the harmonization. If not provided, covariates_dataframe must be\n            provided instead to compute the spectral predictions.\n            These predictions use all set of covariates.\n            The dictionary should contain:\n            - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n            - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n            - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n            This can be obtained using the 'compute_spectral_predictions' method.\n        spectral_predictions_partial: dict | None\n            Optional dictionary of precomputed spectral predictions to use for\n            the harmonization. If not provided, covariates_dataframe must be\n            provided instead to compute the partial spectral predictions.\n            These predictions use all set of covariates except those to harmonize.\n            The covariates to harmonize need to be partialed out using the\n            predict_without parameter.\n            The dictionary should contain:\n            - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n            - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n            - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n            This can be obtained using the 'compute_spectral_predictions' method.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the stored number of modes from model.fit() will be used.\n\n    Returns:\n        npt.NDArray[np.floating[Any]]: Array of harmonized values for the\n            variable of interest.\n    \"\"\"\n    if (spectral_predictions_full is None) or (\n        spectral_predictions_partial is None\n    ):\n        if (covariates_dataframe is None) or (covariates_to_harmonize is None):\n            err = (\n                \"Either [covariates_dataframe and covariates_to_harmonize] or \"\n                \"both spectral_predictions_full \"\n                \"and spectral_predictions_partial must be provided.\"\n            )\n            raise ValueError(err)\n        # Validate the new data\n        validation_columns = [cov.name for cov in self.base_model.spec.covariates]\n        utils.general.validate_dataframe(covariates_dataframe, validation_columns)\n\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(self.model_params[\"n_modes\"])\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Reformat encoded queries (for efficiency)\n    encoded_query = np.asarray(encoded_query[:n_modes])\n    encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n    # Predict the mean and std with all covariates\n    full_predictions = self.predict(\n        encoded_query=encoded_query,\n        spectral_predictions=spectral_predictions_full,\n        test_covariates=covariates_dataframe,\n        model_params=model_params,\n        n_modes=n_modes,\n        predict_without=[],\n    )\n\n    # Predict the mean and std without the covariates to harmonize\n    reduced_predictions = self.predict(\n        encoded_query=encoded_query,\n        spectral_predictions=spectral_predictions_partial,\n        test_covariates=covariates_dataframe,\n        model_params=model_params,\n        n_modes=n_modes,\n        predict_without=covariates_to_harmonize,\n    )\n\n    # Reconstruct observed phenotype for query from spectral coefficients\n    observed_phenotype = spectral_coeff_data @ encoded_query[:n_modes]\n\n    # First standardize the variable of interest based on the full model\n    vois_standardized = (\n        observed_phenotype - full_predictions.predictions[\"mu_estimate\"]\n    ) / full_predictions.predictions[\"std_estimate\"]\n\n    # Then return the harmonized values based on the reduced model\n    return np.asarray(\n        (\n            vois_standardized * reduced_predictions.predictions[\"std_estimate\"]\n            + reduced_predictions.predictions[\"mu_estimate\"]\n        ),\n        dtype=np.float64,\n    )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.identify_covariance_structure","title":"<code>identify_covariance_structure(spectral_coeff_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, n_modes: int, covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5, adapt: dict[str, Any] | None = None) -&gt; None</code>","text":"<p>Identify and set the sparse covariance structure for the spectral normative model based on the provided training data and covariance structure input.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_coeff_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples.</p> required <code>n_modes</code> <code>int</code> <p>int Number of eigenmodes to consider.</p> required <code>covariance_structure</code> <code>NDArray[floating[Any]] | float</code> <p>np.ndarray | float Sparse covariance structure to use for the model fitting. If a (2, n_pairs) array of row and column indices are provided, the model will use this structure. If float, the model will estimate the covariance structure based on the training data and the float value will be used as the sparsity threshold for the number of covariance pairs to keep proportional to the number of modes. Defaults to 0.5, meaning that the number of modeled sparse covariance pairs will be half the number of modes.</p> <code>0.5</code> <code>adapt</code> <code>dict[str, Any] | None</code> <p>dict[str, Any] | None Adaptation parameters from a previously fitted model. If provided, the sparse covariance structure from the pretrained model parameters will be used instead of estimating a new one.</p> <code>None</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def identify_covariance_structure(\n    self,\n    spectral_coeff_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    n_modes: int,\n    covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.5,\n    adapt: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Identify and set the sparse covariance structure for the spectral normative\n    model based on the provided training data and covariance structure input.\n\n    Args:\n        spectral_coeff_train_data: np.ndarray\n            Spectral coefficients of training data\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n        n_modes: int\n            Number of eigenmodes to consider.\n        covariance_structure: np.ndarray | float\n            Sparse covariance structure to use for the model fitting. If a\n            (2, n_pairs) array of row and column indices are provided, the model\n            will use this structure. If float, the model will estimate the\n            covariance structure based on the training data and the float value\n            will be used as the sparsity threshold for the number of covariance\n            pairs to keep proportional to the number of modes. Defaults to 0.5,\n            meaning that the number of modeled sparse covariance pairs will be\n            half the number of modes.\n        adapt: dict[str, Any] | None\n            Adaptation parameters from a previously fitted model. If provided,\n            the sparse covariance structure from the pretrained model parameters\n            will be used instead of estimating a new one.\n    \"\"\"\n    if adapt is not None:\n        covariance_structure = adapt[\"pretrained_model_params\"][\n            \"sparse_covariance_structure\"\n        ]\n\n    # Identify sparse covariance structure if a float value is given\n    if isinstance(covariance_structure, float):\n        # Use trained models to compute z-scores\n        spectral_train_z_scores = np.array(\n            [\n                self.base_model.predict(\n                    test_covariates=covariates_dataframe,\n                    model_params=self.direct_model_params[x],\n                )\n                .extend_predictions(\n                    variable_of_interest=spectral_coeff_train_data[:, x],\n                )\n                .predictions[\"z-score\"]\n                for x in range(n_modes)\n            ],\n        ).T\n\n        self.sparse_covariance_structure = (\n            self.identify_sparse_covariance_structure(\n                spectral_train_z_scores,\n                covariance_structure,\n            )\n        )\n    else:\n        self.sparse_covariance_structure = np.array(covariance_structure)\n\n    # Verify that the covariance structure is valid\n    if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n        err = \"Invalid sparse covariance structure.\"\n        raise ValueError(err)\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.identify_sparse_covariance_structure","title":"<code>identify_sparse_covariance_structure(data: npt.NDArray[np.floating[Any]], sparsity_threshold: float = 1) -&gt; npt.NDArray[np.integer[Any]]</code>","text":"<p>Identify the sparse cross-basis covariance structure in the phenotype. This method analyzes the phenotype's spectral coefficients to determine the covariance pairs that need to be modeled.</p> <p>Note: if the batches become too small, this estimate can become less stable in which case it is recommended to provide the sparse covariance structure to the model instead.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Spectral coefficients of training data representing the phenotype in the graph frequency domain :math:<code>(T_{train} \\Psi_{(k)}) \\in R^{N_p \\times k}</code> as a numpy array (n_samples, n_modes).</p> required <code>sparsity_threshold</code> <code>float</code> <p>float Number of strongest correlations to keep (proportional to the number of modes). Defaults to 1, meaning that the number of sparse covariance pairs will be equal to the number of modes. If set to a lower value, fewer covariance pairs will be retained.</p> <code>1</code> <p>Returns:</p> Type Description <code>NDArray[integer[Any]]</code> <p>np.ndarray: A (N, 2) array: the rows and columns of the identified sparse covariance structure.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def identify_sparse_covariance_structure(\n    self,\n    data: npt.NDArray[np.floating[Any]],\n    sparsity_threshold: float = 1,\n) -&gt; npt.NDArray[np.integer[Any]]:\n    \"\"\"\n    Identify the sparse cross-basis covariance structure in the phenotype.\n    This method analyzes the phenotype's spectral coefficients to determine the\n    covariance pairs that need to be modeled.\n\n    Note: if the batches become too small, this estimate can become less stable\n    in which case it is recommended to provide the sparse covariance structure\n    to the model instead.\n\n    Args:\n        data: np.ndarray\n            Spectral coefficients of training data representing the phenotype in\n            the graph frequency domain\n            :math:`(T_{train} \\\\Psi_{(k)}) \\\\in R^{N_p \\\\times k}`\n            as a numpy array (n_samples, n_modes).\n        sparsity_threshold: float\n            Number of strongest correlations to keep (proportional to the number\n            of modes). Defaults to 1, meaning that the number of sparse covariance\n            pairs will be equal to the number of modes. If set to a lower value,\n            fewer covariance pairs will be retained.\n\n    Returns:\n        np.ndarray:\n            A (N, 2) array: the rows and columns of the\n            identified sparse covariance structure.\n    \"\"\"\n    # Start with correlation structure across the whole sample\n    correlations = np.corrcoef(data.T)\n\n    # Remove self-correlations\n    np.fill_diagonal(correlations, 0)\n\n    # Extract the upper triangle of the correlation matrix\n    upper_triangle_indices = np.triu_indices(correlations.shape[0], k=1)\n\n    # Determine the number of correlations to keep\n    n_correlations_to_keep = int(\n        sparsity_threshold * correlations.shape[0],\n    )\n\n    # Find the cutoff value for the top correlations\n    if n_correlations_to_keep &lt; len(upper_triangle_indices[0]):\n        cutoff_value = np.partition(\n            np.abs(correlations[upper_triangle_indices]),\n            -n_correlations_to_keep,\n        )[-n_correlations_to_keep]\n    else:\n        cutoff_value = 0\n        # Warn the user if they are keeping all correlations\n        logger.warning(\n            \"Sparsity threshold is high, keeping all correlations.\",\n        )\n\n    # Now compute the sparsity structure based on the resulting matrix\n    rows, cols = np.where(np.abs(correlations) &gt; cutoff_value)\n\n    # Remove redundant and duplicate pairs\n    rows_lim = rows[rows &lt; cols]\n    cols_lim = cols[rows &lt; cols]\n\n    return np.array([rows_lim, cols_lim]).T\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.load_model","title":"<code>load_model(directory: Path, mmap_mode: MmapMode | None = 'r') -&gt; SpectralNormativeModel</code>  <code>classmethod</code>","text":"<p>Load a spectral normative model instance from the specified save directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Directory to load the fitted model from. A subdirectory named \"spectral_normative_model\" will be searched within this directory.</p> required <code>mmap_mode</code> <code>MmapMode | None</code> <p>MmapMode | None Memory mapping mode for joblib (default: \"r\"). You can set this to None to disable memory-mapping.</p> <code>'r'</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    directory: Path,\n    mmap_mode: MmapMode | None = \"r\",\n) -&gt; SpectralNormativeModel:\n    \"\"\"\n    Load a spectral normative model instance from the specified save directory.\n\n    Args:\n        directory: Path\n            Directory to load the fitted model from. A subdirectory named\n            \"spectral_normative_model\" will be searched within this directory.\n        mmap_mode: MmapMode | None\n            Memory mapping mode for joblib (default: \"r\").\n            You can set this to None to disable memory-mapping.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"spectral_normative_model\",\n    )\n\n    # Check if the pickled joblib file exists in this directory\n    for filename in [\"spectral_model_dict.joblib\", \"eigenmode_basis.joblib\"]:\n        if not (saved_model_dir / filename).exists():\n            err = f\"Model Load Error: Required file '{filename}' does not exist.\"\n            raise FileNotFoundError(err)\n\n    # Load the pickled model dictionary\n    model_dict = joblib.load(saved_model_dir / \"spectral_model_dict.joblib\")\n\n    # Load the eigenmode basis\n    eigenmode_basis = utils.gsp.EigenmodeBasis.load(\n        str(saved_model_dir / \"eigenmode_basis.joblib\"),\n        mmap_mode=mmap_mode,\n    )\n\n    # Create an instance of the class\n    instance = cls(\n        eigenmode_basis=eigenmode_basis,\n        base_model=DirectNormativeModel(\n            spec=model_dict[\"spec\"],\n            defaults=model_dict[\"defaults\"],\n        ),\n    )\n\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n\n    return instance\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.predict","title":"<code>predict(encoded_query: npt.NDArray[np.floating[Any]], *, spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None, test_covariates: pd.DataFrame | None = None, extended: bool = False, model_params: dict[str, Any] | None = None, spectral_coeff_test_data: npt.NDArray[np.floating[Any]] | None = None, n_modes: int | None = None, predict_without: list[str] | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict normative moments (mean, std) for new data using the fitted spectral normative model. Spectral normative modeling can estimate the normative distribution of any variable of interest defined as a spatial query encoded in the latent low-pass graph spectral space.</p> <p>As such, the predict method requires:     - The encoded query(ies) defining the variable(s) of interest.</p> <p>In addition, the method requires either:     - A dataframe of covariates (test_covariates) to be used for inference       of a set of spectral predictions that will subsequently be combined       to yield the normative predictions for the encoded query(ies).     OR     - A dictionary of precomputed spectral predictions (spectral_predictions)       to be used for efficiently predicting the encoded query(ies).</p> <p>The precomputed spectral predictions can be obtained using the 'compute_spectral_predictions' function. This is particularly useful when predicting multiple queries or when the same covariate set is used for multiple predictions, as it avoids redundant computations.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_query</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded query data defining the normative variable of interest. Can be provided as: - shape = (n_modes) for a single query vector - shape = (n_modes, n_queries) for multiple queries predicted at once</p> required <code>spectral_predictions</code> <code>dict[str, NDArray[floating[Any]]] | None</code> <p>dict | None Optional dictionary of precomputed spectral predictions to use for the prediction. If not provided, test_covariates must be provided instead to compute the spectral predictions. The dictionary should contain: - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes) - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes) - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs) This can be obtained using the 'compute_spectral_predictions' method.</p> <code>None</code> <code>test_covariates</code> <code>DataFrame | None</code> <p>pd.DataFrame | None DataFrame containing the new covariate data to predict. This must include all specified covariates. Note: covariates listed in predict_without will be ignored and are hence not required.</p> <code>None</code> <code>extended</code> <code>bool</code> <p>bool (default: False) If True, return additional stats such as log-likelihood, centiles, etc. Note that extended predictions require spectral_coeff_test_data to be provided in addition to the covariates.</p> <code>False</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>spectral_coeff_test_data</code> <code>NDArray[floating[Any]] | None</code> <p>np.ndarray | None Optional spectral coefficient of test data for the phenotype being modeled :math:<code>(T_{test} \\Psi_{(k)}) \\in R^{N_{test} \\times k}</code> (only required for extended predictions). Expects a numpy array (n_samples, n_modes)</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the number of modes from model_params will be used.</p> <code>None</code> <code>predict_without</code> <code>list[str] | None</code> <p>list[str] | None Optional list of covariate names to ignore during prediction. This can be used to check the effect of removing certain covariates from the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>pd.DataFrame: DataFrame containing the predicted moments (mean, std) for the variable of interest defined by the encoded query.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    encoded_query: npt.NDArray[np.floating[Any]],\n    *,\n    spectral_predictions: dict[str, npt.NDArray[np.floating[Any]]] | None = None,\n    test_covariates: pd.DataFrame | None = None,\n    extended: bool = False,\n    model_params: dict[str, Any] | None = None,\n    spectral_coeff_test_data: npt.NDArray[np.floating[Any]] | None = None,\n    n_modes: int | None = None,\n    predict_without: list[str] | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict normative moments (mean, std) for new data using the fitted spectral\n    normative model.\n    Spectral normative modeling can estimate the normative distribution of any\n    variable of interest defined as a spatial query encoded in the latent low-pass\n    graph spectral space.\n\n    As such, the predict method requires:\n        - The encoded query(ies) defining the variable(s) of interest.\n\n    In addition, the method requires either:\n        - A dataframe of covariates (test_covariates) to be used for inference\n          of a set of spectral predictions that will subsequently be combined\n          to yield the normative predictions for the encoded query(ies).\n        OR\n        - A dictionary of precomputed spectral predictions (spectral_predictions)\n          to be used for efficiently predicting the encoded query(ies).\n\n    The precomputed spectral predictions can be obtained using the\n    'compute_spectral_predictions' function. This is particularly useful when\n    predicting multiple queries or when the same covariate set is used for\n    multiple predictions, as it avoids redundant computations.\n\n    Args:\n        encoded_query: np.ndarray\n            Encoded query data defining the normative variable of interest.\n            Can be provided as:\n            - shape = (n_modes) for a single query vector\n            - shape = (n_modes, n_queries) for multiple queries predicted at once\n        spectral_predictions: dict | None\n            Optional dictionary of precomputed spectral predictions to use for\n            the prediction. If not provided, test_covariates must be provided\n            instead to compute the spectral predictions.\n            The dictionary should contain:\n            - 'eigenmode_mu_estimates': np.ndarray (n_samples, n_modes)\n            - 'eigenmode_std_estimates': np.ndarray (n_samples, n_modes)\n            - 'rho_estimates': np.ndarray (n_samples, n_covariance_pairs)\n            This can be obtained using the 'compute_spectral_predictions' method.\n        test_covariates: pd.DataFrame | None\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n            Note: covariates listed in predict_without will be ignored and are\n            hence not required.\n        extended: bool (default: False)\n            If True, return additional stats such as log-likelihood, centiles, etc.\n            Note that extended predictions require spectral_coeff_test_data to be\n            provided in addition to the covariates.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        spectral_coeff_test_data: np.ndarray | None\n            Optional spectral coefficient of test data for the phenotype being\n            modeled :math:`(T_{test} \\\\Psi_{(k)}) \\\\in R^{N_{test} \\\\times k}`\n            (only required for extended predictions).\n            Expects a numpy array (n_samples, n_modes)\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the number of modes from model_params will be used.\n        predict_without: list[str] | None\n            Optional list of covariate names to ignore during prediction.\n            This can be used to check the effect of removing certain covariates\n            from the model.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the predicted moments (mean, std) for\n            the variable of interest defined by the encoded query.\n    \"\"\"\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(model_params[\"n_modes\"])\n\n    if self.base_model.spec is None:\n        err = \"The base model is not specified. Cannot predict new data.\"\n        raise ValueError(err)\n\n    if spectral_predictions is None:\n        if test_covariates is None:\n            err = \"Either test_covariates or spectral_predictions must be provided.\"\n            raise ValueError(err)\n\n        # Compute spectral predictions if not provided\n        spectral_predictions = self.compute_spectral_predictions(\n            test_covariates=test_covariates,\n            model_params=model_params,\n            n_modes=n_modes,\n            predict_without=predict_without,\n        )\n    elif test_covariates is not None:\n        logger.warning(\n            \"Both test_covariates and spectral_predictions are provided.\"\n            \" Ignoring test_covariates and using spectral_predictions.\",\n        )\n        if predict_without is not None:\n            logger.warning(\n                \"predict_without is ignored when spectral_predictions\"\n                \" are provided directly.\",\n            )\n\n    # Unpack spectral predictions\n    self._validate_spectral_predictions(spectral_predictions)\n    eigenmode_mu_estimates = spectral_predictions[\"eigenmode_mu_estimates\"]\n    eigenmode_std_estimates = spectral_predictions[\"eigenmode_std_estimates\"]\n    rho_estimates = spectral_predictions[\"rho_estimates\"]\n\n    # Reformat encoded queries (for efficiency)\n    encoded_query = np.asarray(encoded_query[:n_modes])\n    encoded_query = encoded_query.reshape(n_modes, -1, order=\"F\")\n\n    # Compute the predictions\n    predictions = self._predict_from_spectral_estimates(\n        encoded_query=encoded_query,\n        eigenmode_mu_estimates=eigenmode_mu_estimates,\n        eigenmode_std_estimates=eigenmode_std_estimates,\n        rho_estimates=rho_estimates,\n        model_params=model_params,\n        n_modes=n_modes,\n    )\n\n    # Check if extended predictions are requested\n    if extended:\n        if spectral_coeff_test_data is None:\n            err = (\n                \"Extended predictions require spectral_coeff_test_data\"\n                \" to be provided.\"\n            )\n            raise ValueError(err)\n        # Add extended statistics to predictions (e.g. centiles, log-loss, etc.)\n        predictions.extend_predictions(\n            variable_of_interest=spectral_coeff_test_data @ encoded_query,\n        )\n\n    return predictions\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.reduce_model","title":"<code>reduce_model(n_modes: int, *, inplace: bool = False) -&gt; SpectralNormativeModel</code>","text":"<p>Create a reduced spectral normative model using only the first n_modes.</p> <p>Parameters:</p> Name Type Description Default <code>n_modes</code> <code>int</code> <p>int Number of modes to retain in the reduced model. Must be less than or equal to the current number of modes considered by the model.</p> required <code>inplace</code> <code>bool</code> <p>bool (default: False) If True, modify the current model instance to reduce its modes. If False, return a new SpectralNormativeModel instance with the reduced modes.</p> <code>False</code> <p>Returns:     SpectralNormativeModel         A new SpectralNormativeModel instance with reduced number of modes.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def reduce_model(\n    self,\n    n_modes: int,\n    *,\n    inplace: bool = False,\n) -&gt; SpectralNormativeModel:\n    \"\"\"\n    Create a reduced spectral normative model using only the first n_modes.\n\n    Args:\n        n_modes: int\n            Number of modes to retain in the reduced model. Must be less than or\n            equal to the current number of modes considered by the model.\n        inplace: bool (default: False)\n            If True, modify the current model instance to reduce its modes. If\n            False, return a new SpectralNormativeModel instance with the reduced\n            modes.\n    Returns:\n        SpectralNormativeModel\n            A new SpectralNormativeModel instance with reduced number of modes.\n    \"\"\"\n    if (n_modes &gt; self.eigenmode_basis.n_modes) or (\n        hasattr(self, \"model_params\") and n_modes &gt; self.model_params[\"n_modes\"]\n    ):\n        available_modes = self.eigenmode_basis.n_modes\n        if hasattr(self, \"model_params\"):\n            available_modes = min(available_modes, self.model_params[\"n_modes\"])\n        err = f\"Cannot reduce to {n_modes} modes, only {available_modes} available.\"\n        raise ValueError(err)\n\n    # Create a reduced eigenbasis\n    reduced_eigenbasis = self.eigenmode_basis.reduce(n_modes)\n\n    if inplace:\n        return_model = self\n        return_model.eigenmode_basis = reduced_eigenbasis\n    else:\n        return_model = SpectralNormativeModel(\n            base_model=self.base_model,\n            eigenmode_basis=reduced_eigenbasis,\n        )\n        return_model.model_params = self.model_params  # Copy model parameters\n\n    # Update model parameters to reflect reduced modes\n    if hasattr(self, \"model_params\"):\n        new_model_params: dict[str, Any] = {}\n        new_model_params[\"n_modes\"] = n_modes\n        new_model_params[\"sample_size\"] = self.model_params[\"sample_size\"]\n        new_model_params[\"direct_model_params\"] = self.model_params[\n            \"direct_model_params\"\n        ][:n_modes]\n        valid_cov_indices = np.where(\n            (\n                return_model.model_params[\"sparse_covariance_structure\"][:, 0]\n                &lt; n_modes\n            )\n            &amp; (\n                return_model.model_params[\"sparse_covariance_structure\"][:, 1]\n                &lt; n_modes\n            ),\n        )[0]\n        new_model_params[\"sparse_covariance_structure\"] = return_model.model_params[\n            \"sparse_covariance_structure\"\n        ][valid_cov_indices]\n        new_model_params[\"covariance_model_params\"] = [\n            return_model.model_params[\"covariance_model_params\"][i]\n            for i in valid_cov_indices\n        ]\n        new_model_params[\"n_params\"] = self.model_params.get(\"n_params\", None)\n        return_model.model_params = new_model_params\n\n    return return_model\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SpectralNormativeModel.save_model","title":"<code>save_model(directory: Path) -&gt; None</code>","text":"<p>Save the fitted spectral normative model to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Directory to save the fitted model. A subdirectory named \"spectral_normative_model\" will be created within this directory.</p> required Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path) -&gt; None:\n    \"\"\"\n    Save the fitted spectral normative model to the specified directory.\n\n    Args:\n        directory: Path\n            Directory to save the fitted model. A subdirectory named\n            \"spectral_normative_model\" will be created within this directory.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(\n        directory,\n        \"spectral_normative_model\",\n    )\n\n    # Save the eigenmode basis separately\n    self.eigenmode_basis.save(str(saved_model_dir / \"eigenmode_basis.joblib\"))\n\n    # Save the model\n    model_dict = {\n        \"spec\": self.base_model.spec,\n        \"defaults\": self.base_model.defaults,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n    joblib.dump(model_dict, saved_model_dir / \"spectral_model_dict.joblib\")\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SplineSpec","title":"<code>SplineSpec</code>  <code>dataclass</code>","text":"<p>Specification for spline basis construction.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>int</code> <p>int Degrees of freedom (number of basis functions).</p> <code>degree</code> <code>int</code> <p>int Degree of the spline (e.g., 3 for cubic splines).</p> <code>lower_bound</code> <code>float</code> <p>float Lower boundary for the spline domain.</p> <code>upper_bound</code> <code>float</code> <p>float Upper boundary for the spline domain.</p> <code>knots</code> <code>list[float] | None</code> <p>Optional[List[float]] Optional list of internal knot locations within the spline domain (excluding the boundary knots). Must be strictly increasing and contain exactly <code>df - degree - 1</code> values. If unspecified, then equally spaced quantiles of the input data are used.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass SplineSpec:\n    \"\"\"\n    Specification for spline basis construction.\n\n    Attributes:\n        df: int\n            Degrees of freedom (number of basis functions).\n        degree: int\n            Degree of the spline (e.g., 3 for cubic splines).\n        lower_bound: float\n            Lower boundary for the spline domain.\n        upper_bound: float\n            Upper boundary for the spline domain.\n        knots: Optional[List[float]]\n            Optional list of internal knot locations within the spline domain\n            (excluding the boundary knots). Must be strictly increasing and\n            contain exactly `df - degree - 1` values. If unspecified, then\n            equally spaced quantiles of the input data are used.\n    \"\"\"\n\n    lower_bound: float\n    upper_bound: float\n    df: int = DEFAULT_SPLINE_DF\n    degree: int = DEFAULT_SPLINE_DEGREE\n    knots: list[float] | None = None\n\n    # Validation checks for the spline specification.\n    def __post_init__(self) -&gt; None:\n        # Check that df (degrees of freedom) is greater than degree\n        if self.df &lt;= self.degree:\n            err = \"df (degrees of freedom) must be greater than degree.\"\n            raise ValueError(err)\n        # Check that degree is at least 1\n        if self.degree &lt; 1:\n            err = \"degree must be at least 1.\"\n            raise ValueError(err)\n        # Check that lower_bound and upper_bound are numeric\n        if self.lower_bound &gt;= self.upper_bound:\n            err = \"lower_bound must be less than upper_bound.\"\n            raise ValueError(err)\n        if self.knots is not None:\n            if not all(isinstance(k, (int, float)) for k in self.knots):\n                err = \"All knots must be numeric (int or float).\"\n                raise TypeError(err)\n            # Check if knots are strictly increasing\n            if not all(x &lt; y for x, y in zip(self.knots, self.knots[1:])):\n                err = \"Knots must be strictly increasing.\"\n                raise ValueError(err)\n            # Check if knots are within bounds\n            if any(k &lt; self.lower_bound or k &gt; self.upper_bound for k in self.knots):\n                err = (\n                    \"All knots must be within the bounds defined by \"\n                    \"lower_bound and upper_bound.\"\n                )\n                raise ValueError(err)\n            # Check if the number of knots is correct\n            if len(self.knots) != (self.df - self.degree - 1):\n                err = (\n                    f\"knots must contain exactly {self.df - self.degree - 1} \"\n                    f\"values, got {len(self.knots)}.\"\n                )\n                raise ValueError(err)\n\n    @classmethod\n    def create_spline_spec(\n        cls,\n        values: pd.Series[float],\n        df: int = DEFAULT_SPLINE_DF,\n        degree: int = DEFAULT_SPLINE_DEGREE,\n        knots: list[float] | None = None,\n        extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n        lower_bound: float | None = None,\n        upper_bound: float | None = None,\n    ) -&gt; SplineSpec:\n        \"\"\"\n        Create a spline specification from a pandas Series.\n\n        Args:\n            values: pd.Series\n                The list of input values to make the spline.\n            df: int\n                Degrees of freedom for the spline (default is 5).\n            degree: int\n                Degree of the spline (default is 3).\n            knots: list[float] | None\n                [Optional] List of internal knot locations within the spline domain.\n                If None, equally spaced quantiles of the input data are used.\n            extrapolation_factor: float, positive, default is 0.1\n                [Optional] Factor to extend the lower and upper bounds of the spline\n                domain.\n            lower_bound: float | None\n                [Optional] Lower boundary for the spline domain. If None, it is set to\n                `values.min() - extrapolation_factor * (values.max() - values.min())`.\n            upper_bound: float | None\n                [Optional] Upper boundary for the spline domain. If None, it is set to\n                `values.max() + extrapolation_factor * (values.max() - values.min())`.\n\n        Returns:\n            SplineSpec\n                The created spline specification.\n        \"\"\"\n        extrapolation = extrapolation_factor * (values.max() - values.min())\n        if lower_bound is None:\n            lower_bound = values.min() - extrapolation\n        if upper_bound is None:\n            upper_bound = values.max() + extrapolation\n        if knots is None:\n            # Use equally spaced quantiles as knots\n            knots = np.linspace(lower_bound, upper_bound, df - degree + 1)[\n                1:-1\n            ].tolist()\n        return cls(\n            lower_bound=lower_bound,\n            upper_bound=upper_bound,\n            df=df,\n            degree=degree,\n            knots=knots,\n        )\n</code></pre>"},{"location":"api/snm/#spectranorm.snm.SplineSpec.create_spline_spec","title":"<code>create_spline_spec(values: pd.Series[float], df: int = DEFAULT_SPLINE_DF, degree: int = DEFAULT_SPLINE_DEGREE, knots: list[float] | None = None, extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR, lower_bound: float | None = None, upper_bound: float | None = None) -&gt; SplineSpec</code>  <code>classmethod</code>","text":"<p>Create a spline specification from a pandas Series.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series[float]</code> <p>pd.Series The list of input values to make the spline.</p> required <code>df</code> <code>int</code> <p>int Degrees of freedom for the spline (default is 5).</p> <code>DEFAULT_SPLINE_DF</code> <code>degree</code> <code>int</code> <p>int Degree of the spline (default is 3).</p> <code>DEFAULT_SPLINE_DEGREE</code> <code>knots</code> <code>list[float] | None</code> <p>list[float] | None [Optional] List of internal knot locations within the spline domain. If None, equally spaced quantiles of the input data are used.</p> <code>None</code> <code>extrapolation_factor</code> <code>float</code> <p>float, positive, default is 0.1 [Optional] Factor to extend the lower and upper bounds of the spline domain.</p> <code>DEFAULT_SPLINE_EXTRAPOLATION_FACTOR</code> <code>lower_bound</code> <code>float | None</code> <p>float | None [Optional] Lower boundary for the spline domain. If None, it is set to <code>values.min() - extrapolation_factor * (values.max() - values.min())</code>.</p> <code>None</code> <code>upper_bound</code> <code>float | None</code> <p>float | None [Optional] Upper boundary for the spline domain. If None, it is set to <code>values.max() + extrapolation_factor * (values.max() - values.min())</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>SplineSpec</code> <p>SplineSpec The created spline specification.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef create_spline_spec(\n    cls,\n    values: pd.Series[float],\n    df: int = DEFAULT_SPLINE_DF,\n    degree: int = DEFAULT_SPLINE_DEGREE,\n    knots: list[float] | None = None,\n    extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n    lower_bound: float | None = None,\n    upper_bound: float | None = None,\n) -&gt; SplineSpec:\n    \"\"\"\n    Create a spline specification from a pandas Series.\n\n    Args:\n        values: pd.Series\n            The list of input values to make the spline.\n        df: int\n            Degrees of freedom for the spline (default is 5).\n        degree: int\n            Degree of the spline (default is 3).\n        knots: list[float] | None\n            [Optional] List of internal knot locations within the spline domain.\n            If None, equally spaced quantiles of the input data are used.\n        extrapolation_factor: float, positive, default is 0.1\n            [Optional] Factor to extend the lower and upper bounds of the spline\n            domain.\n        lower_bound: float | None\n            [Optional] Lower boundary for the spline domain. If None, it is set to\n            `values.min() - extrapolation_factor * (values.max() - values.min())`.\n        upper_bound: float | None\n            [Optional] Upper boundary for the spline domain. If None, it is set to\n            `values.max() + extrapolation_factor * (values.max() - values.min())`.\n\n    Returns:\n        SplineSpec\n            The created spline specification.\n    \"\"\"\n    extrapolation = extrapolation_factor * (values.max() - values.min())\n    if lower_bound is None:\n        lower_bound = values.min() - extrapolation\n    if upper_bound is None:\n        upper_bound = values.max() + extrapolation\n    if knots is None:\n        # Use equally spaced quantiles as knots\n        knots = np.linspace(lower_bound, upper_bound, df - degree + 1)[\n            1:-1\n        ].tolist()\n    return cls(\n        lower_bound=lower_bound,\n        upper_bound=upper_bound,\n        df=df,\n        degree=degree,\n        knots=knots,\n    )\n</code></pre>"},{"location":"api/utils/general/","title":"General Utilities","text":"<p>The general utilities module provides a collection of helper functions that are used across various parts of the SpectraNorm package. These functions include tools for logging, data validation, directory management, and other general-purpose tasks that support the core functionality of the package.</p>"},{"location":"api/utils/general/#spectranorm.utils.general","title":"<code>spectranorm.utils.general</code>","text":"<p>utils/general.py</p> <p>General utility functions for spectranorm.</p>"},{"location":"api/utils/general/#spectranorm.utils.general.ReportTimeFormatter","title":"<code>ReportTimeFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom log formatter that injects general.report_time().</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>class ReportTimeFormatter(logging.Formatter):\n    \"\"\"Custom log formatter that injects general.report_time().\"\"\"\n\n    def format(self, record: logging.LogRecord) -&gt; str:\n        record.report_time = report_time()\n        return super().format(record)\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.ensure_dir","title":"<code>ensure_dir(file_name: Path) -&gt; Path</code>","text":"<p>Ensure that the directory for the given file name exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>Path</code> <p>Path The file name for which to ensure the directory exists.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path The original file name.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def ensure_dir(file_name: Path) -&gt; Path:\n    \"\"\"\n    Ensure that the directory for the given file name exists.\n\n    Args:\n        file_name: Path\n            The file name for which to ensure the directory exists.\n\n    Returns:\n        Path\n            The original file name.\n    \"\"\"\n    file_name.parent.mkdir(parents=True, exist_ok=True)\n    return file_name\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.get_logger","title":"<code>get_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger</code>","text":"<p>Get a logger with standardized formatting and time reporting.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (usually name).</p> required <code>level</code> <code>int</code> <p>Logging level, default INFO.</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger with a custom ReportTimeFormatter.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def get_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger with standardized formatting and time reporting.\n\n    Args:\n        name: Logger name (usually __name__).\n        level: Logging level, default INFO.\n\n    Returns:\n        logging.Logger with a custom ReportTimeFormatter.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if not logger.hasHandlers():\n        handler = logging.StreamHandler()\n        formatter = ReportTimeFormatter(\n            fmt=\"%(report_time)s : [%(levelname)s] - %(name)s - %(message)s\",\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(level)\n    return logger\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.prepare_save_directory","title":"<code>prepare_save_directory(directory: Path, subdirectory: str = 'saved_model') -&gt; Path</code>","text":"<p>Prepare a directory to save a model.</p> <p>A subdirectory named 'saved_model' will be created if it does not exist. If this directory exists, but is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to a directory to save the model.</p> required <code>subdirectory</code> <code>str</code> <p>Name of the subdirectory to create (default: \"saved_model\").</p> <code>'saved_model'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path The path to the created subdirectory.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def prepare_save_directory(directory: Path, subdirectory: str = \"saved_model\") -&gt; Path:\n    \"\"\"\n    Prepare a directory to save a model.\n\n    A subdirectory named 'saved_model' will be created if it does not exist.\n    If this directory exists, but is not empty, an error is raised.\n\n    Args:\n        directory (Path): Path to a directory to save the model.\n        subdirectory (str): Name of the subdirectory to create (default: \"saved_model\").\n\n    Returns:\n        Path\n            The path to the created subdirectory.\n    \"\"\"\n    # Check if the directory exists\n    if not directory.exists():\n        err = f\"Model Save Error: Directory '{directory}' does not exist.\"\n        raise FileNotFoundError(err)\n    # Check if the subdirectory exists and is empty\n    saved_model_dir = directory / subdirectory\n    if saved_model_dir.exists():\n        if any(saved_model_dir.iterdir()):\n            err = f\"Model Save Error: Directory '{saved_model_dir}' is not empty.\"\n            raise ValueError(err)\n    else:\n        saved_model_dir.mkdir(parents=True, exist_ok=True)\n\n    return saved_model_dir\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.report_time","title":"<code>report_time(*, relative_to: float | None = None, absolute: bool = False) -&gt; str | float</code>","text":"<p>Report the current time or the time elapsed since a given reference point.</p> <p>Parameters:</p> Name Type Description Default <code>relative_to</code> <code>float | None</code> <p>float | None The reference time in seconds since the epoch. If None, report the current time.</p> <code>None</code> <code>absolute</code> <code>bool</code> <p>bool If True, report the absolute time format (float). If False, report the time in a human-readable format (str).</p> <code>False</code> <p>Returns:</p> Type Description <code>str | float</code> <p>str | float The current time or the elapsed time since the reference point.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def report_time(\n    *,\n    relative_to: float | None = None,\n    absolute: bool = False,\n) -&gt; str | float:\n    \"\"\"\n    Report the current time or the time elapsed since a given reference point.\n\n    Args:\n        relative_to: float | None\n            The reference time in seconds since the epoch. If None, report the current\n            time.\n        absolute: bool\n            If True, report the absolute time format (float). If False, report the time\n            in a human-readable format (str).\n\n    Returns:\n        str | float\n            The current time or the elapsed time since the reference point.\n    \"\"\"\n    if relative_to is not None:\n        elapsed = time.time() - relative_to\n        return elapsed if absolute else str(datetime.timedelta(seconds=elapsed))\n\n    now = time.time()\n    return (\n        now\n        if absolute\n        else datetime.datetime.fromtimestamp(\n            now,\n            tz=datetime.datetime.now().astimezone().tzinfo,\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.suppress_output","title":"<code>suppress_output() -&gt; Iterator[None]</code>","text":"<p>Context manager to suppress stdout and stderr output.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>@contextmanager\ndef suppress_output() -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to suppress stdout and stderr output.\n    \"\"\"\n    with Path(os.devnull).open(\"w\") as fnull:\n        old_stdout, old_stderr = os.dup(1), os.dup(2)\n        try:\n            os.dup2(fnull.fileno(), 1)  # Redirect stdout\n            os.dup2(fnull.fileno(), 2)  # Redirect stderr\n            yield\n        finally:\n            os.dup2(old_stdout, 1)  # Restore\n            os.dup2(old_stderr, 2)  # Restore\n            os.close(old_stdout)\n            os.close(old_stderr)\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.validate_dataframe","title":"<code>validate_dataframe(dataframe: pd.DataFrame, column_names: list[str]) -&gt; None</code>","text":"<p>Validate the input DataFrame to ensure all required columns are available.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>pd.DataFrame The DataFrame to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame is not valid.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def validate_dataframe(dataframe: pd.DataFrame, column_names: list[str]) -&gt; None:\n    \"\"\"\n    Validate the input DataFrame to ensure all required columns are available.\n\n    Args:\n        dataframe: pd.DataFrame\n            The DataFrame to validate.\n\n    Raises:\n        TypeError: If the DataFrame is not valid.\n    \"\"\"\n    if not isinstance(dataframe, pd.DataFrame):\n        err = \"Input data must be a pandas DataFrame.\"\n        raise TypeError(err)\n    # report if a column is missing\n    missing_columns = [col for col in column_names if col not in dataframe.columns]\n    if missing_columns:\n        err = f\"Missing columns in DataFrame: {', '.join(missing_columns)}\"\n        raise ValueError(err)\n</code></pre>"},{"location":"api/utils/general/#spectranorm.utils.general.validate_load_directory","title":"<code>validate_load_directory(directory: Path, subdirectory: str = 'saved_model') -&gt; Path</code>","text":"<p>Validate the directory structure for loading a model.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to the main directory.</p> required <code>subdirectory</code> <code>str</code> <p>Name of the subdirectory to check (default: \"saved_model\").</p> <code>'saved_model'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path The path to the validated subdirectory.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def validate_load_directory(directory: Path, subdirectory: str = \"saved_model\") -&gt; Path:\n    \"\"\"\n    Validate the directory structure for loading a model.\n\n    Args:\n        directory (Path): Path to the main directory.\n        subdirectory (str): Name of the subdirectory to check (default: \"saved_model\").\n\n    Returns:\n        Path\n            The path to the validated subdirectory.\n    \"\"\"\n    # Check if the directory exists\n    if not directory.exists():\n        err = f\"Model Load Error: Directory '{directory}' does not exist.\"\n        raise FileNotFoundError(err)\n    # Check if the subdirectory exists\n    saved_model_dir = directory / subdirectory\n    if not saved_model_dir.exists():\n        err = f\"Model Load Error: Directory '{saved_model_dir}' does not exist.\"\n        raise FileNotFoundError(err)\n\n    return saved_model_dir\n</code></pre>"},{"location":"api/utils/gsp/","title":"Graph Signal Processing Utilities","text":"<p>The GSP module provides specialized tools for spectral analysis on graphs. This includes functions for computing graph spectral bases (eigenmodes), performing graph Fourier transforms (encoding), and inverse transforms (decoding).</p>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp","title":"<code>spectranorm.utils.gsp</code>","text":"<p>utils/gsp.py</p> <p>Graph Signal Processing (GSP) functions for the Spectranorm package.</p>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis","title":"<code>EigenmodeBasis</code>  <code>dataclass</code>","text":"<p>Data class to hold eigenmode basis information.</p> <p>Attributes:</p> Name Type Description <code>eigenvalues</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Eigenvalues of the Basis (n_modes,).</p> <code>eigenvectors</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Eigenvectors corresponding to the eigenvalues (n_features, n_modes). This is the matrix :math:<code>\\Psi_{(k)} \\in \\mathbb{R}^{N_v \\times k}</code> where k is the number of modes included in the basis.</p> <code>mass_matrix</code> <code>NDArray[floating[Any]] | csr_matrix | None</code> <p>np.ndarray | sparse.csr_matrix | None Mass matrix associated with the eigenmodes (optional). This can be used in generalized eigenvalue problems (e.g. random walk Laplacian), in which case the eigenmodes satisfy :math:<code>\\Psi^T M \\Psi = I</code>.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>@dataclass\nclass EigenmodeBasis:\n    \"\"\"\n    Data class to hold eigenmode basis information.\n\n    Attributes:\n        eigenvalues: np.ndarray\n            Eigenvalues of the Basis (n_modes,).\n        eigenvectors: np.ndarray\n            Eigenvectors corresponding to the eigenvalues (n_features, n_modes).\n            This is the matrix :math:`\\\\Psi_{(k)} \\\\in \\\\mathbb{R}^{N_v \\\\times k}`\n            where k is the number of modes included in the basis.\n        mass_matrix: np.ndarray | sparse.csr_matrix | None\n            Mass matrix associated with the eigenmodes (optional). This can be used\n            in generalized eigenvalue problems (e.g. random walk Laplacian), in which\n            case the eigenmodes satisfy :math:`\\\\Psi^T M \\\\Psi = I`.\n    \"\"\"\n\n    eigenvalues: npt.NDArray[np.floating[Any]]\n    eigenvectors: npt.NDArray[np.floating[Any]]\n    mass_matrix: npt.NDArray[np.floating[Any]] | sparse.csr_matrix | None = None\n\n    # Additional attributes\n    def __post_init__(self) -&gt; None:\n        self.n_modes = self.eigenvalues.shape[0]\n        self.n_features = self.eigenvectors.shape[0]\n\n        if self.eigenvectors.shape[1] != self.n_modes:\n            err = (\n                f\"Eigenvectors must have {self.n_modes} modes, \"\n                f\"got {self.eigenvectors.shape[1]}.\"\n            )\n            raise ValueError(err)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the EigenmodeBasis.\n        \"\"\"\n        return f\"EigenmodeBasis(n_modes={self.n_modes}, n_features={self.n_features})\"\n\n    def inverted_eigenvectors(self) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Compute the inverse of the eigenvector matrix.\n\n        .. math::\n            \\\\Psi^{-1} = \\\\Psi^T M\n\n        Returns:\n            np.ndarray\n                Inverse of the eigenvector matrix (n_modes, n_features).\n        \"\"\"\n        if self.mass_matrix is None:\n            # Standard eigenvalue problem\n            return self.eigenvectors.T\n        # Generalized eigenvalue problem (sparse @ dense is more efficient)\n        return (self.mass_matrix.T @ self.eigenvectors).T\n\n    @classmethod\n    def load(cls, filepath: str, mmap_mode: MmapMode | None = \"r\") -&gt; EigenmodeBasis:\n        \"\"\"\n        Load an EigenmodeBasis instance from a joblib file.\n\n        Args:\n            filepath: str\n                Path to the joblib file.\n            mmap_mode: MmapMode | None\n                Memory mapping mode for joblib (default: \"r\").\n                You can set this to None to disable memory-mapping.\n\n        Returns:\n            EigenmodeBasis instance\n        \"\"\"\n        data = joblib.load(filepath, mmap_mode=mmap_mode)\n        # Expecting the saved file to contain a dict with these keys:\n        # 'eigenvalues', 'eigenvectors'\n        return cls(\n            eigenvalues=data[\"eigenvalues\"],\n            eigenvectors=data[\"eigenvectors\"],\n            mass_matrix=data.get(\"mass_matrix\"),\n        )\n\n    def save(\n        self,\n        filepath: str,\n        compress: int = 0,\n        *,\n        overwrite: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Save the EigenmodeBasis instance to a joblib file.\n\n        Args:\n            filepath: str\n                Path to save the joblib file.\n            compress: int\n                Compression level for joblib (default: 0).\n                Note: Compression is disabled by default to enable support for\n                memory-mapped arrays. However, if you do not need memory-mapping,\n                and want to reduce file size, you can enable compression by setting\n                the compress parameter to a positive integer.\n            overwrite: bool\n                Whether to overwrite an existing file (default: False).\n        \"\"\"\n        # Handle file overwrite\n        if Path(filepath).exists():\n            if not overwrite:\n                err = f\"File '{filepath}' already exists and overwrite is disabled.\"\n                raise FileExistsError(err)\n            Path(filepath).unlink(missing_ok=True)\n        data = {\n            \"eigenvalues\": self.eigenvalues,\n            \"eigenvectors\": self.eigenvectors,\n            \"mass_matrix\": self.mass_matrix,\n        }\n        joblib.dump(data, filepath, compress=compress)\n\n    def reduce(self, n_modes: int, *, inplace: bool = True) -&gt; EigenmodeBasis:\n        \"\"\"\n        Reduce the EigenmodeBasis to only contain the first n_modes.\n\n        This method is useful to reduce the size of the basis for efficiency (e.g.,\n        to remove less important modes/degenerate modes before further processing).\n\n        Args:\n            n_modes: int\n                Number of modes to retain. This must be less than or equal to the\n                current number of modes.\n            inplace: bool\n                Whether to modify the current instance or return a new one.\n                By default, this is True (modify in place, optimizing memory usage).\n\n        Returns:\n            EigenmodeBasis\n                A new EigenmodeBasis instance with reduced modes.\n        \"\"\"\n        if n_modes &gt; self.n_modes:\n            err = f\"Cannot reduce to {n_modes} modes, only {self.n_modes} available.\"\n            raise ValueError(err)\n        if inplace:\n            self.eigenvalues = self.eigenvalues[:n_modes]\n            self.eigenvectors = self.eigenvectors[:, :n_modes]\n            self.n_modes = n_modes\n            return self\n        # else return a new instance\n        return EigenmodeBasis(\n            eigenvalues=self.eigenvalues[:n_modes],\n            eigenvectors=self.eigenvectors[:, :n_modes],\n            mass_matrix=self.mass_matrix,\n        )\n\n    def encode(\n        self,\n        signals: npt.NDArray[np.floating[Any]],\n        n_modes: int | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Encode a signal using the eigenmode basis.\n\n        Given an eigenmode set :math:`\\\\Psi` where :math:`L = \\\\Psi \\\\Lambda \\\\Psi^{-1}`\n        and a list of signals :math:`x`, the encoded signals :math:`\\tilde{x}` are given\n        by:\n\n        .. math::\n            \\\\tilde{x} = \\\\Psi^{-1} x = \\\\Psi^T M x\n\n        Args:\n            signals: np.ndarray\n                Signals to encode (n_signals, n_features).\n            n_modes: int | None\n                Number of modes to use for encoding. If None, use all available modes.\n\n        Returns:\n            np.ndarray\n                Encoded signal in the eigenmode basis (n_signals, n_modes).\n        \"\"\"\n        if signals.shape[-1] != self.n_features:\n            err = (\n                f\"Signal must have {self.n_features} features, got {signals.shape[-1]}.\"\n            )\n            raise ValueError(err)\n\n        if n_modes is None:\n            n_modes = self.n_modes\n\n        if self.mass_matrix is not None:\n            # Generalized eigenvalue problem\n            return signals @ (self.mass_matrix.T @ self.eigenvectors[:, :n_modes])\n        return signals @ self.eigenvectors[:, :n_modes]\n\n    def decode(\n        self,\n        encoded_signals: npt.NDArray[np.floating[Any]],\n        n_modes: int | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Decode encoded signals :math:`\\tilde{x}` using the eigenmode basis.\n\n        Given an eigenmode set :math:`\\\\Psi` where :math:`L = \\\\Psi \\\\Lambda \\\\Psi^{-1}`\n        and a list of encoded signals :math:`\\tilde{x}`, the decoded signals\n        :math:`\\\\hat{x}` are given by:\n\n        .. math::\n            \\\\hat{x} = \\\\Psi \\\\tilde{x}\n\n        Args:\n            encoded_signals: np.ndarray\n                Encoded signals in the eigenmode basis (n_signals, n_modes).\n            n_modes: int | None\n                Number of modes used for decoding. If None, use all available modes.\n                This should not exceed the number of modes in the encoded_signals\n                nor the number of modes in the eigenmode basis.\n\n        Returns:\n            np.ndarray\n                Decoded signal in the original feature space (n_signals, n_features).\n        \"\"\"\n        if n_modes is None:\n            n_modes = self.n_modes\n\n        if encoded_signals.shape[-1] &lt; n_modes:\n            err = (\n                f\"Encoded signal must have at least {n_modes} modes, \"\n                f\"got {encoded_signals.shape[-1]}.\"\n            )\n            raise ValueError(err)\n\n        if n_modes &gt; self.n_modes:\n            err = (\n                f\"Cannot decode with {n_modes} modes, \"\n                f\"only {self.n_modes} available in the basis.\"\n            )\n            raise ValueError(err)\n\n        return encoded_signals @ self.eigenvectors[:, :n_modes].T\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>String representation of the EigenmodeBasis.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation of the EigenmodeBasis.\n    \"\"\"\n    return f\"EigenmodeBasis(n_modes={self.n_modes}, n_features={self.n_features})\"\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.decode","title":"<code>decode(encoded_signals: npt.NDArray[np.floating[Any]], n_modes: int | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Decode encoded signals :math:<code>ilde{x}</code> using the eigenmode basis.</p> <p>Given an eigenmode set :math:<code>\\Psi</code> where :math:<code>L = \\Psi \\Lambda \\Psi^{-1}</code> and a list of encoded signals :math:<code>ilde{x}</code>, the decoded signals :math:<code>\\hat{x}</code> are given by:</p> <p>.. math::     \\hat{x} = \\Psi \\tilde{x}</p> <p>Parameters:</p> Name Type Description Default <code>encoded_signals</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded signals in the eigenmode basis (n_signals, n_modes).</p> required <code>n_modes</code> <code>int | None</code> <p>int | None Number of modes used for decoding. If None, use all available modes. This should not exceed the number of modes in the encoded_signals nor the number of modes in the eigenmode basis.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Decoded signal in the original feature space (n_signals, n_features).</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def decode(\n    self,\n    encoded_signals: npt.NDArray[np.floating[Any]],\n    n_modes: int | None = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Decode encoded signals :math:`\\tilde{x}` using the eigenmode basis.\n\n    Given an eigenmode set :math:`\\\\Psi` where :math:`L = \\\\Psi \\\\Lambda \\\\Psi^{-1}`\n    and a list of encoded signals :math:`\\tilde{x}`, the decoded signals\n    :math:`\\\\hat{x}` are given by:\n\n    .. math::\n        \\\\hat{x} = \\\\Psi \\\\tilde{x}\n\n    Args:\n        encoded_signals: np.ndarray\n            Encoded signals in the eigenmode basis (n_signals, n_modes).\n        n_modes: int | None\n            Number of modes used for decoding. If None, use all available modes.\n            This should not exceed the number of modes in the encoded_signals\n            nor the number of modes in the eigenmode basis.\n\n    Returns:\n        np.ndarray\n            Decoded signal in the original feature space (n_signals, n_features).\n    \"\"\"\n    if n_modes is None:\n        n_modes = self.n_modes\n\n    if encoded_signals.shape[-1] &lt; n_modes:\n        err = (\n            f\"Encoded signal must have at least {n_modes} modes, \"\n            f\"got {encoded_signals.shape[-1]}.\"\n        )\n        raise ValueError(err)\n\n    if n_modes &gt; self.n_modes:\n        err = (\n            f\"Cannot decode with {n_modes} modes, \"\n            f\"only {self.n_modes} available in the basis.\"\n        )\n        raise ValueError(err)\n\n    return encoded_signals @ self.eigenvectors[:, :n_modes].T\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.encode","title":"<code>encode(signals: npt.NDArray[np.floating[Any]], n_modes: int | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Encode a signal using the eigenmode basis.</p> <p>Given an eigenmode set :math:<code>\\Psi</code> where :math:<code>L = \\Psi \\Lambda \\Psi^{-1}</code> and a list of signals :math:<code>x</code>, the encoded signals :math:<code>ilde{x}</code> are given by:</p> <p>.. math::     \\tilde{x} = \\Psi^{-1} x = \\Psi^T M x</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Signals to encode (n_signals, n_features).</p> required <code>n_modes</code> <code>int | None</code> <p>int | None Number of modes to use for encoding. If None, use all available modes.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded signal in the eigenmode basis (n_signals, n_modes).</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def encode(\n    self,\n    signals: npt.NDArray[np.floating[Any]],\n    n_modes: int | None = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Encode a signal using the eigenmode basis.\n\n    Given an eigenmode set :math:`\\\\Psi` where :math:`L = \\\\Psi \\\\Lambda \\\\Psi^{-1}`\n    and a list of signals :math:`x`, the encoded signals :math:`\\tilde{x}` are given\n    by:\n\n    .. math::\n        \\\\tilde{x} = \\\\Psi^{-1} x = \\\\Psi^T M x\n\n    Args:\n        signals: np.ndarray\n            Signals to encode (n_signals, n_features).\n        n_modes: int | None\n            Number of modes to use for encoding. If None, use all available modes.\n\n    Returns:\n        np.ndarray\n            Encoded signal in the eigenmode basis (n_signals, n_modes).\n    \"\"\"\n    if signals.shape[-1] != self.n_features:\n        err = (\n            f\"Signal must have {self.n_features} features, got {signals.shape[-1]}.\"\n        )\n        raise ValueError(err)\n\n    if n_modes is None:\n        n_modes = self.n_modes\n\n    if self.mass_matrix is not None:\n        # Generalized eigenvalue problem\n        return signals @ (self.mass_matrix.T @ self.eigenvectors[:, :n_modes])\n    return signals @ self.eigenvectors[:, :n_modes]\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.inverted_eigenvectors","title":"<code>inverted_eigenvectors() -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the inverse of the eigenvector matrix.</p> <p>.. math::     \\Psi^{-1} = \\Psi^T M</p> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Inverse of the eigenvector matrix (n_modes, n_features).</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def inverted_eigenvectors(self) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the inverse of the eigenvector matrix.\n\n    .. math::\n        \\\\Psi^{-1} = \\\\Psi^T M\n\n    Returns:\n        np.ndarray\n            Inverse of the eigenvector matrix (n_modes, n_features).\n    \"\"\"\n    if self.mass_matrix is None:\n        # Standard eigenvalue problem\n        return self.eigenvectors.T\n    # Generalized eigenvalue problem (sparse @ dense is more efficient)\n    return (self.mass_matrix.T @ self.eigenvectors).T\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.load","title":"<code>load(filepath: str, mmap_mode: MmapMode | None = 'r') -&gt; EigenmodeBasis</code>  <code>classmethod</code>","text":"<p>Load an EigenmodeBasis instance from a joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>str Path to the joblib file.</p> required <code>mmap_mode</code> <code>MmapMode | None</code> <p>MmapMode | None Memory mapping mode for joblib (default: \"r\"). You can set this to None to disable memory-mapping.</p> <code>'r'</code> <p>Returns:</p> Type Description <code>EigenmodeBasis</code> <p>EigenmodeBasis instance</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str, mmap_mode: MmapMode | None = \"r\") -&gt; EigenmodeBasis:\n    \"\"\"\n    Load an EigenmodeBasis instance from a joblib file.\n\n    Args:\n        filepath: str\n            Path to the joblib file.\n        mmap_mode: MmapMode | None\n            Memory mapping mode for joblib (default: \"r\").\n            You can set this to None to disable memory-mapping.\n\n    Returns:\n        EigenmodeBasis instance\n    \"\"\"\n    data = joblib.load(filepath, mmap_mode=mmap_mode)\n    # Expecting the saved file to contain a dict with these keys:\n    # 'eigenvalues', 'eigenvectors'\n    return cls(\n        eigenvalues=data[\"eigenvalues\"],\n        eigenvectors=data[\"eigenvectors\"],\n        mass_matrix=data.get(\"mass_matrix\"),\n    )\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.reduce","title":"<code>reduce(n_modes: int, *, inplace: bool = True) -&gt; EigenmodeBasis</code>","text":"<p>Reduce the EigenmodeBasis to only contain the first n_modes.</p> <p>This method is useful to reduce the size of the basis for efficiency (e.g., to remove less important modes/degenerate modes before further processing).</p> <p>Parameters:</p> Name Type Description Default <code>n_modes</code> <code>int</code> <p>int Number of modes to retain. This must be less than or equal to the current number of modes.</p> required <code>inplace</code> <code>bool</code> <p>bool Whether to modify the current instance or return a new one. By default, this is True (modify in place, optimizing memory usage).</p> <code>True</code> <p>Returns:</p> Type Description <code>EigenmodeBasis</code> <p>EigenmodeBasis A new EigenmodeBasis instance with reduced modes.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def reduce(self, n_modes: int, *, inplace: bool = True) -&gt; EigenmodeBasis:\n    \"\"\"\n    Reduce the EigenmodeBasis to only contain the first n_modes.\n\n    This method is useful to reduce the size of the basis for efficiency (e.g.,\n    to remove less important modes/degenerate modes before further processing).\n\n    Args:\n        n_modes: int\n            Number of modes to retain. This must be less than or equal to the\n            current number of modes.\n        inplace: bool\n            Whether to modify the current instance or return a new one.\n            By default, this is True (modify in place, optimizing memory usage).\n\n    Returns:\n        EigenmodeBasis\n            A new EigenmodeBasis instance with reduced modes.\n    \"\"\"\n    if n_modes &gt; self.n_modes:\n        err = f\"Cannot reduce to {n_modes} modes, only {self.n_modes} available.\"\n        raise ValueError(err)\n    if inplace:\n        self.eigenvalues = self.eigenvalues[:n_modes]\n        self.eigenvectors = self.eigenvectors[:, :n_modes]\n        self.n_modes = n_modes\n        return self\n    # else return a new instance\n    return EigenmodeBasis(\n        eigenvalues=self.eigenvalues[:n_modes],\n        eigenvectors=self.eigenvectors[:, :n_modes],\n        mass_matrix=self.mass_matrix,\n    )\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.EigenmodeBasis.save","title":"<code>save(filepath: str, compress: int = 0, *, overwrite: bool = False) -&gt; None</code>","text":"<p>Save the EigenmodeBasis instance to a joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>str Path to save the joblib file.</p> required <code>compress</code> <code>int</code> <p>int Compression level for joblib (default: 0). Note: Compression is disabled by default to enable support for memory-mapped arrays. However, if you do not need memory-mapping, and want to reduce file size, you can enable compression by setting the compress parameter to a positive integer.</p> <code>0</code> <code>overwrite</code> <code>bool</code> <p>bool Whether to overwrite an existing file (default: False).</p> <code>False</code> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def save(\n    self,\n    filepath: str,\n    compress: int = 0,\n    *,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"\n    Save the EigenmodeBasis instance to a joblib file.\n\n    Args:\n        filepath: str\n            Path to save the joblib file.\n        compress: int\n            Compression level for joblib (default: 0).\n            Note: Compression is disabled by default to enable support for\n            memory-mapped arrays. However, if you do not need memory-mapping,\n            and want to reduce file size, you can enable compression by setting\n            the compress parameter to a positive integer.\n        overwrite: bool\n            Whether to overwrite an existing file (default: False).\n    \"\"\"\n    # Handle file overwrite\n    if Path(filepath).exists():\n        if not overwrite:\n            err = f\"File '{filepath}' already exists and overwrite is disabled.\"\n            raise FileExistsError(err)\n        Path(filepath).unlink(missing_ok=True)\n    data = {\n        \"eigenvalues\": self.eigenvalues,\n        \"eigenvectors\": self.eigenvectors,\n        \"mass_matrix\": self.mass_matrix,\n    }\n    joblib.dump(data, filepath, compress=compress)\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.compute_random_walk_laplacian_eigenmodes","title":"<code>compute_random_walk_laplacian_eigenmodes(adjacency_matrix: sparse.csr_matrix | npt.NDArray[np.floating[Any]], num_eigenvalues: int = 100) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]</code>","text":"<p>Compute the eigenvalues of the random walk Laplacian.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>csr_matrix | NDArray[floating[Any]]</code> <p>sparse.csr_matrix The adjacency matrix of the graph.</p> required <code>num_eigenvalues</code> <code>int</code> <p>int Number of eigenvalues to compute.</p> <code>100</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>eigenvalues, eigenvectors: (np.ndarray, np.ndarray) Eigenvalues and eigenvectors of the random walk Laplacian.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def compute_random_walk_laplacian_eigenmodes(\n    adjacency_matrix: sparse.csr_matrix | npt.NDArray[np.floating[Any]],\n    num_eigenvalues: int = 100,\n) -&gt; tuple[\n    npt.NDArray[np.floating[Any]],\n    npt.NDArray[np.floating[Any]],\n    npt.NDArray[np.floating[Any]],\n]:\n    \"\"\"\n    Compute the eigenvalues of the random walk Laplacian.\n\n    Args:\n        adjacency_matrix: sparse.csr_matrix\n            The adjacency matrix of the graph.\n        num_eigenvalues: int\n            Number of eigenvalues to compute.\n\n    Returns:\n        eigenvalues, eigenvectors: (np.ndarray, np.ndarray)\n            Eigenvalues and eigenvectors of the random walk Laplacian.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Check if num_eigenvalues is less than number of nodes\n    if num_eigenvalues &gt;= adjacency_matrix.shape[0]:\n        err = (\n            f\"num_eigenvalues ({num_eigenvalues}) must be less than \"\n            f\"the number of nodes ({adjacency_matrix.shape[0]}).\"\n        )\n        raise ValueError(err)\n\n    # Ensure the adjacency matrix is symmetric\n    adjacency_matrix = sparse.csr_matrix((adjacency_matrix + adjacency_matrix.T) * 0.5)\n\n    # Compute the degree matrix\n    degrees = np.array(adjacency_matrix.sum(axis=1)).flatten().astype(np.float64)\n\n    # Ensure no zero degrees to avoid division by zero\n    if np.any(degrees == 0):\n        err = \"The adjacency matrix contains isolated nodes with zero degree.\"\n        raise ValueError(err)\n\n    # No need to compute the transition matrix\n    # We can instead solve the generalized eigenvalue problem directly\n    # Use shift invert mode to compute the eigenvalues for the transition matrix,\n    # starting with the trivial eigenvalue 1\n    initial_vector = np.ones(adjacency_matrix.shape[0]) / adjacency_matrix.shape[0]\n    lambdas, vectors = sparse.linalg.eigsh(\n        adjacency_matrix,\n        M=sparse.diags(degrees),  # The Mass matrix in A v = lambda M v\n        k=num_eigenvalues + 1,\n        which=\"LA\",  # type: ignore[call-overload]\n        v0=initial_vector,\n    )\n\n    # Convert to Laplacian eigenvalues\n    lambdas = 1 - lambdas\n    lambda_idx = np.argsort(lambdas)\n    lambdas = lambdas[lambda_idx]\n    vectors = vectors[:, lambda_idx]\n\n    return (lambdas, np.asarray(vectors).T, degrees)\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.compute_symmetric_normalized_laplacian_eigenmodes","title":"<code>compute_symmetric_normalized_laplacian_eigenmodes(adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]], num_eigenvalues: int = 100) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]</code>","text":"<p>Compute the eigenvalues of the symmetric normalized Laplacian.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix | NDArray[floating[Any]]</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <code>num_eigenvalues</code> <code>int</code> <p>int Number of eigenvalues to compute.</p> <code>100</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>eigenvalues, eigenvectors: (np.ndarray, np.ndarray) Eigenvalues and eigenvectors of the symmetric normalized Laplacian.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def compute_symmetric_normalized_laplacian_eigenmodes(\n    adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]],\n    num_eigenvalues: int = 100,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]:\n    \"\"\"\n    Compute the eigenvalues of the symmetric normalized Laplacian.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n        num_eigenvalues: int\n            Number of eigenvalues to compute.\n\n    Returns:\n        eigenvalues, eigenvectors: (np.ndarray, np.ndarray)\n            Eigenvalues and eigenvectors of the symmetric normalized Laplacian.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Ensure the adjacency matrix is symmetric\n    adjacency_matrix = sparse.csr_matrix((adjacency_matrix + adjacency_matrix.T) * 0.5)\n\n    # Perform symmetric normalization\n    normalized_matrix = perform_symmetric_normalization(adjacency_matrix)\n\n    # Use shift invert mode to compute the eigenvalues for the\n    # normalized adjacency matrix\n    lambdas, vectors = sparse.linalg.eigsh(\n        normalized_matrix,\n        k=num_eigenvalues + 1,\n        which=\"LA\",  # type: ignore[call-overload]\n    )\n    # Note the largest eigenvalues of the adjacency matrix correspond to\n    # the smallest eigenvalues of the Laplacian\n\n    # Convert to Laplacian eigenvalues\n    lambdas = 1 - lambdas\n    lambda_idx = np.argsort(lambdas)\n    lambdas = lambdas[lambda_idx]\n    vectors = vectors[:, lambda_idx]\n\n    return (lambdas, vectors.T)\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.compute_symmetric_normalized_laplacian_eigenmodes_amg_lobpcg","title":"<code>compute_symmetric_normalized_laplacian_eigenmodes_amg_lobpcg(adjacency_matrix: sparse.csr_matrix | npt.NDArray[np.floating[Any]], num_eigenvalues: int = 100, amg_cycles: int = 1) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]</code>","text":"<p>Compute the eigenvalues and eigenvectors of the Symmetric Normalized Laplacian (:math:<code>L_{       ext{sym}} = I - D^{-1/2} A D^{-1/2}</code>) using LOBPCG with Algebraic Multigrid (AMG) preconditioning.</p> <p>Note: this is a more advanced and efficient method for large graphs compared to using <code>scipy.sparse.linalg.eigsh</code>.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>csr_matrix | NDArray[floating[Any]]</code> <p>The adjacency matrix of the graph.</p> required <code>num_eigenvalues</code> <code>int</code> <p>Number of eigenvalues (modes) to compute.</p> <code>100</code> <code>amg_cycles</code> <code>int</code> <p>Number of V-cycles to use in the AMG preconditioner solve.</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>eigenvalues, eigenvectors, degrees: (np.ndarray, np.ndarray, np.ndarray) Eigenvalues and eigenvectors of L_sym, and the node degrees.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def compute_symmetric_normalized_laplacian_eigenmodes_amg_lobpcg(\n    adjacency_matrix: sparse.csr_matrix | npt.NDArray[np.floating[Any]],\n    num_eigenvalues: int = 100,\n    amg_cycles: int = 1,\n) -&gt; tuple[\n    npt.NDArray[np.floating[Any]],\n    npt.NDArray[np.floating[Any]],\n    npt.NDArray[np.floating[Any]],\n]:\n    \"\"\"\n    Compute the eigenvalues and eigenvectors of the Symmetric Normalized\n    Laplacian (:math:`L_{\\text{sym}} = I - D^{-1/2} A D^{-1/2}`) using LOBPCG\n    with Algebraic Multigrid (AMG) preconditioning.\n\n    Note: this is a more advanced and efficient method for large graphs compared\n    to using `scipy.sparse.linalg.eigsh`.\n\n    Args:\n        adjacency_matrix: The adjacency matrix of the graph.\n        num_eigenvalues: Number of eigenvalues (modes) to compute.\n        amg_cycles: Number of V-cycles to use in the AMG preconditioner solve.\n\n    Returns:\n        eigenvalues, eigenvectors, degrees: (np.ndarray, np.ndarray, np.ndarray)\n            Eigenvalues and eigenvectors of L_sym, and the node degrees.\n    \"\"\"\n    # Initialize the random number generator\n    rng = np.random.default_rng(42)\n\n    # 1. Preprocessing and Input Validation\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n    n = adjacency_matrix.shape[0]\n\n    if num_eigenvalues &gt;= n:\n        err = (\n            f\"num_eigenvalues ({num_eigenvalues}) must be less than \"\n            f\"the number of nodes ({n}).\"\n        )\n        raise ValueError(err)\n\n    # Ensure the adjacency matrix is symmetric\n    adjacency_matrix = sparse.csr_matrix((adjacency_matrix + adjacency_matrix.T) * 0.5)\n\n    # Compute degrees and handle isolated nodes\n    degrees = np.asarray(adjacency_matrix.sum(axis=1)).flatten().astype(np.float64)\n    if np.any(degrees == 0):\n        err = \"The adjacency matrix contains isolated nodes with zero degree.\"\n        raise ValueError(err)\n\n    # 2. Construct the Symmetric Normalized Laplacian (L_sym)\n    # L_sym = I - D^(-1/2) A D^(-1/2)\n    # The term D^(-1/2) A D^(-1/2) is the normalized adjacency matrix\n    symmetric_normalized_laplacian = sparse.identity(\n        adjacency_matrix.shape[0],\n        format=\"csr\",\n    ) - perform_symmetric_normalization(adjacency_matrix)\n\n    # 3. Construct the AMG Preconditioner (M_amg)\n    # Build the Smoothed Aggregation Multigrid hierarchy on L_sym\n    ml = pyamg.smoothed_aggregation_solver(symmetric_normalized_laplacian)\n\n    # Create a LinearOperator that applies the V-cycle (the preconditioning step)\n    def preconditioner_matvec(\n        x: npt.NDArray[np.floating[Any]],\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        # Solves L_sym * z = x approximately\n        z = ml.solve(x, cycle=\"V\", maxiter=amg_cycles, tol=1e-8)\n        return np.asarray(z, dtype=x.dtype)\n\n    # M_amg is the preconditioner operator\n    m_amg = sparse.linalg.LinearOperator(\n        shape=tuple(symmetric_normalized_laplacian.shape),\n        matvec=preconditioner_matvec,\n        dtype=symmetric_normalized_laplacian.dtype,\n    )  # type: ignore[call-overload]\n\n    # 4. Solve the Eigenvalue Problem L_sym u = mu u using LOBPCG\n    block_size = num_eigenvalues + 1\n    # Random initial block of vectors\n    initial_block = rng.random(\n        (n, block_size),\n        dtype=symmetric_normalized_laplacian.dtype,\n    )\n\n    # We seek the smallest eigenvalues\n    # (\"SA\" - Smallest Algebraic), as L_sym eigenvalues are in [0, 2]\n    # Finding the smallest eigenvalues is crucial for spectral clustering/analysis.\n    lambdas, vectors = sparse.linalg.lobpcg(\n        A=symmetric_normalized_laplacian,\n        X=initial_block,\n        M=m_amg,  # The AMG Preconditioner\n        tol=1e-8,\n        maxiter=1000,\n        largest=False,  # Find the eigenvalues closest to 0\n    )  # type: ignore[operator]\n\n    # 5. Sorting and Output\n    # Sort from smallest to largest L_sym eigenvalues\n    lambda_idx = np.argsort(lambdas)\n    lambdas = lambdas[lambda_idx]\n    vectors = vectors[:, lambda_idx]\n\n    # Return L_sym eigenvalues, L_sym eigenvectors, and degrees\n    return (lambdas, vectors.T, degrees)\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.convert_adjacency_to_transition_matrix","title":"<code>convert_adjacency_to_transition_matrix(adjacency_matrix: sparse.spmatrix) -&gt; sparse.csr_matrix</code>","text":"<p>Convert an adjacency matrix to a transition matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.spmatrix Transition matrix.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def convert_adjacency_to_transition_matrix(\n    adjacency_matrix: sparse.spmatrix,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Convert an adjacency matrix to a transition matrix.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n\n    Returns:\n        sparse.spmatrix\n            Transition matrix.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Compute the degree matrix\n    degrees = np.array(adjacency_matrix.sum(axis=1)).flatten()\n    degree_matrix_inv = sparse.diags(1.0 / degrees)\n\n    # Create the transition matrix\n    return sparse.csr_matrix(degree_matrix_inv @ adjacency_matrix)\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.make_csr_matrix","title":"<code>make_csr_matrix(matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]]) -&gt; sparse.csr_matrix</code>","text":"<p>Ensure the input matrix is in CSR format.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def make_csr_matrix(\n    matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]],\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Ensure the input matrix is in CSR format.\n    \"\"\"\n    if sparse.issparse(matrix):\n        # We cast to Any briefly to tell Mypy:\n        # I've already verified this is sparse, let me call .tocsr()\n        return cast(\"sparse.csr_matrix\", cast(\"Any\", matrix).tocsr())\n\n    # If it's not sparse, it's a dense array; convert it directly.\n    return sparse.csr_matrix(np.asarray(matrix))\n</code></pre>"},{"location":"api/utils/gsp/#spectranorm.utils.gsp.perform_symmetric_normalization","title":"<code>perform_symmetric_normalization(adjacency_matrix: sparse.spmatrix) -&gt; sparse.csr_matrix</code>","text":"<p>Perform symmetric normalization on the adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.spmatrix Symmetrically normalized adjacency matrix.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def perform_symmetric_normalization(\n    adjacency_matrix: sparse.spmatrix,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Perform symmetric normalization on the adjacency matrix.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n\n    Returns:\n        sparse.spmatrix\n            Symmetrically normalized adjacency matrix.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n    # Compute the degree matrix\n    degrees = np.array(adjacency_matrix.sum(axis=1)).flatten()\n    degree_matrix_inv_sqrt = sparse.diags(1.0 / np.sqrt(degrees))\n\n    # Perform symmetric normalization\n    return sparse.csr_matrix(\n        degree_matrix_inv_sqrt @ adjacency_matrix @ degree_matrix_inv_sqrt,\n    )\n</code></pre>"},{"location":"api/utils/metrics/","title":"Metrics Utilities","text":"<p>The metrics utilities module provides functions for evaluating the performance of normative models. This includes functions for computing various error metrics, such as mean squared error (MSE), mean absolute error (MAE), and R-squared (R\u00b2) scores, among others. These metrics are essential for assessing the fit of the model to the data and for comparing different models.</p>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics","title":"<code>spectranorm.utils.metrics</code>","text":"<p>utils/metrics.py</p> <p>Utility functions for computing various model metrics in the Spectranorm package.</p>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_bic","title":"<code>compute_bic(model_log_likelihoods: npt.NDArray[np.floating[Any]], n_params: int, n_samples: int) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Bayesian Information Criterion (BIC) for model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>model_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the model. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>n_params</code> <code>int</code> <p>int Number of parameters in the model.</p> required <code>n_samples</code> <code>int</code> <p>int Number of samples used in the model.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Bayesian Information Criterion. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_bic(\n    model_log_likelihoods: npt.NDArray[np.floating[Any]],\n    n_params: int,\n    n_samples: int,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Bayesian Information Criterion (BIC) for model evaluation.\n\n    Args:\n        model_log_likelihoods: np.ndarray\n            Log likelihoods from the model. (n_samples) or (n_samples, n_outputs)\n            if multiple outputs are being assessed.\n        n_params: int\n            Number of parameters in the model.\n        n_samples: int\n            Number of samples used in the model.\n\n    Returns:\n        np.ndarray\n            Bayesian Information Criterion. (n_outputs,) if multiple outputs are\n            assessed, otherwise a scalar.\n    \"\"\"\n    return np.asarray(\n        -2 * np.mean(model_log_likelihoods, axis=0) + n_params * np.log(n_samples),\n    )\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_expv","title":"<code>compute_expv(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Explained Variance (EXPV) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Explained Variance. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_expv(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Explained Variance (EXPV) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Explained Variance. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(1 - np.var(y - y_pred, axis=0) / np.var(y, axis=0))\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_mae","title":"<code>compute_mae(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Absolute Error (MAE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Absolute Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mae(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Absolute Error (MAE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Absolute Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean(np.abs(y - y_pred), axis=0))\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_mape","title":"<code>compute_mape(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Absolute Percentage Error (MAPE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Absolute Percentage Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mape(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Absolute Percentage Error (MAPE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Absolute Percentage Error. (n_outputs,) if multiple outputs are\n            assessed, otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean(np.abs((y - y_pred) / y), axis=0) * 100)\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_mse","title":"<code>compute_mse(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Squared Error (MSE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Squared Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mse(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Squared Error (MSE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Squared Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean((y - y_pred) ** 2, axis=0))\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_msll","title":"<code>compute_msll(model_log_likelihoods: npt.NDArray[np.floating[Any]], baseline_log_likelihoods: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Standardized Log Loss (MSLL) based on model and baseline log likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>model_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the model. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>baseline_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the baseline model (a trivial model). Same shape as model_log_likelihoods.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Standardized Log Loss. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_msll(\n    model_log_likelihoods: npt.NDArray[np.floating[Any]],\n    baseline_log_likelihoods: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Standardized Log Loss (MSLL) based on model and baseline log\n    likelihoods.\n\n    Args:\n        model_log_likelihoods: np.ndarray\n            Log likelihoods from the model. (n_samples) or (n_samples, n_outputs)\n            if multiple outputs are being assessed.\n        baseline_log_likelihoods: np.ndarray\n            Log likelihoods from the baseline model (a trivial model). Same shape as\n            model_log_likelihoods.\n\n    Returns:\n        np.ndarray\n            Mean Standardized Log Loss. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(\n        -(\n            np.mean(model_log_likelihoods, axis=0)\n            - np.mean(baseline_log_likelihoods, axis=0)\n        ),\n    )\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_r2","title":"<code>compute_r2(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute R-squared (coefficient of determination) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray R-squared value. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_r2(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute R-squared (coefficient of determination) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            R-squared value. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    ss_res = np.sum((y - y_pred) ** 2, axis=0)\n    ss_tot = np.sum((y - np.mean(y, axis=0)) ** 2, axis=0)\n    return np.asarray(1 - (ss_res / ss_tot))\n</code></pre>"},{"location":"api/utils/metrics/#spectranorm.utils.metrics.compute_rmse","title":"<code>compute_rmse(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Root Mean Squared Error (RMSE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Root Mean Squared Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_rmse(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Root Mean Squared Error (RMSE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Root Mean Squared Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.sqrt(compute_mse(y, y_pred)))\n</code></pre>"},{"location":"api/utils/nitools/","title":"Neuroimaging Tools","text":"<p>The neuroimaging tools module provides functions for working with neuroimaging data, including handling cortical surface data (e.g., from FreeSurfer), performing spatial transformations, and computing graph-based representations of brain data. These tools enable seamless integration of spectral normative modeling with neuroimaging datasets, i.e., users can apply spectral normative models to data extracted from common neuroimaging preprocessing pipelines.</p>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools","title":"<code>spectranorm.utils.nitools</code>","text":"<p>utils/nitools.py</p> <p>Neuroimaging functions to deal with imaging files for spectranorm.</p>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_adaptive_area_barycentric_transformation","title":"<code>compute_adaptive_area_barycentric_transformation(source_vertices: npt.NDArray[np.floating[Any]], source_triangles: npt.NDArray[np.integer[Any]], source_vertex_areas: npt.NDArray[np.floating[Any]], target_vertices: npt.NDArray[np.floating[Any]], target_triangles: npt.NDArray[np.integer[Any]], target_vertex_areas: npt.NDArray[np.floating[Any]], source_vertex_mask: npt.NDArray[np.bool_] | None = None, k_candidates: int = 10, tol: float = 1e-10) -&gt; sparse.csr_matrix</code>","text":"<p>Computes an adaptive area barycentric transformation matrix from the source mesh to the target mesh (similar to workbench command's ADAP_BARY_AREA option).</p> <p>Parameters:</p> Name Type Description Default <code>source_vertices</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of shape (N1, 3) containing the coordinates of the source vertices.</p> required <code>source_triangles</code> <code>NDArray[integer[Any]]</code> <p>np.ndarray Array of shape (M1, 3) containing the indices of the source triangles.</p> required <code>source_vertex_areas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of shape (N1,) containing the areas of the source vertices.</p> required <code>target_vertices</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of shape (N2, 3) containing the coordinates of the target vertices.</p> required <code>target_triangles</code> <code>NDArray[integer[Any]]</code> <p>np.ndarray Array of shape (M2, 3) containing the indices of the target triangles.</p> required <code>target_vertex_areas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of shape (N2,) containing the areas of the target vertices.</p> required <code>source_vertex_mask</code> <code>NDArray[bool_] | None</code> <p>np.ndarray | None (N1,) Optional mask for source vertices.</p> <code>None</code> <code>k_candidates</code> <code>int</code> <p>int Number of candidate triangles to consider for each target vertex.</p> <code>10</code> <code>tol</code> <code>float</code> <p>float Tolerance for barycentric coordinate computation.</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.csr_matrix Sparse matrix of shape (N2, N1) representing the transformation.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_adaptive_area_barycentric_transformation(\n    source_vertices: npt.NDArray[np.floating[Any]],  # (n_source, 3)\n    source_triangles: npt.NDArray[np.integer[Any]],  # (n_tri_source, 3)\n    source_vertex_areas: npt.NDArray[np.floating[Any]],  # (n_source,)\n    target_vertices: npt.NDArray[np.floating[Any]],  # (n_target, 3)\n    target_triangles: npt.NDArray[np.integer[Any]],  # (n_tri_target, 3)\n    target_vertex_areas: npt.NDArray[np.floating[Any]],  # (n_target,)\n    source_vertex_mask: npt.NDArray[np.bool_] | None = None,  # (n_source,)\n    k_candidates: int = 10,  # number of candidate triangles to check\n    tol: float = 1e-10,  # tolerance for barycentric coords\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Computes an adaptive area barycentric transformation matrix from the source mesh to\n    the target mesh (similar to workbench command's ADAP_BARY_AREA option).\n\n    Args:\n        source_vertices: np.ndarray\n            Array of shape (N1, 3) containing the coordinates of the source vertices.\n        source_triangles: np.ndarray\n            Array of shape (M1, 3) containing the indices of the source triangles.\n        source_vertex_areas: np.ndarray\n            Array of shape (N1,) containing the areas of the source vertices.\n        target_vertices: np.ndarray\n            Array of shape (N2, 3) containing the coordinates of the target vertices.\n        target_triangles: np.ndarray\n            Array of shape (M2, 3) containing the indices of the target triangles.\n        target_vertex_areas: np.ndarray\n            Array of shape (N2,) containing the areas of the target vertices.\n        source_vertex_mask: np.ndarray | None\n            (N1,) Optional mask for source vertices.\n        k_candidates: int\n            Number of candidate triangles to consider for each target vertex.\n        tol: float\n            Tolerance for barycentric coordinate computation.\n\n    Returns:\n        sparse.csr_matrix\n            Sparse matrix of shape (N2, N1) representing the transformation.\n    \"\"\"\n    # --- Step 1: Compute forward and reverse barycentric transformations ---\n    forward_transform = compute_barycentric_transformation(\n        source_vertices=source_vertices,\n        source_triangles=source_triangles,\n        target_vertices=target_vertices,\n        k_candidates=k_candidates,\n        tol=tol,\n    ).tocsr()\n\n    reverse_transform = compute_barycentric_transformation(\n        source_vertices=target_vertices,\n        source_triangles=target_triangles,\n        target_vertices=source_vertices,\n        k_candidates=k_candidates,\n        tol=tol,\n    ).tocsr()\n\n    # Convert reverse scatter to gather (transpose)\n    reverse_gather_transform = reverse_transform.T.tocsr()\n\n    # --- Step 2: Decide forward vs reverse for each target vertex\n    # Choose the mapping with higher number of sources involved\n    # Count non-zero entries in each row of the forward and reverse matrices\n    # Note: Since we have CSR format, we can use indptr for efficiency\n    forward_sources = np.diff(forward_transform.indptr)\n    reverse_sources = np.diff(reverse_gather_transform.indptr)\n\n    # Pick forward if it has &gt;= as many sources as reverse\n    use_forward = forward_sources &gt;= reverse_sources\n\n    # --- Step 3: Build the adaptive gather matrix\n    adap_gather = (\n        sparse.diags((use_forward).astype(float)) @ forward_transform\n        + sparse.diags((~use_forward).astype(float)) @ reverse_gather_transform\n    )\n\n    # --- Step 4: Multiply each row by target vertex area (area contributions)\n    adap_gather = adap_gather.tocsr()\n    adap_gather = sparse.csr_matrix(sparse.diags(target_vertex_areas) @ adap_gather)\n\n    # --- Step 5: Correction sum (sum over target vertices for each source vertex)\n    correction_sum = np.asarray(adap_gather.sum(axis=0)).ravel()\n\n    # --- Step 6: Scale by source_areas / correction_sum (scatter normalization)\n    scale = np.zeros_like(correction_sum)\n    valid = correction_sum &gt; 0\n    scale[valid] = source_vertex_areas[valid] / correction_sum[valid]\n    adap_gather = sparse.csr_matrix(adap_gather @ sparse.diags(scale))\n\n    # --- Step 7: ROI masking\n    # (remove columns corresponding to masked-out source vertices)\n    if source_vertex_mask is not None:\n        adap_gather = sparse.csr_matrix(\n            adap_gather.multiply(\n                source_vertex_mask[None, :],\n            ),\n        )\n\n    # --- Step 8: Normalize each row to sum to 1\n    row_sums = np.asarray(adap_gather.sum(axis=1)).ravel()\n    row_scale = np.ones_like(row_sums)\n    valid_rows = row_sums &gt; 0\n    row_scale[valid_rows] = 1.0 / row_sums[valid_rows]\n    adap_gather = sparse.diags(row_scale) @ adap_gather\n\n    return sparse.csr_matrix(adap_gather)\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_barycentric_transformation","title":"<code>compute_barycentric_transformation(source_vertices: npt.NDArray[np.floating[Any]], source_triangles: npt.NDArray[np.integer[Any]], target_vertices: npt.NDArray[np.floating[Any]], k_candidates: int = 10, tol: float = 1e-10) -&gt; sparse.coo_matrix</code>","text":"<p>Compute sparse barycentric transformation matrix from target_vertices to source_vertices.</p> <p>Parameters:</p> Name Type Description Default <code>source_vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (N1, 3) containing the coordinates of the source vertices.</p> required <code>source_triangles</code> <code>NDArray[integer[Any]]</code> <p>numpy.ndarray Array of shape (M1, 3) containing the indices of the source triangles.</p> required <code>target_vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (N2, 3) containing the coordinates of the target vertices.</p> required <code>k_candidates</code> <code>int</code> <p>int Number of candidate triangles to consider for each target vertex. (default: 10)</p> <code>10</code> <code>tol</code> <code>float</code> <p>float Tolerance for inside-triangle check. (default: 1e-10)</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>barycentric_transformation</code> <code>coo_matrix</code> <p>scipy.sparse.csr_matrix Sparse matrix of shape (N2, N1) representing the barycentric transformation.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_barycentric_transformation(\n    source_vertices: npt.NDArray[np.floating[Any]],\n    source_triangles: npt.NDArray[np.integer[Any]],\n    target_vertices: npt.NDArray[np.floating[Any]],\n    k_candidates: int = 10,\n    tol: float = 1e-10,\n) -&gt; sparse.coo_matrix:\n    \"\"\"\n    Compute sparse barycentric transformation matrix from target_vertices to\n    source_vertices.\n\n    Args:\n        source_vertices: numpy.ndarray\n            Array of shape (N1, 3) containing the coordinates of the source vertices.\n        source_triangles: numpy.ndarray\n            Array of shape (M1, 3) containing the indices of the source triangles.\n        target_vertices: numpy.ndarray\n            Array of shape (N2, 3) containing the coordinates of the target vertices.\n        k_candidates: int\n            Number of candidate triangles to consider for each target vertex.\n            (default: 10)\n        tol: float\n            Tolerance for inside-triangle check. (default: 1e-10)\n\n    Returns:\n        barycentric_transformation: scipy.sparse.csr_matrix\n            Sparse matrix of shape (N2, N1) representing the barycentric transformation.\n    \"\"\"\n    # Compute source triangle centroids\n    source_triangle_centroids = np.mean(source_vertices[source_triangles], axis=1)\n\n    # Build KD-tree on triangle centroids to get a quick nearest triangle search\n    triangle_tree = spatial.cKDTree(source_triangle_centroids)\n\n    # Find the K nearest source triangles for each target vertex\n    _, candidate_triangles = triangle_tree.query(target_vertices, k=k_candidates)\n\n    # Reshape candidate_triangles to ensure it's 2D\n    candidate_triangles = candidate_triangles.reshape(-1, k_candidates)\n\n    # Compute coordinates of all vertices in the candidate triangles\n    a = source_vertices[source_triangles][candidate_triangles, 0, :]\n    b = source_vertices[source_triangles][candidate_triangles, 1, :]\n    c = source_vertices[source_triangles][candidate_triangles, 2, :]\n\n    # Compute vectors for barycentric coordinates\n    # v0 = b - a, v1 = c - a, v2 = target - a\n    v0 = b - a\n    v1 = c - a\n    v2 = target_vertices[:, None, :] - a\n\n    # Compute vector dot products\n    d00 = np.einsum(\"ijk,ijk-&gt;ij\", v0, v0)\n    d01 = np.einsum(\"ijk,ijk-&gt;ij\", v0, v1)\n    d11 = np.einsum(\"ijk,ijk-&gt;ij\", v1, v1)\n    d20 = np.einsum(\"ijk,ijk-&gt;ij\", v2, v0)\n    d21 = np.einsum(\"ijk,ijk-&gt;ij\", v2, v1)\n\n    # Compute barycentric coordinates\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        den = d00 * d11 - d01 * d01\n        beta = (d11 * d20 - d01 * d21) / den\n        gamma = (d00 * d21 - d01 * d20) / den\n        alpha = 1 - beta - gamma\n\n    # Check if target is inside any of the candidate triangles\n    is_inside = (alpha &gt;= -tol) &amp; (beta &gt;= -tol) &amp; (gamma &gt;= -tol) &amp; (np.abs(den) &gt; tol)\n    found_valid = is_inside.any(axis=1)\n\n    # Choose the optimal candidate\n    selected_candidate = np.argmax(is_inside, axis=1)\n    selected_candidate[~found_valid] = -1  # Mark cases with no valid candidates\n\n    # Start preparing the sparse transformation matrix\n    rows: list[int] = []\n    cols: list[int] = []\n    data: list[float] = []\n\n    valid_idx = np.where(selected_candidate &gt;= 0)[0]\n    if valid_idx.size &gt; 0:\n        selected_triangles = candidate_triangles[\n            valid_idx,\n            selected_candidate[valid_idx],\n        ]\n        selected_vertex_indices = source_triangles[selected_triangles]\n        alpha_values = alpha[valid_idx, selected_candidate[valid_idx]]\n        beta_values = beta[valid_idx, selected_candidate[valid_idx]]\n        gamma_values = gamma[valid_idx, selected_candidate[valid_idx]]\n\n        rows.extend(np.repeat(valid_idx, 3))\n        cols.extend(selected_vertex_indices.flatten())\n        bary_coords = np.stack([alpha_values, beta_values, gamma_values], axis=1)\n        data.extend(bary_coords.flatten())\n\n    # Fallback: nearest vertex for points without valid candidates\n    invalid_idx = np.where(selected_candidate &lt; 0)[0]\n    if invalid_idx.size &gt; 0:\n        vertex_tree = spatial.cKDTree(source_vertices)\n\n        _, nearest_vertices = vertex_tree.query(target_vertices[invalid_idx], k=1)\n        rows.extend(invalid_idx)\n        cols.extend(nearest_vertices.reshape(-1))\n        data.extend(np.ones(invalid_idx.size, dtype=float))\n\n    # Build and return the barycentric transformation matrix\n    return sparse.coo_matrix(\n        (np.asarray(data), (np.asarray(rows), np.asarray(cols))),\n        shape=(target_vertices.shape[0], source_vertices.shape[0]),\n    )\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_fslr_thickness","title":"<code>compute_fslr_thickness(subject_freesurfer_directory: str) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the fs_LR thickness from FreeSurfer output directory.</p> <p>This function can be passed to a <code>snm.utils.gsp.EigenmodeBasis</code> as a dataloader for the <code>load_and_encode_data_list</code> method.</p> <p>Note: Assuming FreeSurfer's recon-all is completed, this function automatically maps thickness estimates from subject's native cortical surface space onto the standard fs_LR space.</p> <p>Parameters:</p> Name Type Description Default <code>subject_freesurfer_directory</code> <code>str</code> <p>str Path to the FreeSurfer subject directory.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of shape (59412,) containing the thickness values for fs_LR vertices (excluding the medial wall).</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_fslr_thickness(\n    subject_freesurfer_directory: str,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the fs_LR thickness from FreeSurfer output directory.\n\n    This function can be passed to a `snm.utils.gsp.EigenmodeBasis` as a dataloader\n    for the `load_and_encode_data_list` method.\n\n    Note: Assuming FreeSurfer's recon-all is completed, this function automatically\n    maps thickness estimates from subject's native cortical surface space onto the\n    standard fs_LR space.\n\n    Args:\n        subject_freesurfer_directory: str\n            Path to the FreeSurfer subject directory.\n\n    Returns:\n        np.ndarray\n            Array of shape (59412,) containing the thickness values for fs_LR vertices\n            (excluding the medial wall).\n    \"\"\"\n    cifti_mask = get_fslr_surface_indices_from_cifti()\n    adap_area_barycentric_transformation = compute_fsnative_to_fslr32k_transformation(\n        subject_freesurfer_directory,\n    )[cifti_mask, :]\n    fsnative_thickness = np.concatenate(\n        [\n            nib.freesurfer.io.read_morph_data(  # type: ignore[no-untyped-call]\n                f\"{subject_freesurfer_directory}/surf/lh.thickness\",\n            ),\n            nib.freesurfer.io.read_morph_data(  # type: ignore[no-untyped-call]\n                f\"{subject_freesurfer_directory}/surf/rh.thickness\",\n            ),\n        ],\n    )\n    return np.asarray(adap_area_barycentric_transformation @ fsnative_thickness)\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_fsnative_to_fslr32k_transformation","title":"<code>compute_fsnative_to_fslr32k_transformation(subject_freesurfer_directory: str) -&gt; sparse.csr_matrix</code>","text":"<p>Compute the transformation matrix from FreeSurfer native space to fs_LR-32k space using an adaptive area barycentric transformation.</p> <p>Note: This function combines the vertices across left and right hemispheres to create a unified transformation matrix (left first, then right).</p> <p>Parameters:</p> Name Type Description Default <code>subject_freesurfer_directory</code> <code>str</code> <p>str Path to the FreeSurfer subject directory.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.csr_matrix Sparse matrix representing the transformation from fsnative space to fs_LR-32k.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_fsnative_to_fslr32k_transformation(\n    subject_freesurfer_directory: str,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Compute the transformation matrix from FreeSurfer native space to fs_LR-32k\n    space using an adaptive area barycentric transformation.\n\n    Note: This function combines the vertices across left and right hemispheres\n    to create a unified transformation matrix (left first, then right).\n\n    Args:\n        subject_freesurfer_directory: str\n            Path to the FreeSurfer subject directory.\n\n    Returns:\n        sparse.csr_matrix\n            Sparse matrix representing the transformation from fsnative space to\n            fs_LR-32k.\n    \"\"\"\n    # Load fs_LR 32k files\n    left_fslr_sphere_vertices, left_fslr_triangles = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.fs_LR_32k\")\n                / \"fs_LR-deformed_to-fsaverage.L.sphere.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    left_fslr_midthickness_vertices, _ = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.HCP\")\n                / \"S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    left_fslr_vertex_areas = compute_vertex_areas(\n        left_fslr_midthickness_vertices,\n        left_fslr_triangles,\n    )\n    right_fslr_sphere_vertices, right_fslr_triangles = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.fs_LR_32k\")\n                / \"fs_LR-deformed_to-fsaverage.R.sphere.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    right_fslr_midthickness_vertices, _ = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.HCP\")\n                / \"S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    right_fslr_vertex_areas = compute_vertex_areas(\n        right_fslr_midthickness_vertices,\n        right_fslr_triangles,\n    )\n\n    # Load subject native files\n    left_fsnative_sphere_vertices, left_fsnative_triangles = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.sphere.reg\",\n    )\n    left_fsnative_white_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.white\",\n    )\n    left_fsnative_pial_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.pial\",\n    )\n    left_fsnative_midthickness_vertices = np.asarray(\n        (left_fsnative_white_vertices + left_fsnative_pial_vertices) * 0.5,\n    )\n    left_fsnative_vertex_areas = compute_vertex_areas(\n        left_fsnative_midthickness_vertices,\n        left_fsnative_triangles,\n    )\n    right_fsnative_sphere_vertices, right_fsnative_triangles = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.sphere.reg\",\n    )\n    right_fsnative_white_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.white\",\n    )\n    right_fsnative_pial_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.pial\",\n    )\n    right_fsnative_midthickness_vertices = np.asarray(\n        (right_fsnative_white_vertices + right_fsnative_pial_vertices) * 0.5,\n    )\n    right_fsnative_vertex_areas = compute_vertex_areas(\n        right_fsnative_midthickness_vertices,\n        right_fsnative_triangles,\n    )\n\n    # Compute transformations\n    left_surface_transformation = compute_adaptive_area_barycentric_transformation(\n        source_vertices=left_fsnative_sphere_vertices,\n        source_triangles=left_fsnative_triangles,\n        source_vertex_areas=left_fsnative_vertex_areas,\n        target_vertices=left_fslr_sphere_vertices,\n        target_triangles=left_fslr_triangles,\n        target_vertex_areas=left_fslr_vertex_areas,\n    )\n    right_surface_transformation = compute_adaptive_area_barycentric_transformation(\n        source_vertices=right_fsnative_sphere_vertices,\n        source_triangles=right_fsnative_triangles,\n        source_vertex_areas=right_fsnative_vertex_areas,\n        target_vertices=right_fslr_sphere_vertices,\n        target_triangles=right_fslr_triangles,\n        target_vertex_areas=right_fslr_vertex_areas,\n    )\n\n    # Combine the transformations and return\n    return sparse.csr_matrix(\n        sparse.block_diag(\n            [\n                left_surface_transformation,\n                right_surface_transformation,\n            ],\n        ),\n    )\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_total_euler_number","title":"<code>compute_total_euler_number(subject_freesurfer_directory: str) -&gt; int</code>","text":"<p>Compute the total Euler number from FreeSurfer output directory. Note: This function assumes FreeSurfer's recon-all is completed.</p> <p>Parameters:</p> Name Type Description Default <code>subject_freesurfer_directory</code> <code>str</code> <p>str Path to the FreeSurfer subject directory.</p> required <p>Returns:</p> Type Description <code>int</code> <p>int The total Euler number (sum over left and right surfaces).</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_total_euler_number(subject_freesurfer_directory: str) -&gt; int:\n    \"\"\"\n    Compute the total Euler number from FreeSurfer output directory.\n    Note: This function assumes FreeSurfer's recon-all is completed.\n\n    Args:\n        subject_freesurfer_directory: str\n            Path to the FreeSurfer subject directory.\n\n    Returns:\n        int\n            The total Euler number (sum over left and right surfaces).\n    \"\"\"\n    left_surface = f\"{subject_freesurfer_directory}/surf/lh.orig.nofix\"\n    right_surface = f\"{subject_freesurfer_directory}/surf/rh.orig.nofix\"\n\n    # Compute the Euler characteristic for each surface\n    left_euler = get_euler_number(*load_freesurfer_surface(left_surface))\n    right_euler = get_euler_number(*load_freesurfer_surface(right_surface))\n\n    # Return the total Euler number\n    return left_euler + right_euler\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.compute_vertex_areas","title":"<code>compute_vertex_areas(vertices: npt.NDArray[np.floating[Any]], triangles: npt.NDArray[np.integer[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the surface area of each vertex in a triangular mesh.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (n_vertices, 3) containing vertex coordinates.</p> required <code>triangles</code> <code>NDArray[integer[Any]]</code> <p>numpy.ndarray Array of shape (n_triangles, 3) containing vertex indices for each triangular face.</p> required <p>Returns:</p> Name Type Description <code>vertex_areas</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (n_vertices,) containing the area associated with each vertex.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_vertex_areas(\n    vertices: npt.NDArray[np.floating[Any]],\n    triangles: npt.NDArray[np.integer[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the surface area of each vertex in a triangular mesh.\n\n    Args:\n        vertices: numpy.ndarray\n            Array of shape (n_vertices, 3) containing vertex coordinates.\n        triangles: numpy.ndarray\n            Array of shape (n_triangles, 3) containing vertex indices for each\n            triangular face.\n\n    Returns:\n        vertex_areas: numpy.ndarray\n            Array of shape (n_vertices,) containing the area associated with each\n            vertex.\n    \"\"\"\n    # Compute the area of each triangle\n    v0 = vertices[triangles[:, 0]]\n    v1 = vertices[triangles[:, 1]]\n    v2 = vertices[triangles[:, 2]]\n    triangle_areas = 0.5 * np.linalg.norm(np.cross(v1 - v0, v2 - v0), axis=1)\n\n    # Accumulate areas for each vertex\n    # One third of each triangle's area is attributed to each of its vertices\n    vertex_areas = np.zeros(vertices.shape[0])\n    np.add.at(vertex_areas, triangles[:, 0], triangle_areas / 3)\n    np.add.at(vertex_areas, triangles[:, 1], triangle_areas / 3)\n    np.add.at(vertex_areas, triangles[:, 2], triangle_areas / 3)\n\n    return vertex_areas\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.get_euler_number","title":"<code>get_euler_number(vertices: npt.NDArray[np.floating[Any]], triangles: npt.NDArray[np.integer[Any]]) -&gt; int</code>","text":"<p>Calculate the Euler number of a surface.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (n_vertices, 3) containing vertex coordinates.</p> required <code>triangles</code> <code>NDArray[integer[Any]]</code> <p>numpy.ndarray Array of shape (n_triangles, 3) containing vertex indices for each triangular face.</p> required <p>Returns:</p> Name Type Description <code>euler_number</code> <code>int</code> <p>int The Euler number of the surface.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def get_euler_number(\n    vertices: npt.NDArray[np.floating[Any]],\n    triangles: npt.NDArray[np.integer[Any]],\n) -&gt; int:\n    \"\"\"\n    Calculate the Euler number of a surface.\n\n    Args:\n        vertices: numpy.ndarray\n            Array of shape (n_vertices, 3) containing vertex coordinates.\n        triangles: numpy.ndarray\n            Array of shape (n_triangles, 3) containing vertex indices for each\n            triangular face.\n\n    Returns:\n        euler_number: int\n            The Euler number of the surface.\n    \"\"\"\n    # Euler characteristic: V - E + F = 2 for closed surfaces\n    # where V = number of vertices, E = number of edges, F = number of faces\n\n    n_vertices = vertices.shape[0]\n    n_faces = triangles.shape[0]\n\n    # Compute number of unique edges\n    # Extract all edges from triangles\n    edges = np.vstack(\n        [\n            triangles[:, [0, 1]],\n            triangles[:, [1, 2]],\n            triangles[:, [2, 0]],\n        ],\n    )\n    # Sort edges so that [1,2] and [2,1] are considered the same\n    edges = np.sort(edges, axis=1)\n    # Count unique edges\n    n_edges = len(np.unique(edges, axis=0))\n\n    # Compute and return the Euler number\n    return int(n_vertices - n_edges + n_faces)\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.get_fslr_surface_indices_from_cifti","title":"<code>get_fslr_surface_indices_from_cifti(cifti_file: Path | None = None) -&gt; npt.NDArray[np.integer[Any]]</code>","text":"<p>Get the fs_LR surface indices from a CIFTI file (excluding the medial wall).</p> <p>Parameters:</p> Name Type Description Default <code>cifti_file</code> <code>Path | None</code> <p>str Path to the CIFTI file. By default a ones.dscalar.nii template will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[integer[Any]]</code> <p>np.ndarray The indices indicating of vertices present in the CIFTI format.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def get_fslr_surface_indices_from_cifti(\n    cifti_file: Path | None = None,\n) -&gt; npt.NDArray[np.integer[Any]]:\n    \"\"\"\n    Get the fs_LR surface indices from a CIFTI file (excluding the medial wall).\n\n    Args:\n        cifti_file: str\n            Path to the CIFTI file. By default a ones.dscalar.nii template will be used.\n\n    Returns:\n        np.ndarray\n            The indices indicating of vertices present in the CIFTI format.\n    \"\"\"\n    # Use default ones.dscalar.nii if no file is provided\n    if cifti_file is None:\n        cifti_file = Path(\n            str(\n                resources.files(\"spectranorm.data.templates.CIFTI\")\n                / \"ones.dscalar.nii\",\n            ),\n        )\n    cifti = nib.loadsave.load(str(cifti_file))\n    if not isinstance(cifti, nib.cifti2.cifti2.Cifti2Image):\n        err = f\"File {cifti_file} is not a valid CIFTI file.\"\n        raise TypeError(err)\n\n    # Extract the brain models for left and right cortical surfaces\n    brain_models = list(cifti.header.get_index_map(1).brain_models)  # type: ignore[no-untyped-call]\n    left_surface_model, right_surface_model = brain_models[0], brain_models[1]\n\n    # Return the indices for left and right surfaces\n    return np.concatenate(\n        [\n            np.asarray(left_surface_model.vertex_indices),\n            (\n                np.asarray(right_surface_model.vertex_indices)\n                + left_surface_model.surface_number_of_vertices\n            ),\n        ],\n    )\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.load_freesurfer_surface","title":"<code>load_freesurfer_surface(file: str) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]</code>","text":"<p>Load a FreeSurfer surface file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>str Path to the FreeSurfer surface file to be loaded.</p> required <p>Returns:</p> Name Type Description <code>vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (n_vertices, 3) containing vertex coordinates.</p> <code>triangles</code> <code>NDArray[integer[Any]]</code> <p>numpy.ndarray Array of shape (n_triangles, 3) containing vertex indices for each triangular face.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def load_freesurfer_surface(\n    file: str,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]:\n    \"\"\"\n    Load a FreeSurfer surface file.\n\n    Args:\n        file: str\n            Path to the FreeSurfer surface file to be loaded.\n\n    Returns:\n        vertices: numpy.ndarray\n            Array of shape (n_vertices, 3) containing vertex coordinates.\n        triangles: numpy.ndarray\n            Array of shape (n_triangles, 3) containing vertex indices for each\n            triangular face.\n    \"\"\"\n    surface = nib.freesurfer.io.read_geometry(file)  # type: ignore[no-untyped-call]\n    vertices = np.asarray(surface[0], dtype=np.float32)\n    triangles = np.asarray(surface[1], dtype=np.int32)\n    return vertices, triangles\n</code></pre>"},{"location":"api/utils/nitools/#spectranorm.utils.nitools.load_gifti_surface","title":"<code>load_gifti_surface(file: Path) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]</code>","text":"<p>Load a GIfTI surface file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path Path to the GIfTI surface file to be loaded (.gii).</p> required <p>Returns:</p> Name Type Description <code>vertices</code> <code>NDArray[floating[Any]]</code> <p>numpy.ndarray Array of shape (n_vertices, 3) containing vertex coordinates.</p> <code>triangles</code> <code>NDArray[integer[Any]]</code> <p>numpy.ndarray Array of shape (n_triangles, 3) containing vertex indices for each triangular face.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def load_gifti_surface(\n    file: Path,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]:\n    \"\"\"\n    Load a GIfTI surface file.\n\n    Args:\n        file: Path\n            Path to the GIfTI surface file to be loaded (.gii).\n\n    Returns:\n        vertices: numpy.ndarray\n            Array of shape (n_vertices, 3) containing vertex coordinates.\n        triangles: numpy.ndarray\n            Array of shape (n_triangles, 3) containing vertex indices for each\n            triangular face.\n    \"\"\"\n    gifti_data = cast(\"nib.gifti.gifti.GiftiImage\", nib.loadsave.load(file))\n    vertices = gifti_data.darrays[0].data\n    triangles = gifti_data.darrays[1].data\n    return vertices, triangles\n</code></pre>"},{"location":"api/utils/parallel/","title":"Parallel Utilities","text":"<p>The SpectraNorm package was implemented with parallel processing capabilities to efficiently handle large datasets and computationally intensive tasks. The parallel utilities module provides functions for managing parallel execution and tracking the progress of parallel tasks.</p>"},{"location":"api/utils/parallel/#spectranorm.utils.parallel","title":"<code>spectranorm.utils.parallel</code>","text":"<p>utils/parallel.py</p> <p>Utility functions for spectranorm's parallel execution (e.g. via Joblib).</p>"},{"location":"api/utils/parallel/#spectranorm.utils.parallel.ParallelTqdm","title":"<code>ParallelTqdm</code>","text":"<p>               Bases: <code>Parallel</code></p> <p>joblib.Parallel, but with a tqdm progressbar</p> <p>Parameters:</p> Name Type Description Default <code>total_tasks</code> <code>int | None</code> <p>int, default: None the number of expected jobs. Used in the tqdm progressbar. If None, try to infer from the length of the called iterator, and fallback to use the number of remaining items as soon as we finish dispatching. Note: use a list instead of an iterator if you want the total_tasks to be inferred from its length.</p> <code>None</code> <code>desc</code> <code>str | None</code> <p>str, default: None the description used in the tqdm progressbar.</p> <code>None</code> <code>disable_progressbar</code> <code>bool</code> <p>bool, default: False If True, a tqdm progressbar is not used.</p> <code>False</code> <code>show_joblib_header</code> <code>bool</code> <p>bool, default: False If True, show joblib header before the progressbar.</p> <code>False</code> Removed parameters <p>verbose: will be ignored</p> <p>Usage:</p> <p>from joblib import delayed</p> <p>from time import sleep</p> <p>ParallelTqdm(n_jobs=-1)([delayed(sleep)(.1) for _ in range(10)])</p> <p>80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:02&lt;00:00,  3.12tasks/s]</p> Source code in <code>src/spectranorm/utils/parallel.py</code> <pre><code>class ParallelTqdm(Parallel):\n    \"\"\"joblib.Parallel, but with a tqdm progressbar\n\n    Args:\n        total_tasks: int, default: None\n            the number of expected jobs. Used in the tqdm progressbar.\n            If None, try to infer from the length of the called iterator, and\n            fallback to use the number of remaining items as soon as we finish\n            dispatching.\n            Note: use a list instead of an iterator if you want the total_tasks\n            to be inferred from its length.\n        desc: str, default: None\n            the description used in the tqdm progressbar.\n        disable_progressbar: bool, default: False\n            If True, a tqdm progressbar is not used.\n        show_joblib_header: bool, default: False\n            If True, show joblib header before the progressbar.\n\n    Removed parameters:\n        verbose: will be ignored\n\n\n    Usage:\n    &gt; from joblib import delayed\n    &gt;\n    &gt; from time import sleep\n    &gt;\n    &gt; ParallelTqdm(n_jobs=-1)([delayed(sleep)(.1) for _ in range(10)])\n    &gt;&gt; 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:02&lt;00:00,  3.12tasks/s]\n\n    \"\"\"\n\n    _original_iterator: Iterable[Any] | None  # mimic joblib internal attribute\n    n_dispatched_tasks: int\n    n_completed_tasks: int\n\n    def __init__(\n        self,\n        *,\n        total_tasks: int | None = None,\n        desc: str | None = None,\n        disable_progressbar: bool = False,\n        show_joblib_header: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        if \"verbose\" in kwargs:\n            err = (\n                \"verbose is not supported. \"\n                \"Use disable_progressbar and show_joblib_header instead.\"\n            )\n            raise ValueError(err)\n        super().__init__(verbose=(1 if show_joblib_header else 0), **kwargs)\n        self.total_tasks = total_tasks\n        self.desc = desc\n        self.disable_progressbar = disable_progressbar\n        self.progress_bar: tqdm.tqdm[Any] | None = None\n\n    def __call__(self, iterable: Iterable[Any]) -&gt; Any:\n        try:\n            if self.total_tasks is None and isinstance(iterable, Sized):\n                # try to infer total_tasks from the length of the called iterator\n                with suppress(TypeError, AttributeError):\n                    self.total_tasks = len(iterable)\n            # call parent function\n            return super().__call__(iterable)\n        finally:\n            # close tqdm progress bar\n            if self.progress_bar is not None:\n                self.progress_bar.close()\n\n    __call__.__doc__ = Parallel.__call__.__doc__\n\n    def dispatch_one_batch(self, iterator: Iterable[Any]) -&gt; Any:\n        # start progress_bar, if not started yet.\n        if self.progress_bar is None:\n            self.progress_bar = tqdm.tqdm(\n                desc=self.desc,\n                total=self.total_tasks,\n                disable=self.disable_progressbar,\n                unit=\"tasks\",\n            )\n        # call parent function\n        return super().dispatch_one_batch(iterator)\n\n    dispatch_one_batch.__doc__ = Parallel.dispatch_one_batch.__doc__\n\n    def print_progress(self) -&gt; None:\n        \"\"\"Display the process of the parallel execution using tqdm\"\"\"\n        # if we finish dispatching, find total_tasks from the number of remaining items\n        if self.total_tasks is None and self._original_iterator is None:\n            self.total_tasks = self.n_dispatched_tasks\n            if self.progress_bar is not None:\n                self.progress_bar.total = self.total_tasks\n                self.progress_bar.refresh()\n        # update progressbar\n        if self.progress_bar is not None:\n            self.progress_bar.update(self.n_completed_tasks - self.progress_bar.n)\n</code></pre>"},{"location":"api/utils/parallel/#spectranorm.utils.parallel.ParallelTqdm.print_progress","title":"<code>print_progress() -&gt; None</code>","text":"<p>Display the process of the parallel execution using tqdm</p> Source code in <code>src/spectranorm/utils/parallel.py</code> <pre><code>def print_progress(self) -&gt; None:\n    \"\"\"Display the process of the parallel execution using tqdm\"\"\"\n    # if we finish dispatching, find total_tasks from the number of remaining items\n    if self.total_tasks is None and self._original_iterator is None:\n        self.total_tasks = self.n_dispatched_tasks\n        if self.progress_bar is not None:\n            self.progress_bar.total = self.total_tasks\n            self.progress_bar.refresh()\n    # update progressbar\n    if self.progress_bar is not None:\n        self.progress_bar.update(self.n_completed_tasks - self.progress_bar.n)\n</code></pre>"},{"location":"api/utils/stats/","title":"Statistical Utilities","text":"<p>The statistical utilities module provides functions for performing various statistical analyses and computations that are commonly used in the context of spectral normative modeling. Notably, this includes functions complementary to the metrics utilities, such as functions for computing log-likelihoods and centile scores, which are essential for evaluating the fit of normative models and for interpreting the results of empirical data analyses.</p>"},{"location":"api/utils/stats/#spectranorm.utils.stats","title":"<code>spectranorm.utils.stats</code>","text":"<p>utils/stats.py</p> <p>Statistical utility functions for the Spectranorm package.</p>"},{"location":"api/utils/stats/#spectranorm.utils.stats.compute_censored_log_likelihood","title":"<code>compute_censored_log_likelihood(observations: npt.NDArray[np.floating[Any]], predicted_mus: npt.NDArray[np.floating[Any]], predicted_sigmas: npt.NDArray[np.floating[Any]], censored_quantile: float = 0.01) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute censored log likelihood, replacing extreme low likelihoods with a censoring threshold.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Observed data points (N,).</p> required <code>predicted_mus</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted means for each observation (N,).</p> required <code>predicted_sigmas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted standard deviations for each observation (N,).</p> required <code>censored_quantile</code> <code>float</code> <p>float (default=0.01) Quantile below which log-likelihoods are censored.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray: The censored log likelihood of all observations.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_censored_log_likelihood(\n    observations: npt.NDArray[np.floating[Any]],\n    predicted_mus: npt.NDArray[np.floating[Any]],\n    predicted_sigmas: npt.NDArray[np.floating[Any]],\n    censored_quantile: float = 0.01,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute censored log likelihood, replacing extreme low likelihoods with a censoring\n    threshold.\n\n    Args:\n        observations: np.ndarray\n            Observed data points (N,).\n        predicted_mus: np.ndarray\n            Predicted means for each observation (N,).\n        predicted_sigmas: np.ndarray\n            Predicted standard deviations for each observation (N,).\n        censored_quantile: float (default=0.01)\n            Quantile below which log-likelihoods are censored.\n\n    Returns:\n        np.ndarray:\n            The censored log likelihood of all observations.\n    \"\"\"\n    # Compute log likelihoods\n    log_likelihoods = compute_log_likelihood(\n        observations,\n        predicted_mus,\n        predicted_sigmas,\n    )\n\n    if censored_quantile &lt;= 0:\n        return log_likelihoods\n\n    # Standardized residuals\n    z_scores = (observations - predicted_mus) / predicted_sigmas\n\n    # Two-sided z threshold based on standard normal\n    z_threshold = stats.norm.ppf(1 - censored_quantile)\n\n    # Replace extreme z's with constant tail mass\n    censored_value = np.log(2 * censored_quantile)\n\n    # Apply censoring (two-sided) and return\n    return np.where(\n        np.abs(z_scores) &gt; z_threshold,\n        censored_value,\n        log_likelihoods,\n    )\n</code></pre>"},{"location":"api/utils/stats/#spectranorm.utils.stats.compute_centiles_from_z_scores","title":"<code>compute_centiles_from_z_scores(z_scores: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Convert z-scores to percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>z_scores</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of z-scores.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of percentiles corresponding to the z-scores.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_centiles_from_z_scores(\n    z_scores: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Convert z-scores to percentiles.\n\n    Args:\n        z_scores: np.ndarray\n            Array of z-scores.\n\n    Returns:\n        np.ndarray\n            Array of percentiles corresponding to the z-scores.\n    \"\"\"\n    # Convert z-scores to percentiles using the cumulative distribution function (CDF)\n    return np.asarray(stats.norm.cdf(z_scores) * 100)\n</code></pre>"},{"location":"api/utils/stats/#spectranorm.utils.stats.compute_correlation_significance","title":"<code>compute_correlation_significance(correlation_matrix: npt.NDArray[np.floating[Any]], n_samples: int, correlation_threshold: float = 0.0, correction_method: str = 'fdr_bh') -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the significance of correlations between variables in the data matrix, thresholded by a specified correlation value, using Fisher's z-transformation.</p> <p>Parameters:</p> Name Type Description Default <code>correlation_matrix</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray A 2D array of pairwise correlation values.</p> required <code>n_samples</code> <code>int</code> <p>int The number of samples used to compute correlation.</p> required <code>correlation_threshold</code> <code>float</code> <p>float (default=0.0) The correlation threshold above which correlations are to be considered significant.</p> <code>0.0</code> <code>correction_method</code> <code>str</code> <p>str (default='fdr_bh') Method for multiple testing correction. Options include 'bonferroni', 'holm', 'fdr_bh', etc. See statsmodels.stats.multitest.multipletests for more details.</p> <code>'fdr_bh'</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray A matrix of p-values indicating the significance of each correlation. (Testing if the correlation is significantly greater than the threshold.)</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_correlation_significance(\n    correlation_matrix: npt.NDArray[np.floating[Any]],\n    n_samples: int,\n    correlation_threshold: float = 0.0,\n    correction_method: str = \"fdr_bh\",\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the significance of correlations between variables in the data matrix,\n    thresholded by a specified correlation value, using Fisher's z-transformation.\n\n    Args:\n        correlation_matrix: np.ndarray\n            A 2D array of pairwise correlation values.\n        n_samples: int\n            The number of samples used to compute correlation.\n        correlation_threshold: float (default=0.0)\n            The correlation threshold above which correlations are to be considered\n            significant.\n        correction_method: str (default='fdr_bh')\n            Method for multiple testing correction. Options include 'bonferroni',\n            'holm', 'fdr_bh', etc. See statsmodels.stats.multitest.multipletests for\n            more details.\n\n    Returns:\n        np.ndarray\n            A matrix of p-values indicating the significance of each correlation.\n            (Testing if the correlation is significantly greater than the threshold.)\n    \"\"\"\n    # set the diagonal to zero to avoid NaNs in arctanh\n    np.fill_diagonal(correlation_matrix, 0)\n\n    # Fisher's z-transformation\n    fisher_z = np.arctanh(correlation_matrix)\n    z_threshold = np.arctanh(correlation_threshold)\n\n    # Standard error of the Fisher z\n    se = 1 / np.sqrt(n_samples - 3)\n\n    # Compute the test statistic\n    z_score = (np.abs(fisher_z) - z_threshold) / se\n\n    # compute uncorrected p-values for one-tailed test\n    p_values = 1 - stats.norm.cdf(z_score)\n\n    # Take upper triangle of the p-value matrix (excluding diagonal)\n    triu_indices = np.triu_indices_from(p_values, k=1)\n    p_values_triu = p_values[triu_indices]\n\n    # Apply multiple testing correction\n    _, corrected_p_values, _, _ = multipletests(\n        p_values_triu,\n        method=correction_method,\n    )\n\n    # Reconstruct the full p-value matrix\n    p_values_corrected = np.full_like(p_values, fill_value=np.nan, dtype=np.float64)\n    p_values_corrected[triu_indices] = corrected_p_values\n    p_values_corrected[(triu_indices[1], triu_indices[0])] = corrected_p_values\n\n    return p_values_corrected\n</code></pre>"},{"location":"api/utils/stats/#spectranorm.utils.stats.compute_log_likelihood","title":"<code>compute_log_likelihood(observations: npt.NDArray[np.floating[Any]], predicted_mus: npt.NDArray[np.floating[Any]], predicted_sigmas: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the log likelihood of observations given predicted means and standard deviations.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Observed data points.</p> required <code>predicted_mus</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted means for the observations.</p> required <code>predicted_sigmas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted standard deviations for the observations.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihood of each observation.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_log_likelihood(\n    observations: npt.NDArray[np.floating[Any]],\n    predicted_mus: npt.NDArray[np.floating[Any]],\n    predicted_sigmas: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the log likelihood of observations given predicted means and standard\n    deviations.\n\n    Args:\n        observations: np.ndarray\n            Observed data points.\n        predicted_mus: np.ndarray\n            Predicted means for the observations.\n        predicted_sigmas: np.ndarray\n            Predicted standard deviations for the observations.\n\n    Returns:\n        np.ndarray\n            Log likelihood of each observation.\n    \"\"\"\n    return np.asarray(\n        stats.norm.logpdf(observations, loc=predicted_mus, scale=predicted_sigmas),\n    )\n</code></pre>"},{"location":"tutorials/","title":"Tutorials overview","text":"<p>The ensuing sections contain interactive Jupyter notebooks that demonstrate how to use various functionalities available in SpectraNorm. Each tutorial focuses on a specific aspect of the package, providing step-by-step instructions and example code. The tutorials are designed to help users get started with spectral normative modeling and to illustrate how to apply the methods to exemplary datasets.</p> <p>The tutorials start by explaining most basic functionalities of the package, such as how to fit a univariate normative model to data and how to visualize the results. Subsequent tutorials will cover more advanced topics, such as how to construct graph spectral eigenbases, and how to apply spectral normative modeling to charachterize data defined on graphs. The tutorials also include neuroimaging-specific examples, such as how to apply spectral normative modeling to cortical thickness data extracted from a new imaging study.</p> <p>These tutorials are intended to be accessible to users with varying levels of experience with programming and normative modeling.</p>"},{"location":"tutorials/#overview-of-available-tutorials","title":"Overview of available tutorials:","text":"<ul> <li> <p>A simple univariate normative model: This tutorial demonstrates how to fit a simple univariate normative model to data, and how to visualize the results using the plotting utilities available in the package.</p> </li> <li> <p>Constructing graph spectral eigenbases: This tutorial demonstrates how to construct graph spectral eigenbases using the utilities available in the package, and how to use these bases to represent high-dimensional data defined on graphs.</p> </li> </ul>"},{"location":"tutorials/eigenmode_bases/","title":"Eigenmode Bases","text":"In\u00a0[1]: Copied! <pre>\"\"\"\nWe want to create a 50x50 lattice structure, containing 2,500 nodes, together\nrepresenting a 2D spatial domain covering points from (0, 0) to (1, 1). Each node in\nthis lattice will be associated with a specific point in this spatial domain. The\nlattice will be constructed such that the nodes are evenly spaced.\n\nTo achieve this, we will define a grid of points where each point corresponds to a node\nin the lattice. The spacing between the nodes will be determined by dividing the range\nof the spatial domain (which is 1 unit in both x and y directions) by the number of\nnodes along each dimension (which is 50). This means that each node will be spaced\n0.02 units apart in both directions.\n\nAssuming every node is connected to its immediate neighbors (up, down, left, right), we\ncan represent the connections between the nodes using a sparse adjacency matrix. This\nmatrix will have dimensions of 2,500 x 2,500, where each entry indicates whether there\nis a connection between the corresponding nodes. The connections will be represented as\n1s in the matrix, while the absence of connections will be represented as 0s. This\nsparse representation is efficient for large matrices where most entries are zero, which\nis the case here since each node is only connected to a few neighbors.\n\"\"\"\n\n# Several imports used in this notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import sparse\n\n# Import the SpectraNorm package\nfrom spectranorm import snm\n\n# Define the number of nodes along each dimension\nnum_nodes = 50\n\n# Create a grid of points\nx = np.linspace(0, 1, num_nodes)\ny = np.linspace(0, 1, num_nodes)\nX, Y = np.meshgrid(x, y)\n\n# Flatten the grid to create a list of node coordinates\nnodes = np.column_stack((X.flatten(), Y.flatten()))\n\n# Let's create a sparse adjacency matrix to represent the connections between the nodes\nadjacency_matrix = sparse.lil_matrix((num_nodes**2, num_nodes**2))\n\n# Connect each node to its immediate neighbors (up, down, left, right)\nfor i in range(num_nodes):\n    for j in range(num_nodes):\n        node_index = i * num_nodes + j\n        if i &gt; 0:  # Connect to the node above\n            adjacency_matrix[node_index, (i - 1) * num_nodes + j] = 1\n        if i &lt; num_nodes - 1:  # Connect to the node below\n            adjacency_matrix[node_index, (i + 1) * num_nodes + j] = 1\n        if j &gt; 0:  # Connect to the node to the left\n            adjacency_matrix[node_index, i * num_nodes + (j - 1)] = 1\n        if j &lt; num_nodes - 1:  # Connect to the node to the right\n            adjacency_matrix[node_index, i * num_nodes + (j + 1)] = 1\n\n# Let's convert the adjacency matrix to a more efficient format for computations\nadjacency_matrix = adjacency_matrix.tocsr()\n\n# Plot the lattice structure and the connections between the nodes\nplt.figure(figsize=(10, 10))\n\n# Plot the nodes\nplt.scatter(nodes[:, 0], nodes[:, 1], color=\"teal\", s=10, label=\"Nodes\", zorder=2)\n\n# Plot the connections\nfor i in range(num_nodes**2):\n    neighbors = adjacency_matrix[i].nonzero()[1]\n    for neighbor in neighbors:\n        plt.plot(\n            [nodes[i, 0],\n             nodes[neighbor, 0]],\n             [nodes[i, 1], nodes[neighbor, 1]],\n             color=\"gray\", linewidth=0.5, zorder=1,\n        )\n\n# Legend outside the axis\nplt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05))\nplt.show()\n</pre> \"\"\" We want to create a 50x50 lattice structure, containing 2,500 nodes, together representing a 2D spatial domain covering points from (0, 0) to (1, 1). Each node in this lattice will be associated with a specific point in this spatial domain. The lattice will be constructed such that the nodes are evenly spaced.  To achieve this, we will define a grid of points where each point corresponds to a node in the lattice. The spacing between the nodes will be determined by dividing the range of the spatial domain (which is 1 unit in both x and y directions) by the number of nodes along each dimension (which is 50). This means that each node will be spaced 0.02 units apart in both directions.  Assuming every node is connected to its immediate neighbors (up, down, left, right), we can represent the connections between the nodes using a sparse adjacency matrix. This matrix will have dimensions of 2,500 x 2,500, where each entry indicates whether there is a connection between the corresponding nodes. The connections will be represented as 1s in the matrix, while the absence of connections will be represented as 0s. This sparse representation is efficient for large matrices where most entries are zero, which is the case here since each node is only connected to a few neighbors. \"\"\"  # Several imports used in this notebook import matplotlib.pyplot as plt import numpy as np from scipy import sparse  # Import the SpectraNorm package from spectranorm import snm  # Define the number of nodes along each dimension num_nodes = 50  # Create a grid of points x = np.linspace(0, 1, num_nodes) y = np.linspace(0, 1, num_nodes) X, Y = np.meshgrid(x, y)  # Flatten the grid to create a list of node coordinates nodes = np.column_stack((X.flatten(), Y.flatten()))  # Let's create a sparse adjacency matrix to represent the connections between the nodes adjacency_matrix = sparse.lil_matrix((num_nodes**2, num_nodes**2))  # Connect each node to its immediate neighbors (up, down, left, right) for i in range(num_nodes):     for j in range(num_nodes):         node_index = i * num_nodes + j         if i &gt; 0:  # Connect to the node above             adjacency_matrix[node_index, (i - 1) * num_nodes + j] = 1         if i &lt; num_nodes - 1:  # Connect to the node below             adjacency_matrix[node_index, (i + 1) * num_nodes + j] = 1         if j &gt; 0:  # Connect to the node to the left             adjacency_matrix[node_index, i * num_nodes + (j - 1)] = 1         if j &lt; num_nodes - 1:  # Connect to the node to the right             adjacency_matrix[node_index, i * num_nodes + (j + 1)] = 1  # Let's convert the adjacency matrix to a more efficient format for computations adjacency_matrix = adjacency_matrix.tocsr()  # Plot the lattice structure and the connections between the nodes plt.figure(figsize=(10, 10))  # Plot the nodes plt.scatter(nodes[:, 0], nodes[:, 1], color=\"teal\", s=10, label=\"Nodes\", zorder=2)  # Plot the connections for i in range(num_nodes**2):     neighbors = adjacency_matrix[i].nonzero()[1]     for neighbor in neighbors:         plt.plot(             [nodes[i, 0],              nodes[neighbor, 0]],              [nodes[i, 1], nodes[neighbor, 1]],              color=\"gray\", linewidth=0.5, zorder=1,         )  # Legend outside the axis plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05)) plt.show() <p>As evident in the plot, the lattice graph consists of nodes arranged in a grid pattern, with edges connecting neighboring nodes. This structure can be used to encode high-dimensional data that has a spatial organization, such as images or information confined to a 2D plane.</p> <p>Theories in graph signal processing allow us to compute eigenmode bases from a given graph structure, which can then be used to represent and analyze data defined over that graph. In the next section, we will see how to compute eigenmode bases using <code>SpectraNorm</code>.</p> In\u00a0[2]: Copied! <pre># Let's first compute the eigenvalues and eigenvectors of the graph Laplacian\n(\n    eigenvalues, eigenvectors\n) = snm.utils.gsp.compute_symmetric_normalized_laplacian_eigenmodes(\n    adjacency_matrix=adjacency_matrix,\n    num_eigenvalues=100,\n)\n\n# Now let's store the outputs in an `EigenmodeBasis` object\neigenmode_basis = snm.utils.gsp.EigenmodeBasis(\n    eigenvalues=eigenvalues[:100],\n    eigenvectors=eigenvectors.T[:, :100],\n)\n\neigenmode_basis\n</pre> # Let's first compute the eigenvalues and eigenvectors of the graph Laplacian (     eigenvalues, eigenvectors ) = snm.utils.gsp.compute_symmetric_normalized_laplacian_eigenmodes(     adjacency_matrix=adjacency_matrix,     num_eigenvalues=100, )  # Now let's store the outputs in an `EigenmodeBasis` object eigenmode_basis = snm.utils.gsp.EigenmodeBasis(     eigenvalues=eigenvalues[:100],     eigenvectors=eigenvectors.T[:, :100], )  eigenmode_basis Out[2]: <pre>EigenmodeBasis(n_modes=100, n_features=2500)</pre> <p>Note that the <code>gsp.EigenmodeBasis</code> class in SpectraNorm provides a convenient interface for working with eigenmode bases. As we will see in the coming sections, this class allows us to easily encode and decode data using the computed eigenmode basis.</p> <p>But before we dive into encoding and decoding, let's first visualize some of the computed eigenmodes to get an intuition for what they represent. The first few eigenmodes typically capture the low-frequency components of the graph, which correspond to smooth variations across the network. As we move to higher eigenmodes, we capture higher-frequency components, which correspond to more localized variations in the data.</p> <p>The following visualization, benefits from the fact that our lattice graph has a spatial structure, allowing us to plot the eigenmodes as images. Each eigenmode can be reshaped back into a 50x50 grid, revealing patterns that correspond to different spatial frequencies on the lattice.</p> In\u00a0[3]: Copied! <pre># Let's use gridspec to visualize the first few eigenmodes as images\nfig, axes = plt.subplots(5, 5, figsize=(15, 15))\n\nfor i in range(25):\n    ax = axes[i // 5, i % 5]\n    eigenmode = eigenmode_basis.eigenvectors[:, i].reshape(num_nodes, num_nodes)\n    ax.imshow(eigenmode, cmap=\"Spectral\")\n    ax.set_title(f\"Eigenmode {i + 1}\")\n    ax.axis(\"off\")\n</pre> # Let's use gridspec to visualize the first few eigenmodes as images fig, axes = plt.subplots(5, 5, figsize=(15, 15))  for i in range(25):     ax = axes[i // 5, i % 5]     eigenmode = eigenmode_basis.eigenvectors[:, i].reshape(num_nodes, num_nodes)     ax.imshow(eigenmode, cmap=\"Spectral\")     ax.set_title(f\"Eigenmode {i + 1}\")     ax.axis(\"off\") In\u00a0[4]: Copied! <pre># Let's use gridspec to visualize the last few eigenmodes as images\nfig, axes = plt.subplots(5, 5, figsize=(15, 15))\n\nfor i in range(25):\n    ax = axes[i // 5, i % 5]\n    eigenmode = eigenmode_basis.eigenvectors[:, -25 + i].reshape(num_nodes, num_nodes)\n    ax.imshow(eigenmode, cmap=\"Spectral\")\n    ax.set_title(f\"Eigenmode {eigenmode_basis.eigenvectors.shape[1] - 25 + i + 1}\")\n    ax.axis(\"off\")\n</pre> # Let's use gridspec to visualize the last few eigenmodes as images fig, axes = plt.subplots(5, 5, figsize=(15, 15))  for i in range(25):     ax = axes[i // 5, i % 5]     eigenmode = eigenmode_basis.eigenvectors[:, -25 + i].reshape(num_nodes, num_nodes)     ax.imshow(eigenmode, cmap=\"Spectral\")     ax.set_title(f\"Eigenmode {eigenmode_basis.eigenvectors.shape[1] - 25 + i + 1}\")     ax.axis(\"off\") <p>As we can see from the visualizations, the first few eigenmodes capture smooth, low-frequency patterns across the lattice, while the higher eigenmodes capture more complex, high-frequency patterns. This illustrates how eigenmode bases can provide a hierarchical representation of data defined over a graph structure.</p> In\u00a0[5]: Copied! <pre>from skimage.transform import resize\nfrom sklearn.datasets import fetch_openml\n\n# 1. Load MNIST data (this may take a minute the first time)\nprint(\"Fetching MNIST...\")\nmnist = fetch_openml(\"mnist_784\", version=1, as_frame=False, parser=\"auto\")\nX, y = mnist.data, mnist.target.astype(int)\n\n# 2. Extract one sample of each digit (0-9)\ndigits = []\nfor i in range(10):\n    # Find the first index where the label matches the digit\n    idx = np.where(y == i)[0][0]\n    # Reshape from flat 784 to 28x28\n    img_28 = X[idx].reshape(28, 28)\n\n    # 3. Resize to 50x50\n    # anti_aliasing prevents jagged edges during upscaling\n    img_50 = resize(img_28, (50, 50), anti_aliasing=True)\n\n    digits.append(img_50)\n\n# 3. For each digit, encode it using either 20 or 100 eigenmodes, then decode it back\n# to the original space to visualize the results\ndigits_20 = []\ndigits_100 = []\nfor digit in digits:\n    # Flatten the 50x50 image to a vector of length 2500\n    digit_vector = digit.flatten()\n\n    # Encode using the first 20 eigenmodes\n    encoded_20 = eigenmode_basis.encode(digit_vector, n_modes=20)\n    digits_20.append(eigenmode_basis.decode(encoded_20, n_modes=20).reshape(50, 50))\n\n    # Encode using the first 100 eigenmodes\n    encoded_100 = eigenmode_basis.encode(digit_vector, n_modes=100)\n    digits_100.append(eigenmode_basis.decode(encoded_100, n_modes=100).reshape(50, 50))\n\n# 4. Visualize the original and reconstructed digits\n# We'll use a grid of 3 columns (original, 20 modes, 100 modes) and 10 rows (digits 0-9)\nfig, axes = plt.subplots(10, 3, figsize=(6, 20))\nfor i in range(10):\n    # Original digit\n    axes[i, 0].imshow(digits[i], cmap=\"gray\")\n    axes[i, 0].set_title(f\"Original: {i}\")\n    axes[i, 0].axis(\"off\")\n\n    # Reconstructed with 20 modes\n    axes[i, 1].imshow(digits_20[i], cmap=\"gray\")\n    axes[i, 1].set_title(f\"20 Modes: {i}\")\n    axes[i, 1].axis(\"off\")\n\n    # Reconstructed with 100 modes\n    axes[i, 2].imshow(digits_100[i], cmap=\"gray\")\n    axes[i, 2].set_title(f\"100 Modes: {i}\")\n    axes[i, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> from skimage.transform import resize from sklearn.datasets import fetch_openml  # 1. Load MNIST data (this may take a minute the first time) print(\"Fetching MNIST...\") mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False, parser=\"auto\") X, y = mnist.data, mnist.target.astype(int)  # 2. Extract one sample of each digit (0-9) digits = [] for i in range(10):     # Find the first index where the label matches the digit     idx = np.where(y == i)[0][0]     # Reshape from flat 784 to 28x28     img_28 = X[idx].reshape(28, 28)      # 3. Resize to 50x50     # anti_aliasing prevents jagged edges during upscaling     img_50 = resize(img_28, (50, 50), anti_aliasing=True)      digits.append(img_50)  # 3. For each digit, encode it using either 20 or 100 eigenmodes, then decode it back # to the original space to visualize the results digits_20 = [] digits_100 = [] for digit in digits:     # Flatten the 50x50 image to a vector of length 2500     digit_vector = digit.flatten()      # Encode using the first 20 eigenmodes     encoded_20 = eigenmode_basis.encode(digit_vector, n_modes=20)     digits_20.append(eigenmode_basis.decode(encoded_20, n_modes=20).reshape(50, 50))      # Encode using the first 100 eigenmodes     encoded_100 = eigenmode_basis.encode(digit_vector, n_modes=100)     digits_100.append(eigenmode_basis.decode(encoded_100, n_modes=100).reshape(50, 50))  # 4. Visualize the original and reconstructed digits # We'll use a grid of 3 columns (original, 20 modes, 100 modes) and 10 rows (digits 0-9) fig, axes = plt.subplots(10, 3, figsize=(6, 20)) for i in range(10):     # Original digit     axes[i, 0].imshow(digits[i], cmap=\"gray\")     axes[i, 0].set_title(f\"Original: {i}\")     axes[i, 0].axis(\"off\")      # Reconstructed with 20 modes     axes[i, 1].imshow(digits_20[i], cmap=\"gray\")     axes[i, 1].set_title(f\"20 Modes: {i}\")     axes[i, 1].axis(\"off\")      # Reconstructed with 100 modes     axes[i, 2].imshow(digits_100[i], cmap=\"gray\")     axes[i, 2].set_title(f\"100 Modes: {i}\")     axes[i, 2].axis(\"off\")  plt.tight_layout() plt.show() <pre>Fetching MNIST...\n</pre> <p>As we can see from the visualizations, the first few eigenmodes can only capture the most basic, low-frequency features of the images, resulting in blurry reconstructions. As we include more eigenmodes in the encoding, we can capture more details and higher-frequency features, leading to sharper reconstructions that more closely resemble the original images. This illustrates how eigenmode bases can be used for efficient data representation and dimensionality reduction while preserving important structural information.</p> <p>The example above demonstrates how keeping only 100 eigenmodes (out of 2500) can still capture a significant amount of information about the original images, to the extent that we can visually distinguish the digits, while drastically reducing the dimensionality of the data. This is a key advantage of using eigenmode bases for representing high-dimensional data defined over graphs.</p> In\u00a0[6]: Copied! <pre>eigenmode_basis.save(\"eigenmode_basis.joblib\")\n</pre> eigenmode_basis.save(\"eigenmode_basis.joblib\")"},{"location":"tutorials/eigenmode_bases/#eigenmode-bases","title":"Eigenmode Bases\u00b6","text":"<p>This tutorial demonstrates how to use the eigenmode basis functionality in SpectraNorm. Eigenmode bases are a powerful tool for representing and analyzing high-dimensional data in a lower-dimensional latent space.</p>"},{"location":"tutorials/eigenmode_bases/#overview","title":"Overview\u00b6","text":"<p>In this tutorial, we will cover the following topics:</p> <ol> <li>What are eigenmode bases?</li> <li>How to compute eigenmode bases using SpectraNorm.</li> <li>How to use eigenmode bases for data analysis.</li> </ol>"},{"location":"tutorials/eigenmode_bases/#what-are-eigenmode-bases","title":"What are Eigenmode Bases?\u00b6","text":"<p>Eigenmode bases are a set of (usually orthonormal) basis functions that can be used to represent high-dimensional data in a lower-dimensional latent space. They can be derived from the eigenvectors of a Laplacian matrix describing the data structure.</p> <p>Here, we'll focus on graph theoretical approaches to compute eigenmode bases, which are particularly useful for analyzing high-dimensional data that is expressed over a graph-like structure (e.g., social networks, brain connectivity, transportation networks, etc.).</p>"},{"location":"tutorials/eigenmode_bases/#a-simple-lattice-example","title":"A simple lattice example\u00b6","text":"<p>To illustrate the concept of eigenmode bases, let's consider a simple 2D lattice graph. This is a simple grid structure that can be used to represent spatial data confined to a 2D plane.</p> <p>In the script below, we will create a 50x50 lattice graph to be used for the rest of the tutorial.</p>"},{"location":"tutorials/eigenmode_bases/#computing-eigenmode-bases","title":"Computing Eigenmode Bases\u00b6","text":"<p>We can use the Graph Signal Processing (GSP) utilities in SpectraNorm to compute eigenmode bases from a given graph, e.g., the lattice graph we just created. The GSP module provides functions to compute the graph Laplacian and its eigenvectors, which form the eigenmode basis.</p> <p>Considering that our lattice graph has 2500 nodes, the complete set of eigenmodes would consist of 2500 eigenvectors. However, in practice, we often only need a subset of these eigenmodes to capture the essential structure of the data. For example, as shown in the script below, we might choose to compute only the first 100 eigenmodes, which correspond to the lowest frequencies in the graph.</p>"},{"location":"tutorials/eigenmode_bases/#encoding-and-decoding-with-eigenmode-bases","title":"Encoding and Decoding with Eigenmode Bases\u00b6","text":"<p>With the computed eigenmode basis, we can now encode and decode data defined over the graph. Encoding involves projecting the original high-dimensional data onto the eigenmode basis to obtain a lower-dimensional representation. Decoding involves reconstructing the original data from its lower-dimensional representation in the eigenmode basis.</p> <p>Benefiting from the orthonormality of the eigenmode basis, the implemented functionality in SpectraNorm (<code>gsp.EigenmodeBasis.encode</code> and <code>gsp.EigenmodeBasis.decode</code>) allows us to easily perform these operations via optimized matrix multiplications.</p> <p>As we used a lattice graph for this tutorial, let's try to encode and decode some exemplary images (as form of data that can be conveniently represented on a lattice graph) using the computed eigenmode basis. We will see how well we can reconstruct the original images using only a subset of the eigenmodes, demonstrating the power of eigenmode bases for dimensionality reduction and data representation.</p> <p>For the sake of simplicity, let's utilize some black and white images that can be easily represented on our 50x50 lattice graph, e.g., digits from the MNIST dataset. We will encode these images into the eigenmode basis and then decode them back to the original space (with different numbers of eigenmodes), comparing the reconstructed images to the originals to assess the quality of the representation.</p>"},{"location":"tutorials/eigenmode_bases/#saving-and-loading-eigenmode-bases","title":"Saving and Loading Eigenmode Bases\u00b6","text":"<p>The computed eigenmode basis can be saved to disk for later use, allowing us to reuse the same basis for encoding and decoding without having to recompute it each time. SpectraNorm provides functionality to save and load eigenmode bases using the <code>gsp.EigenmodeBasis.save</code> and <code>gsp.EigenmodeBasis.load</code> methods.</p> <p>Below, we provide an example of how to save the computed eigenmode basis used in this tutorial. The next tutorial on Spectral Normative Modeling will load this saved eigenmode basis to perform normative modeling analyses on this hypothetical 2D lattice graph data.</p>"},{"location":"tutorials/eigenmode_bases/#concluding-remarks","title":"Concluding Remarks\u00b6","text":"<p>This tutorial was meant as a brief introduction to concepts and applications of graph signal processing and eigenmode bases, using a simple lattice graph as an example. The simple graph spectral encoding and decoding operations, formulated as matrix multiplications, can help summarize high-dimensional data in a lower-dimensional latent space, while preserving important structural information.</p> <p>Note that while this tutorial focused on a simple lattice graph, the same principles apply to more complex graph structures. For instance, in neuroimaging, nodes of a hypothetical graph encoding high-dimensional brain data could represent vertices on the cortical surface, voxels in the brain, or possibly fixels in the case of diffusion MRI data. The edges of the graph could represent geometric proximity, functional connectivity, or structural connectivity between these nodes. By computing eigenmode bases from such graphs, we can find concise representations of complex brain data that capture important patterns and relationships.</p> <p>Without loss of generality, the same approach described above can be applied to any such graph structure, allowing us to reconstruct and analyze high-dimensional data in a lower-dimensional latent space using eigenmode bases.</p>"},{"location":"tutorials/spectral_model/","title":"Spectral Model","text":"In\u00a0[1]: Copied! <pre># Several imports used in this notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse, stats\n\n# Import the SpectraNorm package\nfrom spectranorm import snm\n</pre> # Several imports used in this notebook import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy import sparse, stats  # Import the SpectraNorm package from spectranorm import snm  In\u00a0[2]: Copied! <pre># Define the number of nodes along each dimension\nnum_nodes = 50\n\n# Create a grid of points\nx = np.linspace(0, 1, num_nodes)\ny = np.linspace(0, 1, num_nodes)\nX, Y = np.meshgrid(x, y)\n\n# Flatten the grid to create a list of node coordinates\nnodes = np.column_stack((X.flatten(), Y.flatten()))\n\n# Let's create a sparse adjacency matrix to represent the connections between the nodes\nadjacency_matrix = sparse.lil_matrix((num_nodes**2, num_nodes**2))\n\n# Connect each node to its immediate neighbors (up, down, left, right)\nfor i in range(num_nodes):\n    for j in range(num_nodes):\n        node_index = i * num_nodes + j\n        if i &gt; 0:  # Connect to the node above\n            adjacency_matrix[node_index, (i - 1) * num_nodes + j] = 1\n        if i &lt; num_nodes - 1:  # Connect to the node below\n            adjacency_matrix[node_index, (i + 1) * num_nodes + j] = 1\n        if j &gt; 0:  # Connect to the node to the left\n            adjacency_matrix[node_index, i * num_nodes + (j - 1)] = 1\n        if j &lt; num_nodes - 1:  # Connect to the node to the right\n            adjacency_matrix[node_index, i * num_nodes + (j + 1)] = 1\n\n# Let's convert the adjacency matrix to a more efficient format for computations\nadjacency_matrix = adjacency_matrix.tocsr()\n</pre> # Define the number of nodes along each dimension num_nodes = 50  # Create a grid of points x = np.linspace(0, 1, num_nodes) y = np.linspace(0, 1, num_nodes) X, Y = np.meshgrid(x, y)  # Flatten the grid to create a list of node coordinates nodes = np.column_stack((X.flatten(), Y.flatten()))  # Let's create a sparse adjacency matrix to represent the connections between the nodes adjacency_matrix = sparse.lil_matrix((num_nodes**2, num_nodes**2))  # Connect each node to its immediate neighbors (up, down, left, right) for i in range(num_nodes):     for j in range(num_nodes):         node_index = i * num_nodes + j         if i &gt; 0:  # Connect to the node above             adjacency_matrix[node_index, (i - 1) * num_nodes + j] = 1         if i &lt; num_nodes - 1:  # Connect to the node below             adjacency_matrix[node_index, (i + 1) * num_nodes + j] = 1         if j &gt; 0:  # Connect to the node to the left             adjacency_matrix[node_index, i * num_nodes + (j - 1)] = 1         if j &lt; num_nodes - 1:  # Connect to the node to the right             adjacency_matrix[node_index, i * num_nodes + (j + 1)] = 1  # Let's convert the adjacency matrix to a more efficient format for computations adjacency_matrix = adjacency_matrix.tocsr()  In\u00a0[3]: Copied! <pre>eigenmode_basis = snm.utils.gsp.EigenmodeBasis.load(\"eigenmode_basis.joblib\")\n</pre> eigenmode_basis = snm.utils.gsp.EigenmodeBasis.load(\"eigenmode_basis.joblib\") In\u00a0[22]: Copied! <pre>def generate_bivariate_gaussian_image(center_x, center_y, angle, sigma_major, sigma_minor):\n    num_nodes = 50\n    x = np.linspace(0, 1, num_nodes)\n    y = np.linspace(0, 1, num_nodes)\n    X, Y = np.meshgrid(x, y)\n\n    # Create the covariance matrix for the bivariate Gaussian\n    theta = np.radians(angle)\n    R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n    D = np.diag([sigma_major**2, sigma_minor**2])\n    covariance_matrix = R @ D @ R.T\n    mean = np.array([center_x, center_y])\n\n    # Compute the bivariate Gaussian distribution\n    pos = np.dstack((X, Y))\n    inv_cov = np.linalg.inv(covariance_matrix)\n    norm_const = 1.0 / (np.power(2 * np.pi, 2) * np.sqrt(np.linalg.det(covariance_matrix)))\n    diff = pos - mean\n    exponent = -0.5 * np.einsum('...i,ij,...j-&gt;...', diff, inv_cov, diff)\n    return norm_const * np.exp(exponent)\n\n# a function to generate hypothetical images based on a set of\n# covariate values (e.g., age)\ndef generate_hypothetical_images(age, noise_level=1e-6):\n    # Define the parameters of the bivariate Gaussian based on the covariates\n    center_x = 0.5 + 0.1 * np.cos(age / 10) + np.random.normal(0, noise_level)\n    center_y = 0.5 + 0.1 * np.sin(age / 10) + np.random.normal(0, noise_level)\n    angle = age / 30 * 180 / np.pi  # Convert to degrees\n    sigma_major = 0.25 + 0.02 * np.sin(age / 20) + np.random.normal(0, noise_level)\n    sigma_minor = 0.15 - 0.02 * np.sin(age / 20) + np.random.normal(0, noise_level)\n\n    # Generate the bivariate Gaussian image\n    return generate_bivariate_gaussian_image(\n        center_x, center_y, angle, sigma_major, sigma_minor) + 0.6 * ((age-40)/20)**2\n</pre> def generate_bivariate_gaussian_image(center_x, center_y, angle, sigma_major, sigma_minor):     num_nodes = 50     x = np.linspace(0, 1, num_nodes)     y = np.linspace(0, 1, num_nodes)     X, Y = np.meshgrid(x, y)      # Create the covariance matrix for the bivariate Gaussian     theta = np.radians(angle)     R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])     D = np.diag([sigma_major**2, sigma_minor**2])     covariance_matrix = R @ D @ R.T     mean = np.array([center_x, center_y])      # Compute the bivariate Gaussian distribution     pos = np.dstack((X, Y))     inv_cov = np.linalg.inv(covariance_matrix)     norm_const = 1.0 / (np.power(2 * np.pi, 2) * np.sqrt(np.linalg.det(covariance_matrix)))     diff = pos - mean     exponent = -0.5 * np.einsum('...i,ij,...j-&gt;...', diff, inv_cov, diff)     return norm_const * np.exp(exponent)  # a function to generate hypothetical images based on a set of # covariate values (e.g., age) def generate_hypothetical_images(age, noise_level=1e-6):     # Define the parameters of the bivariate Gaussian based on the covariates     center_x = 0.5 + 0.1 * np.cos(age / 10) + np.random.normal(0, noise_level)     center_y = 0.5 + 0.1 * np.sin(age / 10) + np.random.normal(0, noise_level)     angle = age / 30 * 180 / np.pi  # Convert to degrees     sigma_major = 0.25 + 0.02 * np.sin(age / 20) + np.random.normal(0, noise_level)     sigma_minor = 0.15 - 0.02 * np.sin(age / 20) + np.random.normal(0, noise_level)      # Generate the bivariate Gaussian image     return generate_bivariate_gaussian_image(         center_x, center_y, angle, sigma_major, sigma_minor) + 0.6 * ((age-40)/20)**2   <p>Let's see what this hypothetical function relating age to the target variable (image) looks like. Below, we will generate a set of hypothetical images for a range of ages, and visualize how the images change as a function of age. This will give us an intuition about the hypothetical normative process that we are trying to model with our spectral normative model.</p> In\u00a0[28]: Copied! <pre>candidate_ages = np.linspace(10, 80, 8)\n\nhypothetical_images = np.array([\n    generate_hypothetical_images(age) for age in candidate_ages])\n\n# Visualize the hypothetical images\nfig, axes = plt.subplots(2, len(candidate_ages)//2, figsize=(10, 5))\nfor i, age in enumerate(candidate_ages):\n    row = i // (len(candidate_ages)//2)\n    col = i % (len(candidate_ages)//2)\n    axes[row, col].imshow(\n        hypothetical_images[i].reshape(50, 50),\n        cmap=\"gnuplot2\", vmin=0, vmax=3)\n    axes[row, col].set_title(f\"Age: {age:.1f}\")\n    axes[row, col].axis(\"off\")\n\n# Show the colorbar in a new axis on the right\ncbar_ax = fig.add_axes([1.02, 0.15, 0.02, 0.7])\nnorm = plt.Normalize(vmin=0, vmax=3)\nsm = plt.cm.ScalarMappable(cmap=\"gnuplot2\", norm=norm)\nsm.set_array([])\nfig.colorbar(sm, cax=cbar_ax, label=\"Intensity\")\nplt.tight_layout()\nplt.show()\n</pre> candidate_ages = np.linspace(10, 80, 8)  hypothetical_images = np.array([     generate_hypothetical_images(age) for age in candidate_ages])  # Visualize the hypothetical images fig, axes = plt.subplots(2, len(candidate_ages)//2, figsize=(10, 5)) for i, age in enumerate(candidate_ages):     row = i // (len(candidate_ages)//2)     col = i % (len(candidate_ages)//2)     axes[row, col].imshow(         hypothetical_images[i].reshape(50, 50),         cmap=\"gnuplot2\", vmin=0, vmax=3)     axes[row, col].set_title(f\"Age: {age:.1f}\")     axes[row, col].axis(\"off\")  # Show the colorbar in a new axis on the right cbar_ax = fig.add_axes([1.02, 0.15, 0.02, 0.7]) norm = plt.Normalize(vmin=0, vmax=3) sm = plt.cm.ScalarMappable(cmap=\"gnuplot2\", norm=norm) sm.set_array([]) fig.colorbar(sm, cax=cbar_ax, label=\"Intensity\") plt.tight_layout() plt.show() <pre>/tmp/ipykernel_1334986/407263316.py:23: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <p>As expected, the magnitude, position, angle, and roundedness of the image changes as a function of age, following the hypothetical normative process that we defined.</p> In\u00a0[29]: Copied! <pre># Fix the random seed for reproducibility\nnp.random.seed(42)\n\nsample_size = 3000\ndata = pd.DataFrame({\n    \"age\": np.random.uniform(10, 80, size=sample_size),\n    \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=sample_size),\n})\nsite_effects_prior = np.random.normal(0, 1, size=data[\"site\"].nunique())\nsite_effects = data[\"site\"].map(dict(zip(data[\"site\"].unique(), site_effects_prior)))\n\nage_dependent_images = np.array([\n    generate_hypothetical_images(age, noise_level=1e-3).flatten()\n    for i, age in enumerate(data[\"age\"])\n])\nphenotype_mean = age_dependent_images + 0.5 * np.array(site_effects)[:, np.newaxis]\nphenotype_std = 0.1 * phenotype_mean + np.array([\n    0.01 * age for age in data[\"age\"]\n])[:, np.newaxis]\n\nobserved_phenotypes = (\n    phenotype_mean +\n    np.random.normal(0, 1, size=sample_size)[:, np.newaxis] * phenotype_std)\n\nphenotype_mean.shape, phenotype_std.shape, observed_phenotypes.shape\n</pre> # Fix the random seed for reproducibility np.random.seed(42)  sample_size = 3000 data = pd.DataFrame({     \"age\": np.random.uniform(10, 80, size=sample_size),     \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=sample_size), }) site_effects_prior = np.random.normal(0, 1, size=data[\"site\"].nunique()) site_effects = data[\"site\"].map(dict(zip(data[\"site\"].unique(), site_effects_prior)))  age_dependent_images = np.array([     generate_hypothetical_images(age, noise_level=1e-3).flatten()     for i, age in enumerate(data[\"age\"]) ]) phenotype_mean = age_dependent_images + 0.5 * np.array(site_effects)[:, np.newaxis] phenotype_std = 0.1 * phenotype_mean + np.array([     0.01 * age for age in data[\"age\"] ])[:, np.newaxis]  observed_phenotypes = (     phenotype_mean +     np.random.normal(0, 1, size=sample_size)[:, np.newaxis] * phenotype_std)  phenotype_mean.shape, phenotype_std.shape, observed_phenotypes.shape  Out[29]: <pre>((3000, 2500), (3000, 2500), (3000, 2500))</pre> <p>Let's first look at the distribution of data at a single pixel across all ages:</p> In\u00a0[30]: Copied! <pre>plt.figure(figsize=(10, 6))\nfor site in sorted(data[\"site\"].unique()):\n    subset = data[data[\"site\"] == site]\n    pixel_index = 1000  # Example pixel index to visualize\n    plt.scatter(\n        subset[\"age\"], observed_phenotypes[subset.index, pixel_index],\n        alpha=0.5, label=f\"{site}\", s=7)\nplt.xlabel(\"Age\")\nplt.ylabel(f\"Phenotype at Pixel {pixel_index}\")\nplt.title(\"Phenotype vs Age by Site\")\nplt.legend(title=\"Site\")\nplt.show()\n</pre> plt.figure(figsize=(10, 6)) for site in sorted(data[\"site\"].unique()):     subset = data[data[\"site\"] == site]     pixel_index = 1000  # Example pixel index to visualize     plt.scatter(         subset[\"age\"], observed_phenotypes[subset.index, pixel_index],         alpha=0.5, label=f\"{site}\", s=7) plt.xlabel(\"Age\") plt.ylabel(f\"Phenotype at Pixel {pixel_index}\") plt.title(\"Phenotype vs Age by Site\") plt.legend(title=\"Site\") plt.show()  <p>Let's also look at the data from a few different samples across the age range to see how the images change with age and site (batch effect):</p> In\u00a0[33]: Copied! <pre>number_of_samples = 10\nsample_indices = np.random.choice(\n    observed_phenotypes.shape[0], size=number_of_samples, replace=False)\nfig, axes = plt.subplots(2, number_of_samples//2, figsize=(15, 6))\nfor i, idx in enumerate(sample_indices):\n    row = i // (number_of_samples//2)\n    col = i % (number_of_samples//2)\n    axes[row, col].imshow(\n        observed_phenotypes[idx].reshape(50, 50), cmap=\"gnuplot2\", vmin=0, vmax=3)\n    axes[row, col].set_title(\n        f\"Age: {data.loc[idx, 'age']:.1f}, Site: {data.loc[idx, 'site']}\")\n    axes[row, col].axis(\"off\")\nplt.tight_layout()\nplt.show()\n</pre> number_of_samples = 10 sample_indices = np.random.choice(     observed_phenotypes.shape[0], size=number_of_samples, replace=False) fig, axes = plt.subplots(2, number_of_samples//2, figsize=(15, 6)) for i, idx in enumerate(sample_indices):     row = i // (number_of_samples//2)     col = i % (number_of_samples//2)     axes[row, col].imshow(         observed_phenotypes[idx].reshape(50, 50), cmap=\"gnuplot2\", vmin=0, vmax=3)     axes[row, col].set_title(         f\"Age: {data.loc[idx, 'age']:.1f}, Site: {data.loc[idx, 'site']}\")     axes[row, col].axis(\"off\") plt.tight_layout() plt.show()  <p>Now that we have a better understanding of the data, we can proceed to fit a spectral normative model to this data, and evaluate how well the model captures the underlying normative process that we defined.</p> In\u00a0[67]: Copied! <pre># Fix the random seed for reproducibility\nnp.random.seed(43)\n\ntest_sample_size = 1000\ntest_data = pd.DataFrame({\n    \"age\": np.random.uniform(10, 80, size=test_sample_size),\n    \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=test_sample_size),\n})\nsite_effects = test_data[\"site\"].map(\n    dict(zip(data[\"site\"].unique(), site_effects_prior)))\n\ntest_age_dependent_images = np.array([\n    generate_hypothetical_images(age, noise_level=1e-3).flatten()\n    for i, age in enumerate(test_data[\"age\"])\n])\ntest_phenotype_mean = (\n    test_age_dependent_images + 0.5 * np.array(site_effects)[:, np.newaxis])\ntest_phenotype_std = 0.1 * test_phenotype_mean + np.array([\n    0.01 * age for age in test_data[\"age\"]\n])[:, np.newaxis]\n\nobserved_test_phenotypes = (\n    test_phenotype_mean +\n    np.random.normal(0, 1, size=test_sample_size)[:, np.newaxis] * test_phenotype_std)\n\ntest_phenotype_mean.shape, test_phenotype_std.shape, observed_test_phenotypes.shape\n</pre> # Fix the random seed for reproducibility np.random.seed(43)  test_sample_size = 1000 test_data = pd.DataFrame({     \"age\": np.random.uniform(10, 80, size=test_sample_size),     \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=test_sample_size), }) site_effects = test_data[\"site\"].map(     dict(zip(data[\"site\"].unique(), site_effects_prior)))  test_age_dependent_images = np.array([     generate_hypothetical_images(age, noise_level=1e-3).flatten()     for i, age in enumerate(test_data[\"age\"]) ]) test_phenotype_mean = (     test_age_dependent_images + 0.5 * np.array(site_effects)[:, np.newaxis]) test_phenotype_std = 0.1 * test_phenotype_mean + np.array([     0.01 * age for age in test_data[\"age\"] ])[:, np.newaxis]  observed_test_phenotypes = (     test_phenotype_mean +     np.random.normal(0, 1, size=test_sample_size)[:, np.newaxis] * test_phenotype_std)  test_phenotype_mean.shape, test_phenotype_std.shape, observed_test_phenotypes.shape Out[67]: <pre>((1000, 2500), (1000, 2500), (1000, 2500))</pre> In\u00a0[35]: Copied! <pre># Now we can define the spectral normative model\nspectral_normative_model = snm.SpectralNormativeModel.build_from_dataframe(\n    eigenmode_basis=eigenmode_basis,\n    # Hierarchical Bayesian Regression (use a hierarchical prior on batch effects)\n    model_type=\"HBR\",\n    # Pass the dataframe containing all covariates\n    covariates_dataframe=data,\n    # Specify the numerical covariates to include in the model\n    numerical_covariates=[\"age\"],\n    # Specify which numerical covariates are modeled as nonlinear effects (B-splines)\n    nonlinear_covariates=[\"age\"],\n    # Specify the categorical covariates included in the model (including batch effects)\n    categorical_covariates=[\"site\"],\n    # Specify whether any categorical covariates should be treated as batch effects\n    batch_covariates=[\"site\"],\n    # Specify which covariates influence the mean of the target variable\n    influencing_mean=[\"age\", \"site\"],\n    # Specify which covariates influence the variance of the target variable\n    influencing_variance=[\"age\", \"site\"],\n)\n\nspectral_normative_model\n</pre> # Now we can define the spectral normative model spectral_normative_model = snm.SpectralNormativeModel.build_from_dataframe(     eigenmode_basis=eigenmode_basis,     # Hierarchical Bayesian Regression (use a hierarchical prior on batch effects)     model_type=\"HBR\",     # Pass the dataframe containing all covariates     covariates_dataframe=data,     # Specify the numerical covariates to include in the model     numerical_covariates=[\"age\"],     # Specify which numerical covariates are modeled as nonlinear effects (B-splines)     nonlinear_covariates=[\"age\"],     # Specify the categorical covariates included in the model (including batch effects)     categorical_covariates=[\"site\"],     # Specify whether any categorical covariates should be treated as batch effects     batch_covariates=[\"site\"],     # Specify which covariates influence the mean of the target variable     influencing_mean=[\"age\", \"site\"],     # Specify which covariates influence the variance of the target variable     influencing_variance=[\"age\", \"site\"], )  spectral_normative_model Out[35]: <pre>SpectralNormativeModel(eigenmode_basis=EigenmodeBasis(n_modes=100, n_features=2500), base_model=DirectNormativeModel(spec=NormativeModelSpec(variable_of_interest='dummy_VOI', covariates=[CovariateSpec(name=site, cov_type=categorical, hierarchical=True, n_categories=6), CovariateSpec(name=age, cov_type=numerical, effect=spline)], influencing_mean=['age', 'site'], influencing_variance=['age', 'site'])))</pre> <p>The example above is one of the simplest possible specifications of a spectral normative model. In later tutorials, we will explore more detailed specification approaches by which you can control parameters such as the basis expansion of the covariates, the prior distributions of the model parameters, and the model inference approach. For now, let's keep it simple and see how this specified spectral model performs on the data.</p> In\u00a0[68]: Copied! <pre>spectral_coefficients_train = np.array([\n    observed_phenotypes[i] @ eigenmode_basis.eigenvectors\n    for i in range(observed_phenotypes.shape[0])\n])\n\n# Let's similarly project the test phenotypes into the spectral space\nspectral_coefficients_test = np.array([\n    observed_test_phenotypes[i] @ eigenmode_basis.eigenvectors\n    for i in range(observed_test_phenotypes.shape[0])\n])\n\nspectral_coefficients_train.shape\n</pre> spectral_coefficients_train = np.array([     observed_phenotypes[i] @ eigenmode_basis.eigenvectors     for i in range(observed_phenotypes.shape[0]) ])  # Let's similarly project the test phenotypes into the spectral space spectral_coefficients_test = np.array([     observed_test_phenotypes[i] @ eigenmode_basis.eigenvectors     for i in range(observed_test_phenotypes.shape[0]) ])  spectral_coefficients_train.shape Out[68]: <pre>(3000, 100)</pre> In\u00a0[53]: Copied! <pre>spectral_normative_model.fit(\n    spectral_coeff_train_data=spectral_coefficients_train,\n    covariates_dataframe=data,\n)\n</pre> spectral_normative_model.fit(     spectral_coeff_train_data=spectral_coefficients_train,     covariates_dataframe=data, )  <pre>2026-02-17 21:19:29 : [INFO] - spectranorm.snm - Starting SNM model fitting:\n2026-02-17 21:19:29 : [INFO] - spectranorm.snm - Step 1; direct models for each eigenmode (100 modes)\n</pre> <pre>Fitting direct models:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>2026-02-17 21:19:53 : [INFO] - spectranorm.snm - Step 2; identify sparse covariance structure\n2026-02-17 21:19:54 : [INFO] - spectranorm.snm - Step 3; cross-eigenmode dependency modeling (49 pairs)\n</pre> <pre>Fitting covariance models:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> In\u00a0[54]: Copied! <pre># let's define 10 queries for the first 10 modes\nn_queries = 100\nencoded_queries = np.zeros((spectral_normative_model.model_params[\"n_modes\"], n_queries))\nnp.fill_diagonal(encoded_queries, 1)\n\n# generate predicted charts\nages = np.linspace(10, 80, 200)\n\n# create dummy dataframe for prediction\ndummy_df = pd.DataFrame({\n    \"age\": ages,\n})\n\n# predict moments\npredicted_spectral_moments = spectral_normative_model.predict(\n    encoded_query=encoded_queries[:n_queries],\n    test_covariates=dummy_df, predict_without=['site'],\n    n_modes=n_queries, # to simplify the inference\n).predictions\n</pre> # let's define 10 queries for the first 10 modes n_queries = 100 encoded_queries = np.zeros((spectral_normative_model.model_params[\"n_modes\"], n_queries)) np.fill_diagonal(encoded_queries, 1)  # generate predicted charts ages = np.linspace(10, 80, 200)  # create dummy dataframe for prediction dummy_df = pd.DataFrame({     \"age\": ages, })  # predict moments predicted_spectral_moments = spectral_normative_model.predict(     encoded_query=encoded_queries[:n_queries],     test_covariates=dummy_df, predict_without=['site'],     n_modes=n_queries, # to simplify the inference ).predictions   <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> In\u00a0[55]: Copied! <pre># Let's visualize the first mode's normative ranges as a function of age\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Predict moments (mean and std) of the normative distribution\nmoments = predicted_spectral_moments\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        ages,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    ages,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype on Mode 1\")\nplt.title(\"Phenotype vs Age\");\n</pre> # Let's visualize the first mode's normative ranges as a function of age fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # Predict moments (mean and std) of the normative distribution moments = predicted_spectral_moments  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         ages,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     ages,     phenotype_quantiles[0.5],     c=c2,     lw=4, ) plt.xlabel(\"Age\") plt.ylabel(\"Phenotype on Mode 1\") plt.title(\"Phenotype vs Age\");   <p>We can also plot the scatters of the observed data encoded by this mode to verify if this is consistent with the data distribution in the spectral space.</p> <p>Note: as seen in the visualization below, the observed data approximately follows the estimated normative ranges, which is expected given that the data was generated from a process that follows the normative model specification we defined above. However, given that the visualization bellow is showing scatter points of the observed data (without harmonization), this visualization can become misleading, especially if there are strong batch effects in the data. In the next section, we will see how to harmonize the data in the spectral space to get a more accurate visualization of the observed data distribution (post-harmonization) in the spectral space.</p> In\u00a0[56]: Copied! <pre># Let's visualize the first mode's normative ranges as a function of age\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Predict moments (mean and std) of the normative distribution\nmoments = predicted_spectral_moments\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        ages,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    ages,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\n\n# In addition to the part above, the following add the encoded phenotype values\n# as scatter points along the estimated normative range:\nfor site in sorted(data[\"site\"].unique()):\n    subset = data[data[\"site\"] == site]\n    plt.scatter(\n        subset[\"age\"], spectral_coefficients_train[subset.index][:,0],\n        alpha=0.5, label=f\"{site}\", s=7)\n\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype on Mode 1\")\nplt.title(\"Phenotype vs Age\");\n</pre> # Let's visualize the first mode's normative ranges as a function of age fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # Predict moments (mean and std) of the normative distribution moments = predicted_spectral_moments  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         ages,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     ages,     phenotype_quantiles[0.5],     c=c2,     lw=4, )  # In addition to the part above, the following add the encoded phenotype values # as scatter points along the estimated normative range: for site in sorted(data[\"site\"].unique()):     subset = data[data[\"site\"] == site]     plt.scatter(         subset[\"age\"], spectral_coefficients_train[subset.index][:,0],         alpha=0.5, label=f\"{site}\", s=7)  plt.xlabel(\"Age\") plt.ylabel(\"Phenotype on Mode 1\") plt.title(\"Phenotype vs Age\");   In\u00a0[57]: Copied! <pre>harmonized_spectral_coefficients_train = spectral_normative_model.harmonize(\n    encoded_query=encoded_queries[:n_queries],\n    covariates_dataframe=data,\n    spectral_coeff_data=spectral_coefficients_train[:,:n_queries],\n    covariates_to_harmonize=[\"site\"],\n    n_modes=n_queries, # to simplify the inference\n)\n</pre> harmonized_spectral_coefficients_train = spectral_normative_model.harmonize(     encoded_query=encoded_queries[:n_queries],     covariates_dataframe=data,     spectral_coeff_data=spectral_coefficients_train[:,:n_queries],     covariates_to_harmonize=[\"site\"],     n_modes=n_queries, # to simplify the inference ) <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> In\u00a0[58]: Copied! <pre># Let's visualize the first mode's normative ranges as a function of age\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Predict moments (mean and std) of the normative distribution\nmoments = predicted_spectral_moments\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        ages,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    ages,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\n\n# In addition to the part above, the following add the encoded phenotype values\n# as scatter points along the estimated normative range:\nfor site in sorted(data[\"site\"].unique()):\n    subset = data[data[\"site\"] == site]\n    plt.scatter(\n        subset[\"age\"], harmonized_spectral_coefficients_train[subset.index][:,0],\n        alpha=0.5, label=f\"{site}\", s=7)\n\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype on Mode 1\")\nplt.title(\"Phenotype vs Age\");\n</pre> # Let's visualize the first mode's normative ranges as a function of age fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # Predict moments (mean and std) of the normative distribution moments = predicted_spectral_moments  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         ages,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     ages,     phenotype_quantiles[0.5],     c=c2,     lw=4, )  # In addition to the part above, the following add the encoded phenotype values # as scatter points along the estimated normative range: for site in sorted(data[\"site\"].unique()):     subset = data[data[\"site\"] == site]     plt.scatter(         subset[\"age\"], harmonized_spectral_coefficients_train[subset.index][:,0],         alpha=0.5, label=f\"{site}\", s=7)  plt.xlabel(\"Age\") plt.ylabel(\"Phenotype on Mode 1\") plt.title(\"Phenotype vs Age\");   <p>As seen above, after harmonization, the scatter points of the observed data in the spectral space are now more accurately aligned with the estimated normative ranges, which is expected given that the batch effect has been removed from the data. This highlights the importance of harmonizing the data before visualizing it in the spectral space, especially when there are strong batch effects present in the data.</p> In\u00a0[59]: Copied! <pre>from sklearn.preprocessing import normalize\n# queries encoding the high-dimensional data with a gaussian kernel smoother\ngaussian_sigma = 0.05  # Adjust the sigma for the Gaussian kernel\npixel_queries = []\nfor i in range(2500):\n    query = np.exp(\n        -(\n            (X - x[i % num_nodes])**2 +\n            (Y - y[i // num_nodes])**2\n        ) / (2 * gaussian_sigma**2)).flatten()\n    pixel_queries.append(query / np.sum(query))  # Normalize the query to sum to 1\npixel_queries = np.array(pixel_queries)\npixel_queries_encoded = eigenmode_basis.encode(pixel_queries).T\n\n# create dummy dataframe for prediction\ncandidate_dummy_df = pd.DataFrame({\n    \"age\": candidate_ages,\n})\n\n# produce high-dimensional normative estimates\ncandidate_predictions = spectral_normative_model.predict(\n    encoded_query=pixel_queries_encoded,\n    test_covariates=candidate_dummy_df, predict_without=['site'],\n).predictions\n\n# Visualize the hypothetical images\nfig, axes = plt.subplots(len(candidate_ages), 2, figsize=(5, 20))\nfor i, age in enumerate(candidate_ages):\n    axes[i, 0].imshow(\n        candidate_predictions[\"mu_estimate\"][i].reshape(50, 50),\n        cmap=\"gnuplot2\", vmin=0, vmax=3.5)\n    axes[i, 0].set_title(f\"Age: {age:.1f}\")\n    axes[i, 0].axis(\"off\")\n    axes[i, 1].imshow(\n        eigenmode_basis.decode(\n            eigenmode_basis.encode(\n                hypothetical_images[i].flatten() @ pixel_queries.T,\n            ),\n        ).reshape(50, 50),\n        cmap=\"gnuplot2\", vmin=0, vmax=3.5)\n    axes[i, 1].set_title(f\"Age: {age:.1f}\")\n    axes[i, 1].axis(\"off\")\n\nplt.suptitle(\n    \"Predicted (left) vs Ground Truth (right) Images\",\n    fontsize=16, y=0.99)\n\n# Show the colorbar in a new axis on the right\ncbar_ax = fig.add_axes([1.02, 0.15, 0.02, 0.7])\nnorm = plt.Normalize(vmin=0, vmax=3)\nsm = plt.cm.ScalarMappable(cmap=\"gnuplot2\", norm=norm)\nsm.set_array([])\nfig.colorbar(sm, cax=cbar_ax, label=\"Intensity\")\nplt.tight_layout()\nplt.show()\n</pre> from sklearn.preprocessing import normalize # queries encoding the high-dimensional data with a gaussian kernel smoother gaussian_sigma = 0.05  # Adjust the sigma for the Gaussian kernel pixel_queries = [] for i in range(2500):     query = np.exp(         -(             (X - x[i % num_nodes])**2 +             (Y - y[i // num_nodes])**2         ) / (2 * gaussian_sigma**2)).flatten()     pixel_queries.append(query / np.sum(query))  # Normalize the query to sum to 1 pixel_queries = np.array(pixel_queries) pixel_queries_encoded = eigenmode_basis.encode(pixel_queries).T  # create dummy dataframe for prediction candidate_dummy_df = pd.DataFrame({     \"age\": candidate_ages, })  # produce high-dimensional normative estimates candidate_predictions = spectral_normative_model.predict(     encoded_query=pixel_queries_encoded,     test_covariates=candidate_dummy_df, predict_without=['site'], ).predictions  # Visualize the hypothetical images fig, axes = plt.subplots(len(candidate_ages), 2, figsize=(5, 20)) for i, age in enumerate(candidate_ages):     axes[i, 0].imshow(         candidate_predictions[\"mu_estimate\"][i].reshape(50, 50),         cmap=\"gnuplot2\", vmin=0, vmax=3.5)     axes[i, 0].set_title(f\"Age: {age:.1f}\")     axes[i, 0].axis(\"off\")     axes[i, 1].imshow(         eigenmode_basis.decode(             eigenmode_basis.encode(                 hypothetical_images[i].flatten() @ pixel_queries.T,             ),         ).reshape(50, 50),         cmap=\"gnuplot2\", vmin=0, vmax=3.5)     axes[i, 1].set_title(f\"Age: {age:.1f}\")     axes[i, 1].axis(\"off\")  plt.suptitle(     \"Predicted (left) vs Ground Truth (right) Images\",     fontsize=16, y=0.99)  # Show the colorbar in a new axis on the right cbar_ax = fig.add_axes([1.02, 0.15, 0.02, 0.7]) norm = plt.Normalize(vmin=0, vmax=3) sm = plt.cm.ScalarMappable(cmap=\"gnuplot2\", norm=norm) sm.set_array([]) fig.colorbar(sm, cax=cbar_ax, label=\"Intensity\") plt.tight_layout() plt.show()  <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> <pre>/tmp/ipykernel_1334986/1772272670.py:54: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <p>On the left, we have visualized the predictions of the spectral normative model (i.e., the normative median) at different ages, while on the right we have visualized the ground truth images at those ages (after encoding by the low-pass graph filter). This can show us how well the model captures the underlying normative process as captured by the low-pass graph filter (i.e., the eigenmode basis).</p> <p>Notably, the model captures the changes in magnitude, direction and spatial extent of the images as characterized by our hypothetical process.</p> <p>Homework: you can try refitting a similar spectral normative model, but with inclusion of more modes (e.g., 200--500 modes instead of 100 modes), and see how this affects the model fit in the original data space, note how the number of modes affects the smoothness and spatial specificity of the fitted model.</p> In\u00a0[65]: Copied! <pre># training performance\ntraining_performance = spectral_normative_model.evaluate(\n    encoded_query=pixel_queries_encoded,\n    spectral_coeff_test_data=spectral_coefficients_train,\n    test_covariates=data,\n)\n\n# let's visualize MSLL across all pixels queried\nplt.figure(figsize=(10, 6))\nplt.hist(\n    training_performance.evaluations[\"MSLL\"],\n    bins=50, color=\"skyblue\", edgecolor=\"black\")\nplt.xlabel(\"MSLL\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of MSLL across Queried Pixels\")\nplt.axvline(0, color=\"red\", linestyle=\"--\", label=\"MSLL = 0\")\nplt.legend()\nplt.show()\n</pre> # training performance training_performance = spectral_normative_model.evaluate(     encoded_query=pixel_queries_encoded,     spectral_coeff_test_data=spectral_coefficients_train,     test_covariates=data, )  # let's visualize MSLL across all pixels queried plt.figure(figsize=(10, 6)) plt.hist(     training_performance.evaluations[\"MSLL\"],     bins=50, color=\"skyblue\", edgecolor=\"black\") plt.xlabel(\"MSLL\") plt.ylabel(\"Frequency\") plt.title(\"Histogram of MSLL across Queried Pixels\") plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"MSLL = 0\") plt.legend() plt.show()  <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> <pre>2026-02-17 21:39:01 : [WARNING] - spectranorm.snm - Query moments not provided. Using test data moments as an approximation, which may lead to overestimating MSLL.\n</pre> <p>Rest assured, we see that across all pixels, the MSLL values are negative, which indicates that the model is performing better than a trivial model with no normative assumption (i.e., a model that assumes that the data distribution is a gaussian distribution independent of the covariates). This is expected given that the data was generated from a process that follows a normative structure (i.e., data was generated from a distribution derived from the age covariate).</p> <p>Notably, when generalizing to new unseen test data, it's recommended to provide the evaluate method with the <code>query_train_moments</code> argument, which contains the normative moments (mean and standard deviation) of the training data as estimated by the model for the provided spatial queries. This is mainly to get an accurate estimate of the MSLL metric, which is based on the comparison of the observed data distribution (a trivial gaussian with no normative assumption) to the normative distribution as estimated by the model. If this parameter is not provided, then the reported MSLL is incorrect, and can underestimate model performance (in the absence of training moments, these parameters are estimated from the provided test data).</p> <p>The script below evaluates the model fit on the test data, and provides the training moments to get an accurate estimate of the MSLL metric:</p> In\u00a0[69]: Copied! <pre># training performance\ntest_performance = spectral_normative_model.evaluate(\n    encoded_query=pixel_queries_encoded,\n    spectral_coeff_test_data=spectral_coefficients_test,\n    test_covariates=test_data,\n    query_train_moments=np.array([\n        np.mean(spectral_coefficients_train @ pixel_queries_encoded, axis=0),\n        np.std(spectral_coefficients_train @ pixel_queries_encoded, axis=0, ddof=1),\n    ]),\n)\n\n# let's visualize MSLL across all pixels queried\nplt.figure(figsize=(10, 6))\nplt.hist(\n    test_performance.evaluations[\"MSLL\"],\n    bins=50, color=\"skyblue\", edgecolor=\"black\")\nplt.xlabel(\"MSLL\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of MSLL across Queried Pixels\")\nplt.axvline(0, color=\"red\", linestyle=\"--\", label=\"MSLL = 0\")\nplt.legend()\nplt.show()\n</pre> # training performance test_performance = spectral_normative_model.evaluate(     encoded_query=pixel_queries_encoded,     spectral_coeff_test_data=spectral_coefficients_test,     test_covariates=test_data,     query_train_moments=np.array([         np.mean(spectral_coefficients_train @ pixel_queries_encoded, axis=0),         np.std(spectral_coefficients_train @ pixel_queries_encoded, axis=0, ddof=1),     ]), )  # let's visualize MSLL across all pixels queried plt.figure(figsize=(10, 6)) plt.hist(     test_performance.evaluations[\"MSLL\"],     bins=50, color=\"skyblue\", edgecolor=\"black\") plt.xlabel(\"MSLL\") plt.ylabel(\"Frequency\") plt.title(\"Histogram of MSLL across Queried Pixels\") plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"MSLL = 0\") plt.legend() plt.show()  <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> <p>These test evaluations, indicate that the model is generalizing well to unseen data, as the MSLL values are negative across all pixels, which indicates that the model is performing better than a trivial model with no normative assumption on the test data as well.</p> <p>Similar to the visualization with the training data, harmonization can be applied to the test data. The script below applies harmonization to the test data, and visualizes the observed data distribution along with the model fit for the spatial query corresponding to the first eigenmode coefficient:</p> In\u00a0[70]: Copied! <pre># harmonizing the test data\nharmonized_spectral_coefficients_test = spectral_normative_model.harmonize(\n    encoded_query=encoded_queries[:n_queries],\n    covariates_dataframe=test_data,\n    spectral_coeff_data=spectral_coefficients_test[:,:n_queries],\n    covariates_to_harmonize=[\"site\"],\n)\n</pre> # harmonizing the test data harmonized_spectral_coefficients_test = spectral_normative_model.harmonize(     encoded_query=encoded_queries[:n_queries],     covariates_dataframe=test_data,     spectral_coeff_data=spectral_coefficients_test[:,:n_queries],     covariates_to_harmonize=[\"site\"], ) <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing direct eigenmode estimates:   0%|          | 0/100 [00:00&lt;?, ?tasks/s]</pre> <pre>Computing cross-mode dependence estimates:   0%|          | 0/49 [00:00&lt;?, ?tasks/s]</pre> In\u00a0[71]: Copied! <pre># Let's visualize the first mode's normative ranges as a function of age\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Predict moments (mean and std) of the normative distribution\nmoments = predicted_spectral_moments\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        ages,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    ages,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\n\n# In addition to the part above, the following add the encoded phenotype values\n# as scatter points along the estimated normative range:\n# the harmonized test data values are being used here\nfor site in sorted(test_data[\"site\"].unique()):\n    subset = test_data[test_data[\"site\"] == site]\n    plt.scatter(\n        subset[\"age\"], harmonized_spectral_coefficients_test[subset.index][:,0],\n        alpha=0.5, label=f\"{site}\", s=7)\n\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype on Mode 1\")\nplt.title(\"Phenotype vs Age\");\n</pre> # Let's visualize the first mode's normative ranges as a function of age fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # Predict moments (mean and std) of the normative distribution moments = predicted_spectral_moments  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"][:,0] + stats.norm.ppf(q=q) * moments[\"std_estimate\"][:,0])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         ages,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(ages, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(ages, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     ages,     phenotype_quantiles[0.5],     c=c2,     lw=4, )  # In addition to the part above, the following add the encoded phenotype values # as scatter points along the estimated normative range: # the harmonized test data values are being used here for site in sorted(test_data[\"site\"].unique()):     subset = test_data[test_data[\"site\"] == site]     plt.scatter(         subset[\"age\"], harmonized_spectral_coefficients_test[subset.index][:,0],         alpha=0.5, label=f\"{site}\", s=7)  plt.xlabel(\"Age\") plt.ylabel(\"Phenotype on Mode 1\") plt.title(\"Phenotype vs Age\");   In\u00a0[72]: Copied! <pre>from pathlib import Path\n\nmodel_save_path = Path(\"./tmp_directory\")\n# ensure the directory exists\nmodel_save_path.mkdir(exist_ok=True)\n# ensure the directory is empty (no files or directories inside)\nif any(model_save_path.iterdir()):\n    msg = (\n        f\"The directory {model_save_path} is not empty. \"\n        f\"Please provide an empty directory for saving the model.\"\n    )\n    raise ValueError(msg)\n\nspectral_normative_model.save_model(model_save_path)\n</pre> from pathlib import Path  model_save_path = Path(\"./tmp_directory\") # ensure the directory exists model_save_path.mkdir(exist_ok=True) # ensure the directory is empty (no files or directories inside) if any(model_save_path.iterdir()):     msg = (         f\"The directory {model_save_path} is not empty. \"         f\"Please provide an empty directory for saving the model.\"     )     raise ValueError(msg)  spectral_normative_model.save_model(model_save_path)  In\u00a0[73]: Copied! <pre>loaded_spectral_normative_model = snm.SpectralNormativeModel.load_model(model_save_path)\n</pre> loaded_spectral_normative_model = snm.SpectralNormativeModel.load_model(model_save_path)"},{"location":"tutorials/spectral_model/#spectral-model","title":"Spectral Model\u00b6","text":"<p>This tutorial demonstrates how spectral normative modeling can be performed using the SpectraNorm package.</p> <p>We will use the same hypothetical 2D lattice graph data and the eigenmode basis computed in the previous tutorial on constructing graph spectral eigenbases). Here, we will first describe how a hypothetical target variable (e.g., a hypothetical phenotype confined to the 2D lattice graph) could possibly be generated through a normative process, such that the value of the target variable at any given node on the lattice follows a normative distribution that depends on the value of a continous covariate (e.g., age).</p> <p>This hypothetical case, will in a way combine the concepts of normative modeling (described for a univariate case in an earlier tutorial) with spectral representations of data defined on graphs (described in the previous tutorial). We will then fit a spectral normative model to this hypothetical data, then visualize and evaluate the results.</p>"},{"location":"tutorials/spectral_model/#package-imports","title":"Package Imports\u00b6","text":"<p>The following notebook cell imports the necessary packages and modules for this tutorial:</p>"},{"location":"tutorials/spectral_model/#2d-lattice-graph","title":"2D Lattice Graph\u00b6","text":"<p>Similar to the previous tutorial, we will generate a 2D lattice graph with 50x50 nodes to represent our hypothetical high-dimensional feature space.</p>"},{"location":"tutorials/spectral_model/#loading-the-eigenmode-basis","title":"Loading the Eigenmode Basis\u00b6","text":"<p>We will load the eigenmode basis that was computed and saved in the previous tutorial:</p>"},{"location":"tutorials/spectral_model/#hypothetical-target-variable","title":"Hypothetical Target Variable\u00b6","text":"<p>Given that we have a 2D lattice graph and its corresponding eigenmode basis, we can now generate a hypothetical target variable that follows a normative process defined over this 2D lattice. The 2D lattice can be used to describe a hypothetical image. For simplicity, we assume that this image (a 50x50 pixel image) describes a bivariate gaussian distribution of pixel intensities.</p> <p>For this let's define a function that can generate such images from a minimal set of parameters (e.g., center coordinates of the bivariate gaussian, it's angle of orientation, and its standard deviation along the major and minor axes). Next, we will use this function to generate a set of hypothetical images (training data) that describes the normative dependency of the target variable (image) on a continuous covariate (e.g., age).</p>"},{"location":"tutorials/spectral_model/#generating-training-data","title":"Generating Training Data\u00b6","text":"<p>Now that we have defined the hypothetical normative process that relates a covariate (age) to the target variable (image intensities) characteristics, we can generate a set of training data by sampling from this process at different ages. We will assume that the normative mean is described by the hypothetical function we defined above, and that the variance is proportional to the mean while linearly increasing with age. Finally, we will also add a random batch effect (e.g., akin to a scanner effect) that introduces a uniform shift in the observed image intensities across all pixels.</p> <p>Notably, the function to generate the hypothetical images includes a noise term that adds some variability to the generated images, which is similar to the variability we would expect to see in real-world data (e.g., due to individual differences, measurement noise, etc.).</p> <p>In a sense, this setup is equivalent to a normative modeling setup where we have a separate univariate model for each pixel (the value of a single pixel follows a predefined distribution that depends on the covariate values). However, given that we have formulated the normative process to be defined in the image space, we expect a multivariate spectral normative model to be able to capture this process indirectly.</p> <p>All that said, let's generate the training data and generate visualizations to better understand the data that we are working with.</p>"},{"location":"tutorials/spectral_model/#generating-test-data","title":"Generating Test Data\u00b6","text":"<p>Noting the we will also likely need some test data to evaluate the model, we can generate a separate test set using the same process as above, but with different random seeds to ensure that the test data is different from the training data.</p>"},{"location":"tutorials/spectral_model/#defining-the-spectral-model","title":"Defining the Spectral Model\u00b6","text":"<p>To define the spectral normative model, we will need to specify two main components: the spectral basis functions (which we have already computed and loaded, i.e., the eigenmode basis), and the normative model specification (i.e., how the normative mean and variance depend on the covariates). For the normative model specification, we can use a base univariate normative model (defined by <code>snm.DirectNormativeModel</code>). Here, we will specify a similar model to what was defined in the previous univariate normative modeling tutorial.</p> <p>The Spectral Normative Model will be defined using the <code>snm.SpectralNormativeModel</code> class, which can be initialized with the eigenmode basis and the base univariate normative model instances. Here, we will use the <code>SpectralNormativeModel.build_from_dataframe</code> method, which automates the process of specifying the base model, directly from a dataframe containing the covariates (much similar to the univariate normative modeling tutorial).</p>"},{"location":"tutorials/spectral_model/#training-the-spectral-model","title":"Training the spectral model\u00b6","text":"<p>In order to train the spectral normative model, we first need to compute the spectral coefficients summarizing the high-dimensional image data (across all pixels) into the spectral space defined by the eigenmode basis.</p>"},{"location":"tutorials/spectral_model/#computing-spectral-coefficients","title":"Computing Spectral Coefficients\u00b6","text":"<p>The script below computes the spectral coefficients for the training data by projecting the high-dimensional image data onto the eigenmode basis:</p>"},{"location":"tutorials/spectral_model/#fitting-the-model","title":"Fitting the model\u00b6","text":"<p>The script below fits the spectral normative model to the training data by calling the <code>SpectralNormativeModel.fit</code> method, which estimates the parameters of the model based on the provided data and covariates.</p> <p>Note: the fitting process can be parallelized across multiple CPU cores by setting the <code>n_jobs</code> parameter of the <code>fit</code> method to a value greater than 1 (e.g., <code>n_jobs=-1</code> to use all available cores, which is the default setting).</p>"},{"location":"tutorials/spectral_model/#model-fit-in-spectral-space","title":"Model fit in spectral space\u00b6","text":"<p>Now that the spectral normative model has been fitted, we have a multivariate normative model explaining how the spectral coefficients (which are the projections of the original high-dimensional data onto the eigenmode basis) depend on the covariates.</p> <p>E.g. the following visualization shows the normative ranges for the first eigenmode coefficient as a function of age:</p> <p>Note: these ranges were extracted by definging a spatial query in the spectral space, such that it only considers the first eigenmode coefficient (i.e., the first element is one, and the rest are zeros). By using other spatial queries, we can extract normative ranges for any hypothetical linear combination of the high-dimensional data (as long as it can be captured by the eigenmode basis).</p>"},{"location":"tutorials/spectral_model/#harmonizing-the-data-in-the-spectral-space","title":"Harmonizing the data in the spectral space\u00b6","text":"<p>As explained in the note earlier, it is important to first harmonize the data to remove the batch effect (e.g., site effect) from the observed data. The <code>SpectralNormativeModel.harmonize</code> implements such a functionality. Notably, the harmonize function expects a list of spatial queries to harmonize the observed data as described by those spatial queries. Since we are currently using the eigenmode basis as the spatial query (i.e. the first query is the loading on the first mode), the results of this harmonization procedure gives us harmonized counterparts to the spectral coeefficients.</p>"},{"location":"tutorials/spectral_model/#probing-model-fit-in-the-original-data-space","title":"Probing model fit in the original data space\u00b6","text":"<p>Having verified the accuracy of the model fit in the spectral space, we can now turn our attention to the original data space (i.e., the image space) to see if the model fit in the spectral space translates to an accurate fit in the original data space.</p> <p>Knowing that the eigenmode basis set captures more smooth patterns in the data (lower graph frequencies), we can use a gaussian kernel smoother to estimate the mean value centered at each pixel (by a small gaussian kernel). We will then use multiple such kernels (one centered at each pixel) as spatial queries to interrogate the model fit in the spectral space, and produce normative estimates of the expected median image (i.e., the normative mean) at different ages. Given that we are using synthetic data generated from a known normative process, we can compare these interrogated normative estimates against the known ground truth.</p>"},{"location":"tutorials/spectral_model/#quantifying-goodness-of-fit","title":"Quantifying goodness of fit\u00b6","text":"<p>Having trained a normative model, we can now quantify the goodness of fit of the model to both the training data and the test data. To do this, we can use the <code>SpectralNormativeModel.evaluate</code> method, which computes various evaluation metrics to quantify the goodness of fit of the model to any given data.</p> <p>You should note that the goodness of fit will also depend on the specific spatial query used to interrogate the model. For example, a simple model might give good fit metrics when interrogated with a spatial query that captures only the first few modes (e.g., very smooth spatial queries), but might give poor fit metrics when interrogated with a spatial query that captures more modes (e.g., more complex spatial queries). Therefore, it is important to consider the choice of spatial query when evaluating the goodness of fit of a spectral normative model.</p> <p>Here, we will use the smooth gaussian kernel queries (as described in the previous section) to evaluate the goodness of fit of the model in the original data space. This will give us an estimate of how well the model captures the underlying normative process in the original data space, which is ultimately one of the hardest tasks in spectral normative modeling.</p> <p>First, let's evaluate the model fit on the training data (we're showing the distribution of MSLL values across all pixels as an example):</p>"},{"location":"tutorials/spectral_model/#saving-the-model","title":"Saving the model\u00b6","text":"<p>To finish this tutorial on spectral normative modeling, it's worth mentioning that the fitted spectral normative model can be saved to disk using the <code>SpectralNormativeModel.save_model</code> method, which saves the model parameters and specifications to a directory created at the specific path. This allows you to load the model later for inference or further analysis without needing to refit the model from scratch.</p> <p>Notably, this saved instance will contain both the eigenmode basis and the normative model specifications. As such, when sharing pretrained spectral normative models, all you need to provide is the directory path to wich this file was saved. Note that we recommend saving the model in a new/empty directory. The save method may create multiple files in the provided directory. When sharing pretrained models, you can simply share the whole directory, or a compressed version of the directory (e.g., a zip file) to share the pretrained model.</p>"},{"location":"tutorials/spectral_model/#loading-the-model","title":"Loading the model\u00b6","text":"<p>Finally, now that the model is saved, you can load it later using the <code>SpectralNormativeModel.load_model</code> method, which loads the model parameters and specifications from the provided directory path. This allows you to use a shared pretrained model for inference or further analysis.</p>"},{"location":"tutorials/spectral_model/#concluding-remarks","title":"Concluding remarks\u00b6","text":"<p>This tutorial provided an introductory walkthrough of how to perform spectral normative modeling using the SpectraNorm package. We covered the process of defining a spectral normative model, fitting it to data, visualizing the model fit in both the spectral and original data spaces, evaluating the goodness of fit, and saving/loading the model for future use. This tutorial serves as a starting point for understanding the capabilities of spectral normative modeling, and how it can be applied to data defined on graphs.</p> <p>In future tutorials, we will explore how spectral normative models can be applied to brain imaging data specifically. This will provide us with the opportunity to cover specific neuroimaging-related utilities of the package, and to demonstrate how previously provided pretrained spectral normative models can be used for inference on new data.</p>"},{"location":"tutorials/univariate_model/","title":"Univariate Model","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Import the SpectraNorm package\nfrom spectranorm import snm\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy import stats  # Import the SpectraNorm package from spectranorm import snm  In\u00a0[2]: Copied! <pre># Fix the random seed for reproducibility\nnp.random.seed(42)\n\nsample_size = 5000\ndata = pd.DataFrame({\n    \"age\": np.random.uniform(20, 80, size=sample_size),\n    \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=sample_size),\n})\n\n# Generate a synthetic phenotype\n# Let's assume the phenotype is nonlinearly related to age,\n# and site effect comes from a normal distribution with mean depending\n# on the site and some random noise\nsite_effects_prior = np.random.normal(0, 5, size=data[\"site\"].nunique())\nsite_effects = data[\"site\"].map(dict(zip(data[\"site\"].unique(), site_effects_prior)))\nphenotype_mean = 0.01 * (\n    400 + 1.5 * data[\"age\"] -\n    0.01 * (data[\"age\"] - 50)**3 +\n    10 * site_effects)\n# Heteroscedasticity: variance increases with age\nphenotype_std = 0.01 * (10 + 0.8 * data[\"age\"])\ndata[\"phenotype\"] = np.random.normal(phenotype_mean, phenotype_std)\n</pre> # Fix the random seed for reproducibility np.random.seed(42)  sample_size = 5000 data = pd.DataFrame({     \"age\": np.random.uniform(20, 80, size=sample_size),     \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=sample_size), })  # Generate a synthetic phenotype # Let's assume the phenotype is nonlinearly related to age, # and site effect comes from a normal distribution with mean depending # on the site and some random noise site_effects_prior = np.random.normal(0, 5, size=data[\"site\"].nunique()) site_effects = data[\"site\"].map(dict(zip(data[\"site\"].unique(), site_effects_prior))) phenotype_mean = 0.01 * (     400 + 1.5 * data[\"age\"] -     0.01 * (data[\"age\"] - 50)**3 +     10 * site_effects) # Heteroscedasticity: variance increases with age phenotype_std = 0.01 * (10 + 0.8 * data[\"age\"]) data[\"phenotype\"] = np.random.normal(phenotype_mean, phenotype_std) In\u00a0[3]: Copied! <pre>data.head()\n</pre> data.head() Out[3]: age site phenotype 0 42.472407 C 4.638801 1 77.042858 C 4.186396 2 63.919637 F 3.992819 3 55.919509 D 5.027764 4 29.361118 C 5.115786 <p>The synthetic data above simulates a scenario where we have a phenotype that is influenced by age and site, with some added noise. The <code>site</code> variable is categorical, representing different data collection sites, while <code>age</code> is a continuous variable. The <code>phenotype</code> is the target variable we want to model.</p> <p>The visualization below illustrates the relationship between age and the phenotype across different sites, showing how the phenotype varies with age and site.</p> In\u00a0[4]: Copied! <pre>plt.figure(figsize=(10, 6))\nfor site in sorted(data[\"site\"].unique()):\n    subset = data[data[\"site\"] == site]\n    plt.scatter(subset[\"age\"], subset[\"phenotype\"], alpha=0.5, label=f\"{site}\", s=7)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype\")\nplt.title(\"Phenotype vs Age by Site\")\nplt.legend(title=\"Site\")\nplt.show()\n</pre> plt.figure(figsize=(10, 6)) for site in sorted(data[\"site\"].unique()):     subset = data[data[\"site\"] == site]     plt.scatter(subset[\"age\"], subset[\"phenotype\"], alpha=0.5, label=f\"{site}\", s=7) plt.xlabel(\"Age\") plt.ylabel(\"Phenotype\") plt.title(\"Phenotype vs Age by Site\") plt.legend(title=\"Site\") plt.show()  In\u00a0[5]: Copied! <pre># Define the normative model:\nnormative_model = snm.DirectNormativeModel.from_dataframe(\n    # Hierarchical Bayesian Regression (use a hierarchical prior on batch effects)\n    model_type=\"HBR\",\n    # Pass the dataframe containing all covariates and the target variable of interest\n    dataframe=data,\n    # Specify the name of the target variable (the variable we want to model)\n    variable_of_interest=\"phenotype\",\n    # Specify the numerical covariates to include in the model\n    numerical_covariates=[\"age\"],\n    # Specify which numerical covariates are modeled as nonlinear effects (B-splines)\n    nonlinear_covariates=[\"age\"],\n    # Specify the categorical covariates included in the model (including batch effects)\n    categorical_covariates=[\"site\"],\n    # Specify whether any categorical covariates should be treated as batch effects\n    batch_covariates=[\"site\"],\n    # Specify which covariates influence the mean of the target variable\n    influencing_mean=[\"age\", \"site\"],\n    # Specify which covariates influence the variance of the target variable\n    influencing_variance=[\"age\", \"site\"],\n)\n\nnormative_model\n</pre> # Define the normative model: normative_model = snm.DirectNormativeModel.from_dataframe(     # Hierarchical Bayesian Regression (use a hierarchical prior on batch effects)     model_type=\"HBR\",     # Pass the dataframe containing all covariates and the target variable of interest     dataframe=data,     # Specify the name of the target variable (the variable we want to model)     variable_of_interest=\"phenotype\",     # Specify the numerical covariates to include in the model     numerical_covariates=[\"age\"],     # Specify which numerical covariates are modeled as nonlinear effects (B-splines)     nonlinear_covariates=[\"age\"],     # Specify the categorical covariates included in the model (including batch effects)     categorical_covariates=[\"site\"],     # Specify whether any categorical covariates should be treated as batch effects     batch_covariates=[\"site\"],     # Specify which covariates influence the mean of the target variable     influencing_mean=[\"age\", \"site\"],     # Specify which covariates influence the variance of the target variable     influencing_variance=[\"age\", \"site\"], )  normative_model Out[5]: <pre>DirectNormativeModel(spec=NormativeModelSpec(variable_of_interest='phenotype', covariates=[CovariateSpec(name=site, cov_type=categorical, hierarchical=True, n_categories=6), CovariateSpec(name=age, cov_type=numerical, effect=spline)], influencing_mean=['age', 'site'], influencing_variance=['age', 'site']))</pre> <p>Notably, the minimal definition above, will fit the phenotype $y$ as a function of age and site, with the following model specification:</p> <p>$$ y \\sim \\mathcal{N}(\\mu, \\sigma) $$ $$ \\mu = f_{\\mu}(\\Phi(\\text{age}), \\text{site}) $$ $$ \\sigma = f^+_{\\sigma}(\\Phi(\\text{age}), \\text{site}) $$</p> <p>Where $\\Phi(\\text{age})$ represents the B-spline basis expansion of the age covariate, and $f_{\\mu}$ and $f^+_{\\sigma}$ are the functions that model the mean and standard deviation of the phenotype as a generalized additive model of the covariates.</p> In\u00a0[6]: Copied! <pre># Fit the normative model to the training data\nnormative_model.fit(train_data=data)\n</pre> # Fit the normative model to the training data normative_model.fit(train_data=data)  <pre>Output()</pre> <pre></pre> <pre>Finished [100%]: Average Loss = 3,115.5\n</pre> In\u00a0[7]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# 100 evenly spaced age values for prediction\nxi = np.linspace(20, 80, 100)\n\n# create dummy dataframe for prediction\ndummy_df = pd.DataFrame({\n    \"age\": xi,\n})\n\n# Predict moments (mean and std) of the normative distribution\nmoments = normative_model.predict(dummy_df, predict_without=['site']).predictions\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        xi,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    xi,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype\")\nplt.title(\"Phenotype vs Age\");\n</pre> fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # 100 evenly spaced age values for prediction xi = np.linspace(20, 80, 100)  # create dummy dataframe for prediction dummy_df = pd.DataFrame({     \"age\": xi, })  # Predict moments (mean and std) of the normative distribution moments = normative_model.predict(dummy_df, predict_without=['site']).predictions  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         xi,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     xi,     phenotype_quantiles[0.5],     c=c2,     lw=4, ) plt.xlabel(\"Age\") plt.ylabel(\"Phenotype\") plt.title(\"Phenotype vs Age\");   <p>The plot shows how the expected phenotype values change with age, along with the variability around the expected values. Visually, the fitted normative model looks reasonable, as it captures the general trend of the data.</p> <p>Notably, given that this is synthetic data, for which we know the ground truth distribution, we can also compare the estimated normative range to the true underlying distribution of the data. This allows us to directly assess how well the fitted model captures the true (synthetic) relationship between age and the phenotype.</p> <p>While this validation step is not possible with real data, it is a useful exercise to understand the capabilities and limitations of the normative modeling approach using synthetic data, where we have access to the ground truth distribution.</p> In\u00a0[8]: Copied! <pre># A figure with two subplots:\n# one showing the fitted normative model with shaded quantile ranges, and\n# another showing the true underlying distribution of the synthetic data for comparison\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\n\n# First axis: Fitted normative model with shaded quantile ranges\nax = axes[0]\n\n# 100 evenly spaced age values for prediction\nxi = np.linspace(20, 80, 100)\n\n# create dummy dataframe for prediction\ndummy_df = pd.DataFrame({\n    \"age\": xi,\n})\n\n# Predict moments (mean and std) of the normative distribution\nmoments = normative_model.predict(dummy_df, predict_without=['site']).predictions\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        xi,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    xi,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Phenotype\")\nax.set_title(\"Phenotype vs Age (Fitted Normative Model)\")\n\n# Second axis: True underlying distribution of the synthetic data\n# (after disregarding site effects)\nax = axes[1]\nsynthetic_mean = 0.01 * (\n    400 + 1.5 * xi -\n    0.01 * (xi - 50)**3)\nsynthetic_std = 0.01 * (10 + 0.8 * xi)\n\n# Compute true quantiles of the synthetic distribution\nsynthetic_quantiles = {\n    q: (synthetic_mean + stats.norm.ppf(q=q) * synthetic_std)\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#609c44\"  # brighter shade\nc2 = \"#0ea603\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        xi,\n        synthetic_quantiles[q1],\n        synthetic_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(xi, synthetic_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(xi, synthetic_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    xi,\n    synthetic_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Phenotype\")\nax.set_title(\"Phenotype vs Age (ground truth)\")\n\nplt.tight_layout()\n</pre> # A figure with two subplots: # one showing the fitted normative model with shaded quantile ranges, and # another showing the true underlying distribution of the synthetic data for comparison  fig, axes = plt.subplots(1, 2, figsize=(18, 8))  # First axis: Fitted normative model with shaded quantile ranges ax = axes[0]  # 100 evenly spaced age values for prediction xi = np.linspace(20, 80, 100)  # create dummy dataframe for prediction dummy_df = pd.DataFrame({     \"age\": xi, })  # Predict moments (mean and std) of the normative distribution moments = normative_model.predict(dummy_df, predict_without=['site']).predictions  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         xi,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     xi,     phenotype_quantiles[0.5],     c=c2,     lw=4, ) ax.set_xlabel(\"Age\") ax.set_ylabel(\"Phenotype\") ax.set_title(\"Phenotype vs Age (Fitted Normative Model)\")  # Second axis: True underlying distribution of the synthetic data # (after disregarding site effects) ax = axes[1] synthetic_mean = 0.01 * (     400 + 1.5 * xi -     0.01 * (xi - 50)**3) synthetic_std = 0.01 * (10 + 0.8 * xi)  # Compute true quantiles of the synthetic distribution synthetic_quantiles = {     q: (synthetic_mean + stats.norm.ppf(q=q) * synthetic_std)     for q in quantiles }  # Some colors for the plot c1 = \"#609c44\"  # brighter shade c2 = \"#0ea603\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         xi,         synthetic_quantiles[q1],         synthetic_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(xi, synthetic_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(xi, synthetic_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     xi,     synthetic_quantiles[0.5],     c=c2,     lw=4, ) ax.set_xlabel(\"Age\") ax.set_ylabel(\"Phenotype\") ax.set_title(\"Phenotype vs Age (ground truth)\")  plt.tight_layout() <p>As we can see from the plot, the estimated normative range accurately captures the true underlying distribution of the data, indicating that the fitted model is able to capture the relationship between age and the phenotype effectively.</p> In\u00a0[9]: Copied! <pre># First, we'll predict the normative ranges for the training data:\n# Note that the predict method will only require the covariates as input\ntrain_predictions = normative_model.predict(data[[\"age\", \"site\"]])\n\n# Now we can evaluate how well the model captures the variability in the target variable\ntrain_predictions.evaluate_predictions(\n    variable_of_interest=data[\"phenotype\"].to_numpy(),\n    train_mean=normative_model.model_params[\"mean_VOI\"],\n    train_std=normative_model.model_params[\"std_VOI\"],\n    n_params=normative_model.model_params[\"n_params\"],\n).evaluations\n</pre> # First, we'll predict the normative ranges for the training data: # Note that the predict method will only require the covariates as input train_predictions = normative_model.predict(data[[\"age\", \"site\"]])  # Now we can evaluate how well the model captures the variability in the target variable train_predictions.evaluate_predictions(     variable_of_interest=data[\"phenotype\"].to_numpy(),     train_mean=normative_model.model_params[\"mean_VOI\"],     train_std=normative_model.model_params[\"std_VOI\"],     n_params=normative_model.model_params[\"n_params\"], ).evaluations Out[9]: <pre>{'MAE': array(0.39788493),\n 'MSE': array(0.26895753),\n 'RMSE': array(0.51861116),\n 'MAPE': array(10.17445066),\n 'R-squared': array(0.77135881),\n 'Explained Variance': array(0.77135901),\n 'MSLL': array(-0.78767534)}</pre> In\u00a0[10]: Copied! <pre># Alternatively, we can simply use the model's built-in method to evaluate\n# goodness-of-fit with a one-liner (note that the target variable is passed to\n# the evaluate method):\nnormative_model.evaluate(data).evaluations\n</pre> # Alternatively, we can simply use the model's built-in method to evaluate # goodness-of-fit with a one-liner (note that the target variable is passed to # the evaluate method): normative_model.evaluate(data).evaluations  Out[10]: <pre>{'MAE': array(0.39788493),\n 'MSE': array(0.26895753),\n 'RMSE': array(0.51861116),\n 'MAPE': array(10.17445066),\n 'R-squared': array(0.77135881),\n 'Explained Variance': array(0.77135901),\n 'MSLL': array(-0.78767534)}</pre> In\u00a0[11]: Copied! <pre># Fix the random seed for reproducibility\nnp.random.seed(42)\n\ntest_sample_size = 5000\ntest_data = pd.DataFrame({\n    \"age\": np.random.uniform(20, 80, size=test_sample_size),\n    \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=test_sample_size),\n})\n\n# Generate the synthetic phenotype\n# We'll use the same underlying distribution as before\ntest_site_effects = test_data[\"site\"].map(\n    dict(zip(data[\"site\"].unique(), site_effects_prior)))\ntest_phenotype_mean = 0.01 * (\n    400 + 1.5 * test_data[\"age\"] -\n    0.01 * (test_data[\"age\"] - 50)**3\n    + 10 * test_site_effects)\ntest_phenotype_std = 0.01 * (10 + 0.8 * test_data[\"age\"])\ntest_data[\"phenotype\"] = np.random.normal(test_phenotype_mean, test_phenotype_std)\n</pre> # Fix the random seed for reproducibility np.random.seed(42)  test_sample_size = 5000 test_data = pd.DataFrame({     \"age\": np.random.uniform(20, 80, size=test_sample_size),     \"site\": np.random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"], size=test_sample_size), })  # Generate the synthetic phenotype # We'll use the same underlying distribution as before test_site_effects = test_data[\"site\"].map(     dict(zip(data[\"site\"].unique(), site_effects_prior))) test_phenotype_mean = 0.01 * (     400 + 1.5 * test_data[\"age\"] -     0.01 * (test_data[\"age\"] - 50)**3     + 10 * test_site_effects) test_phenotype_std = 0.01 * (10 + 0.8 * test_data[\"age\"]) test_data[\"phenotype\"] = np.random.normal(test_phenotype_mean, test_phenotype_std) In\u00a0[12]: Copied! <pre># We'll use the evaluate method for brevity here,\n# but we can also break it down to prediction + evaluation\n# similar to the training data if we want\ntest_predictions = normative_model.evaluate(test_data)\n\ntest_predictions.evaluations\n</pre> # We'll use the evaluate method for brevity here, # but we can also break it down to prediction + evaluation # similar to the training data if we want test_predictions = normative_model.evaluate(test_data)  test_predictions.evaluations  Out[12]: <pre>{'MAE': array(0.3962901),\n 'MSE': array(0.26426399),\n 'RMSE': array(0.51406614),\n 'MAPE': array(9.63360171),\n 'R-squared': array(0.77598759),\n 'Explained Variance': array(0.77599224),\n 'MSLL': array(-0.79278783)}</pre> <p>As it can be seen from the evaluation results, the model captures the general trend in the new data, as the performance metrics indicate that the model is able to explain a significant portion of the variance in the new data (77%), and the metrics are largely consistent with the in-sample evaluation, suggesting that the model generalizes well to new data with minimal overfitting.</p> In\u00a0[13]: Copied! <pre># Let's first compute the harmonized phenotype values by removing the estimated\n# site effects from the observed phenotype values\ntest_data[\"harmonized_phenotype\"] = normative_model.harmonize(\n    # The dataframe containing the covariates and the target variable\n    data=test_data,\n    # The list of covariates to harmonize (partial out)\n    covariates_to_harmonize=[\"site\"],\n)\n</pre> # Let's first compute the harmonized phenotype values by removing the estimated # site effects from the observed phenotype values test_data[\"harmonized_phenotype\"] = normative_model.harmonize(     # The dataframe containing the covariates and the target variable     data=test_data,     # The list of covariates to harmonize (partial out)     covariates_to_harmonize=[\"site\"], )  <p>Combining the harmonized phenotype values as scatter points with the estimated normative range allows us to visualize how the new data points relate to the normative model while accounting for site effects. This can provide insights into whether the new data points fall within the expected normative range after removing site-related variability, and can help identify any potential outliers or deviations from the normative pattern that may be of interest for further investigation.</p> In\u00a0[14]: Copied! <pre># The first part of the script is copied from the previous visualization section\n# this part visualizes the normative range of the phenotype values across age.\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# 100 evenly spaced age values for prediction\nxi = np.linspace(20, 80, 100)\n\n# create dummy dataframe for prediction\ndummy_df = pd.DataFrame({\n    \"age\": xi,\n})\n\n# Predict moments (mean and std) of the normative distribution\nmoments = normative_model.predict(dummy_df, predict_without=['site']).predictions\n\n# Let's compute charts along different quantiles\nquantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]\n\n# Predict the phenotype values corresponding to different quantiles\nphenotype_quantiles = {\n    q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])\n    for q in quantiles\n}\n\n# Some colors for the plot\nc1 = \"#448e9c\"  # brighter shade\nc2 = \"#038da6\"  # darker shade\n\n# Plotting shaded ranges for different quantiles\nfor q1, q2, alpha in [\n    (0.001, 0.999, 0.05),\n    (0.01, 0.99, 0.1),\n    (0.05, 0.95, 0.2),\n    (0.25, 0.75, 0.3)]:\n    ax.fill_between(\n        xi,\n        phenotype_quantiles[q1],\n        phenotype_quantiles[q2],\n        alpha=alpha,\n        color=c1,\n    )\n\n    # Now add boundary lines with different colors\n    ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")\n    ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")\n\n# Finally, add the median line with a different color and higher line width\nax.plot(\n    xi,\n    phenotype_quantiles[0.5],\n    c=c2,\n    lw=4,\n)\n\n# In addition to the part above, the following add the harmonized phenotype values\n# as scatter points along the estimated normative range:\nfor site in sorted(test_data[\"site\"].unique()):\n    subset = test_data[test_data[\"site\"] == site]\n    plt.scatter(\n        subset[\"age\"], subset[\"harmonized_phenotype\"], alpha=0.5, label=f\"{site}\", s=7)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Phenotype\")\nplt.title(\"Phenotype vs Age by Site\")\nplt.legend(title=\"Site\")\nplt.show()\n</pre> # The first part of the script is copied from the previous visualization section # this part visualizes the normative range of the phenotype values across age.  fig, ax = plt.subplots(1, 1, figsize=(12, 8))  # 100 evenly spaced age values for prediction xi = np.linspace(20, 80, 100)  # create dummy dataframe for prediction dummy_df = pd.DataFrame({     \"age\": xi, })  # Predict moments (mean and std) of the normative distribution moments = normative_model.predict(dummy_df, predict_without=['site']).predictions  # Let's compute charts along different quantiles quantiles = [0.001, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 0.999]  # Predict the phenotype values corresponding to different quantiles phenotype_quantiles = {     q: (moments[\"mu_estimate\"] + stats.norm.ppf(q=q) * moments[\"std_estimate\"])     for q in quantiles }  # Some colors for the plot c1 = \"#448e9c\"  # brighter shade c2 = \"#038da6\"  # darker shade  # Plotting shaded ranges for different quantiles for q1, q2, alpha in [     (0.001, 0.999, 0.05),     (0.01, 0.99, 0.1),     (0.05, 0.95, 0.2),     (0.25, 0.75, 0.3)]:     ax.fill_between(         xi,         phenotype_quantiles[q1],         phenotype_quantiles[q2],         alpha=alpha,         color=c1,     )      # Now add boundary lines with different colors     ax.plot(xi, phenotype_quantiles[q1], color=c1, lw=0.5, linestyle=\":\")     ax.plot(xi, phenotype_quantiles[q2], color=c1, lw=0.5, linestyle=\":\")  # Finally, add the median line with a different color and higher line width ax.plot(     xi,     phenotype_quantiles[0.5],     c=c2,     lw=4, )  # In addition to the part above, the following add the harmonized phenotype values # as scatter points along the estimated normative range: for site in sorted(test_data[\"site\"].unique()):     subset = test_data[test_data[\"site\"] == site]     plt.scatter(         subset[\"age\"], subset[\"harmonized_phenotype\"], alpha=0.5, label=f\"{site}\", s=7) plt.xlabel(\"Age\") plt.ylabel(\"Phenotype\") plt.title(\"Phenotype vs Age by Site\") plt.legend(title=\"Site\") plt.show()  <p>As seen in the plot, the harmonized phenotype values (i.e. after removing site effects) show a clearer relationship with age, and we can see how the new data points relate to the normative range without the confounding influence of site effects. For instance, note that the earlier scatter plot (before harmonization) indicated a clear site effect, with certain sites having visibly higher or lower phenotype values. After harmonization, this site effect is removed, allowing us to see the underlying relationship between age and the phenotype more clearly.</p>"},{"location":"tutorials/univariate_model/#univariate-model","title":"Univariate Model\u00b6","text":"<p>This tutorial demonstrates how to use the functionalities implemented in <code>SpectraNorm</code> to fit a univariate normative model to a single feature (i.e. a direct normative model). The tutorial covers data preparation, model fitting, and basic interpretation of results.</p>"},{"location":"tutorials/univariate_model/#importing-libraries","title":"Importing Libraries\u00b6","text":""},{"location":"tutorials/univariate_model/#data-preparation","title":"Data Preparation\u00b6","text":"<p>Here, we will generate synthetic data to illustrate the process of fitting a univariate normative model. In practice, you would replace this with loading your actual dataset.</p>"},{"location":"tutorials/univariate_model/#minimal-model-fitting-example","title":"Minimal model fitting example\u00b6","text":"<p>The example below demonstrates how to fit a univariate normative model using <code>snm.DirectNormativeModel</code>. We will use the synthetic data generated in the previous section.</p> <p>We start with the simplest way of defining and fitting a univariate normative model, in which most of the fitting parameters are set to their default values. This is a good starting point for users who are new to normative modeling and want to quickly fit a model without delving into the details of the fitting process.</p>"},{"location":"tutorials/univariate_model/#defining-the-model","title":"Defining the model\u00b6","text":"<p>In this example, we define a univariate normative model directly from the training data. The <code>DirectNormativeModel.from_dataframe</code> method allows us to specify the target variable, covariates, and other parameters directly from a pandas DataFrame. This approach is straightforward and does not require manual specification of the model specification and priors.</p>"},{"location":"tutorials/univariate_model/#fitting-the-model","title":"Fitting the model\u00b6","text":"<p>Now that the normative model has been defined, we can proceed to fit the model to the data. The <code>DirectNormativeModel.fit</code> method uses a prespecified model instance to estimate the parameters of the model based on the provided data and covariates. After fitting, we can examine the results to understand how well the model captures the relationship between the phenotype and the covariates.</p>"},{"location":"tutorials/univariate_model/#examining-the-fitted-model","title":"Examining the fitted model\u00b6","text":"<p>Now that the normative model has been fitted, we can examine the results to understand how well the model captures the relationship between the phenotype and the covariates.</p>"},{"location":"tutorials/univariate_model/#visualizing-the-model-fit","title":"Visualizing the model fit\u00b6","text":"<p>Here, we will look at the estimated normative ranges describing the trend by which the phenotype changes with age. Assuming that we are not interested in site effects, we can marginalize over the site covariate to obtain a single normative range that describes the expected phenotype values across age, regardless of site (i.e. partialing out the site effects). The plot below shows the estimated normative range with different shades indicating different centiles of the normative distribution.</p>"},{"location":"tutorials/univariate_model/#quantitative-evaluation-of-model-fit","title":"Quantitative evaluation of model fit\u00b6","text":"<p>Above and beyond the visual inspection, we can also evaluate the model fit quantitatively by looking at the estimated parameters and how well the model captures the variability in the training data. This can be done by examining the estimated predictions (using either the <code>DirectNormativeModel.predict</code> &amp; <code>NormativePredictions.evluate_predictions</code> methods or by directly using <code>DirectNormativeModel.evaluate</code> method):</p>"},{"location":"tutorials/univariate_model/#extending-the-model-to-unseen-data","title":"Extending the model to unseen data\u00b6","text":"<p>Now that we have fitted the model to the training data, we can also apply the fitted model to new, unseen data to obtain predictions and evaluate how well the model generalizes. This is an important step in normative modeling, as it allows us to assess the model's performance on data that was not used during training.</p> <p>This can verify whether the model has possibly overfitted to the training data or if it captures generalizable patterns that can be applied to new data.</p> <p>Let's generate some new synthetic data that was not used during training, but follows the same underlying distribution. We can then use the fitted model to make predictions on this new data and evaluate the results.</p>"},{"location":"tutorials/univariate_model/#quantitative-evaluation-of-out-of-sample-predictions","title":"Quantitative evaluation of out-of-sample predictions\u00b6","text":"<p>Similar to the evaluation of in-sample predictions, we can also evaluate the out-of-sample predictions quantitatively by looking at the estimated parameters and how well the model captures the variability in the new data. This can be done by examining the estimated predictions on the new data and comparing them to the actual values.</p>"},{"location":"tutorials/univariate_model/#visualizing-out-of-sample-predictions-with-harmonization","title":"Visualizing out-of-sample predictions (with harmonization)\u00b6","text":"<p>Notably, 'SpectraNorm' also provides the capability to harmonize the target variable by removing the estimated batch effects (i.e. site effects in this case) from the phenotype values. This allows us to visualize the relationship between age and the phenotype without the confounding influence of site effects, which can be particularly useful when we are interested in understanding the underlying relationship between age and the phenotype across different sites.</p> <p>The script below demonstrates how to compute the harmonized phenotype values by removing the estimated site effects from the observed phenotype values:</p>"},{"location":"tutorials/univariate_model/#concluding-remarks","title":"Concluding remarks\u00b6","text":"<p>In summary, this tutorial demonstrates how to fit a univariate normative model using <code>SpectraNorm</code>, evaluate the model fit both in-sample and out-of-sample, and visualize the results while accounting for batch effects. This provides an introductory overview of the process of normative modeling and how to interpret the results effectively.</p>"}]}