{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spectranorm","text":"<p>Documentation: https://sina-mansour.github.io/spectranorm</p> <p>Source Code: https://github.com/sina-mansour/spectranorm</p> <p>PyPI: https://pypi.org/project/spectranorm/</p> <p>A Python package for spectral normative modeling of neuroimaging and other high-dimensional data.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install spectranorm\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.8+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the docs directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github Pages page automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatting (<code>ruff format</code>), linters (e.g. <code>ruff</code> and <code>mypy</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This project was generated using the wolt-python-package-cookiecutter template.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#spectranorm.snm","title":"<code>snm</code>","text":"<p>snm.py</p> <p>Core implementation of spectral normative modeling (SNM).</p> <p>This module provides the code base for using spectral normative models. It can be used to fit direct and spectral normative models to data and also to predict normative centiles using pre-trained models.</p> <p>See full documentation at: https://sina-mansour.github.io/spectranorm</p>"},{"location":"api_docs/#spectranorm.snm.CovarianceModelSpec","title":"<code>CovarianceModelSpec</code>  <code>dataclass</code>","text":"<p>General specification of a normative model for covariance.</p> <p>This model aims to learn the relationships between two variables of interest for both of which a normative model is specified. This can capture patterns where the normative trends in two variables are related.</p> <p>Attributes:</p> Name Type Description <code>variable_of_interest_1</code> <code>str</code> <p>str Name of the first variable of interest.</p> <code>variable_of_interest_2</code> <code>str</code> <p>str Name of the second variable of interest.</p> <code>covariates</code> <code>list[CovariateSpec]</code> <p>list[CovariateSpec] Listing all model covariates and specifying how each covariate is modeled.</p> <code>influencing_covariance</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the covariance between the two variables of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovarianceModelSpec:\n    \"\"\"\n    General specification of a normative model for covariance.\n\n    This model aims to learn the relationships between two variables of interest for\n    both of which a normative model is specified. This can capture patterns where the\n    normative trends in two variables are related.\n\n    Attributes:\n        variable_of_interest_1: str\n            Name of the first variable of interest.\n        variable_of_interest_2: str\n            Name of the second variable of interest.\n        covariates: list[CovariateSpec]\n            Listing all model covariates and specifying how each covariate is modeled.\n        influencing_covariance: list[str]\n            List of covariate names that influence the covariance between the two\n            variables of interest.\n    \"\"\"\n\n    variable_of_interest_1: str\n    variable_of_interest_2: str\n    covariates: list[CovariateSpec]\n    influencing_covariance: list[str]\n\n    def __post_init__(self) -&gt; None:\n        if not isinstance(self.variable_of_interest_1, str):\n            err = \"variable_of_interest_1 must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.variable_of_interest_2, str):\n            err = \"variable_of_interest_2 must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.covariates, list):\n            err = \"covariates must be a list of CovariateSpec instances.\"\n            raise TypeError(err)\n        if not all(isinstance(cov, CovariateSpec) for cov in self.covariates):\n            err = \"All items in covariates must be CovariateSpec instances.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_covariance, list):\n            err = \"influencing_covariance must be a list of covariate names.\"\n            raise TypeError(err)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel","title":"<code>CovarianceNormativeModel</code>  <code>dataclass</code>","text":"<p>Covariance normative model implementation.</p> <p>This class implements covariance normative modeling, which models the covariance structure between a pair of variables as a normative random variable.</p> <p>Attributes:</p> Name Type Description <code>spec</code> <code>CovarianceModelSpec</code> <p>CovarianceModelSpec Specification of the covariance model including variables of interest, and list of covariates.</p> <code>batch_covariates</code> <code>list[str]</code> <p>list[str] List of covariate names that are treated as batch effects.</p> <code>defaults</code> <code>dict[str, Any]</code> <p>dict Default parameters for the model, including spline specifications, ADVI iterations, convergence tolerance, random seed, and Adam optimizer learning rates.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovarianceNormativeModel:\n    \"\"\"\n    Covariance normative model implementation.\n\n    This class implements covariance normative modeling, which models the covariance\n    structure between a pair of variables as a normative random variable.\n\n    Attributes:\n        spec: CovarianceModelSpec\n            Specification of the covariance model including variables of interest,\n            and list of covariates.\n        batch_covariates: list[str]\n            List of covariate names that are treated as batch effects.\n        defaults: dict\n            Default parameters for the model, including spline specifications,\n            ADVI iterations, convergence tolerance, random seed, and Adam optimizer\n            learning rates.\n    \"\"\"\n\n    spec: CovarianceModelSpec\n    batch_covariates: list[str]\n    defaults: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"spline_df\": DEFAULT_SPLINE_DF,\n            \"spline_degree\": DEFAULT_SPLINE_DEGREE,\n            \"spline_extrapolation_factor\": DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n            \"advi_iterations\": DEFAULT_ADVI_ITERATIONS,\n            \"advi_convergence_tolerance\": DEFAULT_ADVI_CONVERGENCE_TOLERANCE,\n            \"random_seed\": DEFAULT_RANDOM_SEED,\n            \"adam_learning_rate\": DEFAULT_ADAM_LEARNING_RATE,\n            \"adam_learning_rate_decay\": DEFAULT_ADAM_LEARNING_RATE_DECAY,\n            \"augmentation_multiplicity\": DEFAULT_COVARIANCE_AUGMENTATION_MULTIPLICITY,\n        },\n    )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the CovarianceNormativeModel instance.\n        \"\"\"\n        return (\n            f\"CovarianceNormativeModel(\\n\\tspec={self.spec}, \\n\\t\"\n            f\"batch_covariates={self.batch_covariates}\\n)\"\n        )\n\n    @classmethod\n    def from_direct_model(\n        cls,\n        direct_model: DirectNormativeModel,\n        variable_of_interest_1: str,\n        variable_of_interest_2: str,\n        influencing_covariance: list[str] | None = None,\n        defaults_overwrite: dict[str, Any] | None = None,\n    ) -&gt; CovarianceNormativeModel:\n        \"\"\"\n        Initialize the model from a direct model instance, and two variable names.\n\n        Args:\n            direct_model: DirectNormativeModel\n                This model will be used to instantiate a similar covariance model.\n            variable_of_interest_1: str\n                Name of the first target variable to model.\n            variable_of_interest_2: str\n                Name of the second target variable to model.\n            influencing_covariance: list[str] | None\n                List of covariates that influence the covariance structure. If not\n                provided, this will be copied from the direct model's\n                `influencing_variance`.\n\n        Returns:\n            CovarianceNormativeModel\n                An instance of CovarianceNormativeModel initialized with the provided\n                data.\n        \"\"\"\n        # Validity checks for input parameters\n        if not isinstance(direct_model, DirectNormativeModel):\n            err = \"direct_model must be an instance of DirectNormativeModel.\"\n            raise TypeError(err)\n        if not (\n            isinstance(variable_of_interest_1, str)\n            and isinstance(variable_of_interest_2, str)\n        ):\n            err = \"Variables of interest must be strings.\"\n            raise TypeError(err)\n\n        # Substitute influencing_covariance if not provided\n        if influencing_covariance is None:\n            influencing_covariance = direct_model.spec.influencing_variance\n\n        # Use the same setup as the direct model\n        model = cls(\n            spec=CovarianceModelSpec(\n                variable_of_interest_1=variable_of_interest_1,\n                variable_of_interest_2=variable_of_interest_2,\n                covariates=direct_model.spec.covariates,\n                influencing_covariance=influencing_covariance,\n            ),\n            batch_covariates=direct_model.batch_covariates,\n        )\n\n        # update defaults\n        model.defaults.update(direct_model.defaults)\n        model.defaults.update(defaults_overwrite or {})\n\n        return model\n\n    def _validate_model(self) -&gt; None:\n        \"\"\"\n        Validate the covariance model instance.\n\n        This method checks if the model instance is complete and valid.\n        It raises errors if any required fields are missing or if there are\n        inconsistencies in the model instance.\n        \"\"\"\n        if self.spec is None:\n            err = (\n                \"Model specification is not set. Please initialize the model,\"\n                \" e.g., with 'from_dataframe'.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.covariates) == 0:\n            err = (\n                \"No covariates specified in the model. \"\n                \"Please add covariates to the specification.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.influencing_covariance) == 0:\n            err = (\n                \"No covariates specified to influence the covariance \"\n                \"between the variables of interest.\"\n            )\n            raise ValueError(err)\n\n    def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n        \"\"\"\n        Save the fitted model and it's posterior to a directory.\n        The model will be saved in a subdirectory named 'saved_model'.\n        If this directory is not empty, an error is raised.\n\n        Args:\n            directory: Path\n                Path to a directory to save the model.\n            save_posterior: bool (default=False)\n                If True, save the model's posterior trace inference data.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n        model_dict = {\n            \"spec\": self.spec,\n            \"batch_covariates\": self.batch_covariates,\n            \"defaults\": self.defaults,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n            if hasattr(self, \"model_inference_data\") and save_posterior:\n                self.model_inference_data.to_netcdf(\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n        joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n\n    @classmethod\n    def load_model(\n        cls,\n        directory: Path,\n        *,\n        load_posterior: bool = False,\n    ) -&gt; CovarianceNormativeModel:\n        \"\"\"\n        Load the model and its posterior from a directory.\n        The model will be loaded from a subdirectory named 'saved_model'.\n\n        Args:\n            directory: Path\n                Path to the directory containing the model.\n            load_posterior: bool (default=False)\n                If True, load the model's posterior trace from the saved inference data.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"saved_model\",\n        )\n\n        # Load the saved model dict\n        model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n        # Create an instance of the class\n        instance = cls(\n            spec=model_dict[\"spec\"],\n            batch_covariates=model_dict[\"batch_covariates\"],\n        )\n\n        # Set the attributes from the loaded model dictionary\n        instance.defaults.update(model_dict[\"defaults\"])\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n            if load_posterior:\n                instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n\n        return instance\n\n    def _validate_dataframe_for_fitting(self, train_data: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Validate the training DataFrame for fitting.\n        \"\"\"\n        utils.general.validate_dataframe(\n            train_data,\n            (\n                [cov.name for cov in self.spec.covariates]\n                + [\n                    self.spec.variable_of_interest_1,\n                    self.spec.variable_of_interest_2,\n                    f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_std_estimate\",\n                ]\n            ),\n        )\n\n    def _build_model_coordinates(\n        self,\n        observations: npt.NDArray[np.integer[Any]],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Build the model coordinates for the training DataFrame.\n        \"\"\"\n        # Data coordinates\n        model_coords = {\"observations\": observations, \"scalar\": [0]}\n\n        # Additional coordinates for covariates\n        for cov in self.spec.covariates:\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"spline\":\n                    if cov.spline_spec is not None:  # to satisfy type checker\n                        model_coords[f\"{cov.name}_splines\"] = np.arange(\n                            cov.spline_spec.df,\n                        )\n                elif cov.effect == \"linear\":\n                    model_coords[f\"{cov.name}_linear\"] = np.arange(1)\n            elif cov.cov_type == \"categorical\":\n                model_coords[cov.name] = cov.categories\n            else:\n                err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                raise ValueError(err)\n        return model_coords\n\n    def _model_linear_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        sigma_prior: float = 0.1,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate.\n        \"\"\"\n        # Linear effect\n        linear_beta = pm.Normal(\n            f\"linear_beta_{cov.name}\",\n            mu=0,\n            sigma=sigma_prior,\n            size=1,\n            dims=(f\"{cov.name}_linear\",),\n        )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                ((train_data[cov.name].to_numpy() - cov.moments[0]) / cov.moments[1])\n                * linear_beta,\n            )\n        # Increment parameter count for linear effect\n        self.model_params[\"n_params\"] += 1\n\n    def _model_spline_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(train_data[cov.name].to_numpy()),\n        )\n        spline_betas = pm.ZeroSumNormal(\n            f\"spline_betas_{cov.name}\",\n            sigma=sigma_prior,\n            shape=spline_bases[cov.name].shape[1],\n            dims=(f\"{cov.name}_splines\",),\n        )\n        # Note ZeroSumNormal imposes a centering constraint (ensuring identifiability)\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))  # type: ignore[no-untyped-call]\n        # Increment parameter count for spline effects\n        if cov.spline_spec is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n\n    def _model_categorical_correlation_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 1,\n        sigma_hierarchical_prior: float = 0.1,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(train_data[cov.name].to_numpy()),\n        )\n        if cov.hierarchical:\n            # Hierarchical categorical effect\n            # Hyperpriors for category (Bayesian equivalent of random effects)\n            sigma_intercept_category = pm.HalfNormal(\n                f\"sigma_intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(\"scalar\",),\n            )\n\n            # Hierarchical intercepts for each category (using reparameterized form)\n            categorical_intercept_offset = pm.ZeroSumNormal(\n                f\"intercept_offset_{cov.name}\",\n                sigma=sigma_hierarchical_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n            categorical_intercept = pm.Deterministic(\n                f\"intercept_{cov.name}\",\n                (\n                    categorical_intercept_offset\n                    * pt.reshape(sigma_intercept_category, (1,))  # type: ignore[attr-defined]\n                ),\n                dims=(cov.name,),\n            )\n\n            # Increment parameter count for hierarchical intercept\n            self.model_params[\"n_params\"] += 1\n\n        else:\n            # Non-hierarchical (linear) categorical effect\n            categorical_intercept = pm.ZeroSumNormal(\n                f\"intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n        # Increment parameter count for categorical effects\n        if cov.categories is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += len(cov.categories) - 1\n\n    def _combine_all_correlation_effects(\n        self,\n        z_transformed_correlation_effects: list[TensorVariable[Any, Any]],\n        combination_indices: npt.NDArray[np.integer[Any]],\n        combination_weights: npt.NDArray[np.floating[Any]],\n        standardized_vois: npt.NDArray[np.floating[Any]],\n        standardized_vois_mu_estimate: npt.NDArray[np.floating[Any]],\n        standardized_vois_std_estimate: npt.NDArray[np.floating[Any]],\n    ) -&gt; None:\n        \"\"\"\n        Combine all effects to model the observed data likelihood from the list of\n        correlation effects.\n        \"\"\"\n        # Combine all covariance effects\n        z_transformed_correlation_estimate = sum(z_transformed_correlation_effects)\n\n        # Convert z-transformed score to correlation\n        correlation_estimate = pt.tanh(z_transformed_correlation_estimate)\n\n        # Now apply the random combinations to get final distribution estimates\n        # Apply combination weights for mu estimate\n        combined_mu_estimate = pt.sum(  # type: ignore[no-untyped-call]\n            pt.mul(\n                standardized_vois_mu_estimate[combination_indices, :],\n                combination_weights,\n            ),\n            axis=1,\n        )\n        # Apply combination weights for sigma estimate\n        combined_sigma_estimate = pt.mul(\n            standardized_vois_std_estimate[combination_indices, :],\n            combination_weights,\n        )\n        # Apply combination weights for correlation estimate\n        combined_correlation_estimate = correlation_estimate[combination_indices]  # pyright: ignore[reportOptionalSubscript]\n        # Now build the std estimate\n        combined_std_estimate = pt.sqrt(\n            combined_sigma_estimate[:, 0] ** 2  # pyright: ignore[reportOptionalSubscript]\n            + combined_sigma_estimate[:, 1] ** 2  # pyright: ignore[reportOptionalSubscript]\n            + (\n                2\n                * combined_sigma_estimate[:, 0]  # pyright: ignore[reportOptionalSubscript]\n                * combined_sigma_estimate[:, 1]  # pyright: ignore[reportOptionalSubscript]\n                * combined_correlation_estimate\n            ),\n        )\n\n        # Apply combination to the variables of interest\n        combined_variable_of_interest = pt.sum(  # type: ignore[no-untyped-call]\n            pt.mul(\n                standardized_vois[combination_indices, :],\n                combination_weights,\n            ),\n            axis=1,\n        )\n\n        # Model likelihood estimation for covariance model\n        _likelihood = pm.Normal(\n            f\"likelihood_cov_{self.spec.variable_of_interest_1}_{self.spec.variable_of_interest_2}\",\n            mu=combined_mu_estimate,\n            sigma=combined_std_estimate,\n            observed=combined_variable_of_interest,\n            total_size=(\n                self.model_params[\"sample_size\"]\n                * self.defaults[\"augmentation_multiplicity\"]\n            ),\n        )\n\n    def _fit_model_with_advi(self, *, progress_bar: bool = True) -&gt; None:\n        \"\"\"\n        Fit the model using Automatic Differentiation Variational Inference (ADVI).\n        \"\"\"\n        base_lr = self.defaults[\"adam_learning_rate\"]\n        decay = self.defaults[\"adam_learning_rate_decay\"]\n        lr = shared(base_lr)  # type: ignore[no-untyped-call]  # type: ignore[no-untyped-call]\n        optimizer = pm.adam(learning_rate=cast(\"float\", lr))\n\n        # Adaptive learning rate schedule callback\n        def update_learning_rate(_approx: Any, _loss: Any, iteration: int) -&gt; None:\n            lr.set_value(base_lr * (decay**iteration))\n\n        # Run automatic differential variational inference to fit the model\n        self._trace = pm.fit(\n            method=\"advi\",\n            n=self.defaults[\"advi_iterations\"],\n            random_seed=self.defaults[\"random_seed\"],  # For reproducibility\n            obj_optimizer=optimizer,\n            callbacks=[\n                update_learning_rate,\n                pm.callbacks.CheckParametersConvergence(\n                    tolerance=self.defaults[\"advi_convergence_tolerance\"],\n                    diff=\"relative\",\n                ),\n            ],\n            progressbar=progress_bar,\n        )\n\n        # Sample from the posterior distribution and store the results\n        self.model_inference_data = self._trace.sample(\n            2000,\n            random_seed=self.defaults[\"random_seed\"],\n        )\n\n        # Compute posterior means and standard deviations\n        posterior_means = self.model_inference_data.posterior.mean(\n            dim=(\"chain\", \"draw\"),\n        )\n        posterior_stds = self.model_inference_data.posterior.std(\n            dim=(\"chain\", \"draw\"),\n        )\n\n        # Store posterior means and stds as a dictionary in model parameters\n        self.model_params[\"posterior_means\"] = {\n            x: posterior_means.data_vars[x].to_numpy()\n            for x in posterior_means.data_vars\n        }\n        self.model_params[\"posterior_stds\"] = {\n            x: posterior_stds.data_vars[x].to_numpy() for x in posterior_stds.data_vars\n        }\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Fit the normative model to the training data.\n\n        This method implements the fitting logic for the normative model\n        based on the provided training data and model specification.\n\n        Args:\n            train_data: pd.DataFrame\n                DataFrame containing the training data. It must include the variable\n                of interest, their predicted moments, and all specified covariates.\n            save_directory: Path | None\n                A path to a directory to save the model. If provided, the fitted model\n                will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # Random number generator seed for reproducibility\n        rng = np.random.default_rng(seed=self.defaults[\"random_seed\"])\n\n        # A dictionary to hold the model parameters after fitting\n        self.model_params = {}\n\n        # Data preparation\n        # Combination weights\n        combination_weights = np.ones(\n            shape=(train_data.shape[0] * self.defaults[\"augmentation_multiplicity\"], 2),\n        )\n        if self.defaults[\"augmentation_multiplicity\"] &gt; 1:\n            combination_weights = np.cbrt(\n                rng.uniform(\n                    low=-1,\n                    high=1,\n                    size=(\n                        train_data.shape[0]\n                        * self.defaults[\"augmentation_multiplicity\"],\n                        2,\n                    ),\n                ),\n            )\n        # Data coordinates\n        combination_indices = np.repeat(\n            np.arange(train_data.shape[0])[np.newaxis, :],\n            repeats=self.defaults[\"augmentation_multiplicity\"],\n            axis=0,\n        ).ravel()\n        model_coords = self._build_model_coordinates(\n            observations=combination_indices,\n        )\n\n        # Fitting logic\n        with pm.Model(coords=model_coords) as self._model:\n            # Standardize the variable of interest, and store mean and std\n            # This is done to ensure that the model is not sensitive to\n            # the scale of the variable\n            variables_of_interest = train_data[\n                [self.spec.variable_of_interest_1, self.spec.variable_of_interest_2]\n            ].to_numpy()\n            self.model_params[\"mean_vois\"] = variables_of_interest.mean(axis=0)\n            self.model_params[\"std_vois\"] = variables_of_interest.std(axis=0)\n            standardized_vois = (\n                variables_of_interest - self.model_params[\"mean_vois\"]\n            ) / self.model_params[\"std_vois\"]\n            sample_size = variables_of_interest.shape[0]\n            self.model_params[\"sample_size\"] = sample_size\n            variables_of_interest_mu_estimate = train_data[\n                [\n                    f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n                ]\n            ].to_numpy()\n            standardized_vois_mu_estimate = (\n                variables_of_interest_mu_estimate - self.model_params[\"mean_vois\"]\n            ) / self.model_params[\"std_vois\"]\n            variables_of_interest_std_estimate = train_data[\n                [\n                    f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                    f\"{self.spec.variable_of_interest_2}_std_estimate\",\n                ]\n            ].to_numpy()\n            standardized_vois_std_estimate = (\n                variables_of_interest_std_estimate / self.model_params[\"std_vois\"]\n            )\n\n            # Create a list to contain the effects of covariates on\n            # the z-transformed correlation\n            z_transformed_correlation_effects = []\n\n            # A dictionary for precomputed bspline basis functions\n            spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n            # A dictionary for factorized categories\n            category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n            # Model the z-transformed correlation between the variables of interest\n            self.model_params[\"n_params\"] = 0  # Initialize parameter count\n            # Model the global intercept for z\n            global_intercept_z = pm.Normal(\n                \"global_intercept_z\",\n                mu=0,\n                sigma=5,\n                dims=(\"scalar\",),\n            )\n            z_transformed_correlation_effects.append(global_intercept_z)\n            # Increment parameter count for global intercept\n            self.model_params[\"n_params\"] += 1\n            # Model additional covariate effects on the z estimate\n            for cov in self.spec.covariates:\n                if cov.name in self.spec.influencing_covariance:\n                    if cov.cov_type == \"numerical\":\n                        if cov.effect == \"linear\":\n                            self._model_linear_correlation_effect(\n                                train_data=train_data,\n                                cov=cov,\n                                effects_list=z_transformed_correlation_effects,\n                            )\n                        elif cov.effect == \"spline\":\n                            self._model_spline_correlation_effect(\n                                train_data=train_data,\n                                cov=cov,\n                                effects_list=z_transformed_correlation_effects,\n                                spline_bases=spline_bases,\n                            )\n                    elif cov.cov_type == \"categorical\":\n                        self._model_categorical_correlation_effect(\n                            train_data=train_data,\n                            cov=cov,\n                            effects_list=z_transformed_correlation_effects,\n                            category_indices=category_indices,\n                        )\n\n                    else:\n                        err = (\n                            f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                        )\n                        raise ValueError(err)\n\n            # Combine all covariance effects\n            self._combine_all_correlation_effects(\n                z_transformed_correlation_effects=z_transformed_correlation_effects,\n                combination_indices=combination_indices,\n                combination_weights=combination_weights,\n                standardized_vois=standardized_vois,\n                standardized_vois_mu_estimate=standardized_vois_mu_estimate,\n                standardized_vois_std_estimate=standardized_vois_std_estimate,\n            )\n\n            # Fit the model using ADVI\n            self._fit_model_with_advi(progress_bar=progress_bar)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(Path(save_directory))\n\n    def predict(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any] | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict correlation for new data (from covariates) using the fitted model.\n\n        Args:\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n\n        Returns:\n            NormativePredictions: Object containing the predicted pairwise correlations\n                for the variables of interest.\n        \"\"\"\n        # Validate the new data\n        validation_columns = [cov.name for cov in self.spec.covariates]\n        utils.general.validate_dataframe(test_covariates, validation_columns)\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Posterior means\n        posterior_means = model_params[\"posterior_means\"]\n\n        # Calculate mean and variance effects\n        z_transformed_correlation_estimate = np.zeros(test_covariates.shape[0]) + float(\n            posterior_means[\"global_intercept_z\"],\n        )\n\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_covariance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is None:\n                            err = (\n                                f\"Covariate '{cov.name}' is missing moments for\"\n                                \" standardization.\"\n                            )\n                            raise ValueError(err)\n                        z_transformed_correlation_estimate += (\n                            (test_covariates[cov.name].to_numpy() - cov.moments[0])\n                            / cov.moments[1]\n                        ) * posterior_means[f\"linear_beta_{cov.name}\"]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            test_covariates[cov.name].to_numpy(),\n                        )\n                        spline_betas = posterior_means[f\"spline_betas_{cov.name}\"]\n                        z_transformed_correlation_estimate += (\n                            spline_bases @ spline_betas\n                        )\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        test_covariates[cov.name].to_numpy(),\n                    )\n                    categorical_intercept = None\n                    if cov.hierarchical:\n                        categorical_intercept = (\n                            posterior_means[f\"intercept_offset_{cov.name}\"]\n                            * posterior_means[f\"sigma_intercept_{cov.name}\"]\n                        )\n                    else:\n                        categorical_intercept = posterior_means[f\"intercept_{cov.name}\"]\n                    z_transformed_correlation_estimate += categorical_intercept[\n                        category_indices\n                    ]\n\n        # Convert z-transformed score to correlation\n        correlation_estimate = np.tanh(z_transformed_correlation_estimate)\n\n        # Create a the predictions object and return\n        return NormativePredictions({\"correlation_estimate\": correlation_estimate})\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>String representation of the CovarianceNormativeModel instance.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation of the CovarianceNormativeModel instance.\n    \"\"\"\n    return (\n        f\"CovarianceNormativeModel(\\n\\tspec={self.spec}, \\n\\t\"\n        f\"batch_covariates={self.batch_covariates}\\n)\"\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.fit","title":"<code>fit(train_data: pd.DataFrame, *, save_directory: Path | None = None, progress_bar: bool = True) -&gt; None</code>","text":"<p>Fit the normative model to the training data.</p> <p>This method implements the fitting logic for the normative model based on the provided training data and model specification.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data. It must include the variable of interest, their predicted moments, and all specified covariates.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    train_data: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n) -&gt; None:\n    \"\"\"\n    Fit the normative model to the training data.\n\n    This method implements the fitting logic for the normative model\n    based on the provided training data and model specification.\n\n    Args:\n        train_data: pd.DataFrame\n            DataFrame containing the training data. It must include the variable\n            of interest, their predicted moments, and all specified covariates.\n        save_directory: Path | None\n            A path to a directory to save the model. If provided, the fitted model\n            will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # Random number generator seed for reproducibility\n    rng = np.random.default_rng(seed=self.defaults[\"random_seed\"])\n\n    # A dictionary to hold the model parameters after fitting\n    self.model_params = {}\n\n    # Data preparation\n    # Combination weights\n    combination_weights = np.ones(\n        shape=(train_data.shape[0] * self.defaults[\"augmentation_multiplicity\"], 2),\n    )\n    if self.defaults[\"augmentation_multiplicity\"] &gt; 1:\n        combination_weights = np.cbrt(\n            rng.uniform(\n                low=-1,\n                high=1,\n                size=(\n                    train_data.shape[0]\n                    * self.defaults[\"augmentation_multiplicity\"],\n                    2,\n                ),\n            ),\n        )\n    # Data coordinates\n    combination_indices = np.repeat(\n        np.arange(train_data.shape[0])[np.newaxis, :],\n        repeats=self.defaults[\"augmentation_multiplicity\"],\n        axis=0,\n    ).ravel()\n    model_coords = self._build_model_coordinates(\n        observations=combination_indices,\n    )\n\n    # Fitting logic\n    with pm.Model(coords=model_coords) as self._model:\n        # Standardize the variable of interest, and store mean and std\n        # This is done to ensure that the model is not sensitive to\n        # the scale of the variable\n        variables_of_interest = train_data[\n            [self.spec.variable_of_interest_1, self.spec.variable_of_interest_2]\n        ].to_numpy()\n        self.model_params[\"mean_vois\"] = variables_of_interest.mean(axis=0)\n        self.model_params[\"std_vois\"] = variables_of_interest.std(axis=0)\n        standardized_vois = (\n            variables_of_interest - self.model_params[\"mean_vois\"]\n        ) / self.model_params[\"std_vois\"]\n        sample_size = variables_of_interest.shape[0]\n        self.model_params[\"sample_size\"] = sample_size\n        variables_of_interest_mu_estimate = train_data[\n            [\n                f\"{self.spec.variable_of_interest_1}_mu_estimate\",\n                f\"{self.spec.variable_of_interest_2}_mu_estimate\",\n            ]\n        ].to_numpy()\n        standardized_vois_mu_estimate = (\n            variables_of_interest_mu_estimate - self.model_params[\"mean_vois\"]\n        ) / self.model_params[\"std_vois\"]\n        variables_of_interest_std_estimate = train_data[\n            [\n                f\"{self.spec.variable_of_interest_1}_std_estimate\",\n                f\"{self.spec.variable_of_interest_2}_std_estimate\",\n            ]\n        ].to_numpy()\n        standardized_vois_std_estimate = (\n            variables_of_interest_std_estimate / self.model_params[\"std_vois\"]\n        )\n\n        # Create a list to contain the effects of covariates on\n        # the z-transformed correlation\n        z_transformed_correlation_effects = []\n\n        # A dictionary for precomputed bspline basis functions\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n        # A dictionary for factorized categories\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n        # Model the z-transformed correlation between the variables of interest\n        self.model_params[\"n_params\"] = 0  # Initialize parameter count\n        # Model the global intercept for z\n        global_intercept_z = pm.Normal(\n            \"global_intercept_z\",\n            mu=0,\n            sigma=5,\n            dims=(\"scalar\",),\n        )\n        z_transformed_correlation_effects.append(global_intercept_z)\n        # Increment parameter count for global intercept\n        self.model_params[\"n_params\"] += 1\n        # Model additional covariate effects on the z estimate\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_covariance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_correlation_effect(\n                            train_data=train_data,\n                            cov=cov,\n                            effects_list=z_transformed_correlation_effects,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_correlation_effect(\n                            train_data=train_data,\n                            cov=cov,\n                            effects_list=z_transformed_correlation_effects,\n                            spline_bases=spline_bases,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_correlation_effect(\n                        train_data=train_data,\n                        cov=cov,\n                        effects_list=z_transformed_correlation_effects,\n                        category_indices=category_indices,\n                    )\n\n                else:\n                    err = (\n                        f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    )\n                    raise ValueError(err)\n\n        # Combine all covariance effects\n        self._combine_all_correlation_effects(\n            z_transformed_correlation_effects=z_transformed_correlation_effects,\n            combination_indices=combination_indices,\n            combination_weights=combination_weights,\n            standardized_vois=standardized_vois,\n            standardized_vois_mu_estimate=standardized_vois_mu_estimate,\n            standardized_vois_std_estimate=standardized_vois_std_estimate,\n        )\n\n        # Fit the model using ADVI\n        self._fit_model_with_advi(progress_bar=progress_bar)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(Path(save_directory))\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.from_direct_model","title":"<code>from_direct_model(direct_model: DirectNormativeModel, variable_of_interest_1: str, variable_of_interest_2: str, influencing_covariance: list[str] | None = None, defaults_overwrite: dict[str, Any] | None = None) -&gt; CovarianceNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize the model from a direct model instance, and two variable names.</p> <p>Parameters:</p> Name Type Description Default <code>direct_model</code> <code>DirectNormativeModel</code> <p>DirectNormativeModel This model will be used to instantiate a similar covariance model.</p> required <code>variable_of_interest_1</code> <code>str</code> <p>str Name of the first target variable to model.</p> required <code>variable_of_interest_2</code> <code>str</code> <p>str Name of the second target variable to model.</p> required <code>influencing_covariance</code> <code>list[str] | None</code> <p>list[str] | None List of covariates that influence the covariance structure. If not provided, this will be copied from the direct model's <code>influencing_variance</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>CovarianceNormativeModel</code> <p>CovarianceNormativeModel An instance of CovarianceNormativeModel initialized with the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef from_direct_model(\n    cls,\n    direct_model: DirectNormativeModel,\n    variable_of_interest_1: str,\n    variable_of_interest_2: str,\n    influencing_covariance: list[str] | None = None,\n    defaults_overwrite: dict[str, Any] | None = None,\n) -&gt; CovarianceNormativeModel:\n    \"\"\"\n    Initialize the model from a direct model instance, and two variable names.\n\n    Args:\n        direct_model: DirectNormativeModel\n            This model will be used to instantiate a similar covariance model.\n        variable_of_interest_1: str\n            Name of the first target variable to model.\n        variable_of_interest_2: str\n            Name of the second target variable to model.\n        influencing_covariance: list[str] | None\n            List of covariates that influence the covariance structure. If not\n            provided, this will be copied from the direct model's\n            `influencing_variance`.\n\n    Returns:\n        CovarianceNormativeModel\n            An instance of CovarianceNormativeModel initialized with the provided\n            data.\n    \"\"\"\n    # Validity checks for input parameters\n    if not isinstance(direct_model, DirectNormativeModel):\n        err = \"direct_model must be an instance of DirectNormativeModel.\"\n        raise TypeError(err)\n    if not (\n        isinstance(variable_of_interest_1, str)\n        and isinstance(variable_of_interest_2, str)\n    ):\n        err = \"Variables of interest must be strings.\"\n        raise TypeError(err)\n\n    # Substitute influencing_covariance if not provided\n    if influencing_covariance is None:\n        influencing_covariance = direct_model.spec.influencing_variance\n\n    # Use the same setup as the direct model\n    model = cls(\n        spec=CovarianceModelSpec(\n            variable_of_interest_1=variable_of_interest_1,\n            variable_of_interest_2=variable_of_interest_2,\n            covariates=direct_model.spec.covariates,\n            influencing_covariance=influencing_covariance,\n        ),\n        batch_covariates=direct_model.batch_covariates,\n    )\n\n    # update defaults\n    model.defaults.update(direct_model.defaults)\n    model.defaults.update(defaults_overwrite or {})\n\n    return model\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.load_model","title":"<code>load_model(directory: Path, *, load_posterior: bool = False) -&gt; CovarianceNormativeModel</code>  <code>classmethod</code>","text":"<p>Load the model and its posterior from a directory. The model will be loaded from a subdirectory named 'saved_model'.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to the directory containing the model.</p> required <code>load_posterior</code> <code>bool</code> <p>bool (default=False) If True, load the model's posterior trace from the saved inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    directory: Path,\n    *,\n    load_posterior: bool = False,\n) -&gt; CovarianceNormativeModel:\n    \"\"\"\n    Load the model and its posterior from a directory.\n    The model will be loaded from a subdirectory named 'saved_model'.\n\n    Args:\n        directory: Path\n            Path to the directory containing the model.\n        load_posterior: bool (default=False)\n            If True, load the model's posterior trace from the saved inference data.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"saved_model\",\n    )\n\n    # Load the saved model dict\n    model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n    # Create an instance of the class\n    instance = cls(\n        spec=model_dict[\"spec\"],\n        batch_covariates=model_dict[\"batch_covariates\"],\n    )\n\n    # Set the attributes from the loaded model dictionary\n    instance.defaults.update(model_dict[\"defaults\"])\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n        if load_posterior:\n            instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n\n    return instance\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.predict","title":"<code>predict(test_covariates: pd.DataFrame, model_params: dict[str, Any] | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict correlation for new data (from covariates) using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates.</p> required <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted pairwise correlations for the variables of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    test_covariates: pd.DataFrame,\n    model_params: dict[str, Any] | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict correlation for new data (from covariates) using the fitted model.\n\n    Args:\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n\n    Returns:\n        NormativePredictions: Object containing the predicted pairwise correlations\n            for the variables of interest.\n    \"\"\"\n    # Validate the new data\n    validation_columns = [cov.name for cov in self.spec.covariates]\n    utils.general.validate_dataframe(test_covariates, validation_columns)\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Posterior means\n    posterior_means = model_params[\"posterior_means\"]\n\n    # Calculate mean and variance effects\n    z_transformed_correlation_estimate = np.zeros(test_covariates.shape[0]) + float(\n        posterior_means[\"global_intercept_z\"],\n    )\n\n    for cov in self.spec.covariates:\n        if cov.name in self.spec.influencing_covariance:\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"linear\":\n                    if cov.moments is None:\n                        err = (\n                            f\"Covariate '{cov.name}' is missing moments for\"\n                            \" standardization.\"\n                        )\n                        raise ValueError(err)\n                    z_transformed_correlation_estimate += (\n                        (test_covariates[cov.name].to_numpy() - cov.moments[0])\n                        / cov.moments[1]\n                    ) * posterior_means[f\"linear_beta_{cov.name}\"]\n                elif cov.effect == \"spline\":\n                    spline_bases = cov.make_spline_bases(\n                        test_covariates[cov.name].to_numpy(),\n                    )\n                    spline_betas = posterior_means[f\"spline_betas_{cov.name}\"]\n                    z_transformed_correlation_estimate += (\n                        spline_bases @ spline_betas\n                    )\n            elif cov.cov_type == \"categorical\":\n                category_indices = cov.factorize_categories(\n                    test_covariates[cov.name].to_numpy(),\n                )\n                categorical_intercept = None\n                if cov.hierarchical:\n                    categorical_intercept = (\n                        posterior_means[f\"intercept_offset_{cov.name}\"]\n                        * posterior_means[f\"sigma_intercept_{cov.name}\"]\n                    )\n                else:\n                    categorical_intercept = posterior_means[f\"intercept_{cov.name}\"]\n                z_transformed_correlation_estimate += categorical_intercept[\n                    category_indices\n                ]\n\n    # Convert z-transformed score to correlation\n    correlation_estimate = np.tanh(z_transformed_correlation_estimate)\n\n    # Create a the predictions object and return\n    return NormativePredictions({\"correlation_estimate\": correlation_estimate})\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovarianceNormativeModel.save_model","title":"<code>save_model(directory: Path, *, save_posterior: bool = False) -&gt; None</code>","text":"<p>Save the fitted model and it's posterior to a directory. The model will be saved in a subdirectory named 'saved_model'. If this directory is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to a directory to save the model.</p> required <code>save_posterior</code> <code>bool</code> <p>bool (default=False) If True, save the model's posterior trace inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n    \"\"\"\n    Save the fitted model and it's posterior to a directory.\n    The model will be saved in a subdirectory named 'saved_model'.\n    If this directory is not empty, an error is raised.\n\n    Args:\n        directory: Path\n            Path to a directory to save the model.\n        save_posterior: bool (default=False)\n            If True, save the model's posterior trace inference data.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n    model_dict = {\n        \"spec\": self.spec,\n        \"batch_covariates\": self.batch_covariates,\n        \"defaults\": self.defaults,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n        if hasattr(self, \"model_inference_data\") and save_posterior:\n            self.model_inference_data.to_netcdf(\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n    joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovariateSpec","title":"<code>CovariateSpec</code>  <code>dataclass</code>","text":"<p>Specification of a single covariate and how it should be modeled.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>str Name of the covariate (e.g., 'age', 'site').</p> <code>cov_type</code> <code>CovariateType</code> <p>str Type of the covariate ('numerical' or 'categorical').</p> <code>effect</code> <code>NumericalEffect | None</code> <p>str For numerical covariates, how the effect is modeled ('linear' or 'spline').</p> <code>categories</code> <code>NDArray[str_] | None</code> <p>np.ndarray | None For categorical covariates, the category labels stored as a NumPy array.</p> <code>hierarchical</code> <code>bool | None</code> <p>bool For categorical covariates, whether to model with a hierarchical structure.</p> <code>spline_spec</code> <code>SplineSpec | None</code> <p>SplineSpec | None Optional SplineSpec instance for spline modeling; required if effect is 'spline'.</p> Validation <ul> <li>Numerical covariates must specify 'effect'.</li> <li>If 'effect' is 'spline', 'spline_spec' must be provided.</li> <li>Categorical covariates must specify 'hierarchical'.</li> <li>Categorical covariates cannot have 'effect' or 'spline_spec'.</li> <li>Categorical covariates must have categories listed.</li> </ul> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass CovariateSpec:\n    \"\"\"\n    Specification of a single covariate and how it should be modeled.\n\n    Attributes:\n        name: str\n            Name of the covariate (e.g., 'age', 'site').\n        cov_type: str\n            Type of the covariate ('numerical' or 'categorical').\n        effect: str\n            For numerical covariates, how the effect is modeled ('linear'\n            or 'spline').\n        categories: np.ndarray | None\n            For categorical covariates, the category labels stored as a NumPy array.\n        hierarchical: bool\n            For categorical covariates, whether to model with a\n            hierarchical structure.\n        spline_spec: SplineSpec | None\n            Optional SplineSpec instance for spline modeling;\n            required if effect is 'spline'.\n\n    Validation:\n        - Numerical covariates must specify 'effect'.\n        - If 'effect' is 'spline', 'spline_spec' must be provided.\n        - Categorical covariates must specify 'hierarchical'.\n        - Categorical covariates cannot have 'effect' or 'spline_spec'.\n        - Categorical covariates must have categories listed.\n    \"\"\"\n\n    name: str\n    cov_type: CovariateType  # \"categorical\" or \"numerical\"\n    effect: NumericalEffect | None = None  # Only if numerical\n    categories: npt.NDArray[np.str_] | None = None  # Only if categorical\n    hierarchical: bool | None = None  # Only if categorical\n    spline_spec: SplineSpec | None = None  # Only for spline modeling\n    moments: tuple[float, float] | None = None  # Only for linear effects\n\n    # Validation checks for the covariate specification.\n    def validate_numerical(self) -&gt; None:\n        if self.effect not in {\"linear\", \"spline\"}:\n            err = (\n                f\"Numerical covariate '{self.name}' must specify effect as \"\n                \"'linear' or 'spline'.\"\n            )\n            raise ValueError(err)\n        if self.hierarchical is not None:\n            err = (\n                f\"Numerical covariate '{self.name}' should not specify 'hierarchical'.\"\n            )\n            raise ValueError(err)\n        if self.categories is not None:\n            err = f\"Numerical covariate '{self.name}' should not specify 'categories'.\"\n            raise ValueError(err)\n        if self.effect == \"spline\":\n            if self.spline_spec is None:\n                err = (\n                    f\"Numerical covariate '{self.name}' must have spline \"\n                    \"specification if effect is 'spline'.\"\n                )\n                raise ValueError(err)\n            if self.moments is not None:\n                err = (\n                    f\"Numerical covariate '{self.name}' should not specify \"\n                    \"moments if effect is 'spline'.\"\n                )\n                raise ValueError(err)\n        if self.effect == \"linear\":\n            if self.spline_spec is not None:\n                err = (\n                    f\"Numerical covariate '{self.name}' should not have spline \"\n                    \"specification unless effect is 'spline'.\"\n                )\n                raise ValueError(err)\n            if self.moments is None:\n                err = (\n                    f\"Numerical covariate '{self.name}' must specify moments \"\n                    \"(mean and standard deviation) for linear effects.\"\n                )\n                raise ValueError(err)\n\n    def validate_categorical(self) -&gt; None:\n        if self.effect is not None:\n            err = (\n                f\"Categorical covariate '{self.name}' should not have a \"\n                \"numerical effect type.\"\n            )\n            raise ValueError(err)\n        if self.spline_spec is not None:\n            err = (\n                f\"Categorical covariate '{self.name}' should not have spline \"\n                \"specification.\"\n            )\n            raise ValueError(err)\n        if self.hierarchical is None:\n            err = (\n                f\"Categorical covariate '{self.name}' must specify whether \"\n                \"it is hierarchical.\"\n            )\n            raise ValueError(err)\n        if self.categories is None:\n            err = f\"Categorical covariate '{self.name}' must specify categories.\"\n            raise ValueError(err)\n        if not isinstance(self.categories, np.ndarray):\n            err = (\n                f\"Categorical covariate '{self.name}' must specify categories \"\n                \"as a NumPy array.\"\n            )\n            raise TypeError(err)\n\n    def __post_init__(self) -&gt; None:\n        if self.cov_type == \"numerical\":\n            self.validate_numerical()\n        elif self.cov_type == \"categorical\":\n            self.validate_categorical()\n        else:\n            err = f\"Invalid covariate type '{self.cov_type}' for '{self.name}'.\"\n            raise ValueError(err)\n\n    def make_spline_bases(\n        self,\n        values: npt.NDArray[np.floating[Any]],\n        *,\n        include_intercept: bool = True,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Create B-spline basis expansion functions for a given covariate.\n\n        Args:\n            values (np.ndarray): The values to create the spline basis functions for.\n\n        Returns:\n            np.ndarray: The B-spline basis function expansion.\n        \"\"\"\n        if self.effect != \"spline\" or self.spline_spec is None:\n            err = f\"Covariate '{self.name}' is not a spline covariate.\"\n            raise ValueError(err)\n\n        # Create B-spline basis functions\n        return np.array(\n            patsy.bs(  # pyright: ignore[reportAttributeAccessIssue]\n                values,\n                knots=self.spline_spec.knots,\n                df=self.spline_spec.df,\n                degree=self.spline_spec.degree,\n                lower_bound=self.spline_spec.lower_bound,\n                upper_bound=self.spline_spec.upper_bound,\n                include_intercept=include_intercept,\n            ),\n        )\n\n    def factorize_categories(\n        self,\n        values: npt.NDArray[np.str_],\n    ) -&gt; npt.NDArray[np.int_]:\n        \"\"\"\n        Factorize categorical covariate values into numerical indices.\n\n        Args:\n            values (np.ndarray): The values to factorize.\n\n        Returns:\n            np.ndarray: The factorized numerical indices for the categories.\n        \"\"\"\n        if self.cov_type != \"categorical\":\n            err = (\n                f\"Covariate '{self.name}' is not a categorical \"\n                \"covariate to be factorized.\"\n            )\n            raise ValueError(err)\n\n        # Create a mapping from category values to indices\n        if self.categories is None:  # to satisfy type checker\n            err = f\"Covariate '{self.name}' does not have categories defined.\"\n            raise ValueError(err)\n        category_mapping: dict[str, int] = {\n            category: idx for idx, category in enumerate(self.categories)\n        }\n        # Factorize the values using the mapping\n        return np.array([category_mapping[val] for val in values], dtype=int)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovariateSpec.factorize_categories","title":"<code>factorize_categories(values: npt.NDArray[np.str_]) -&gt; npt.NDArray[np.int_]</code>","text":"<p>Factorize categorical covariate values into numerical indices.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>The values to factorize.</p> required <p>Returns:</p> Type Description <code>NDArray[int_]</code> <p>np.ndarray: The factorized numerical indices for the categories.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def factorize_categories(\n    self,\n    values: npt.NDArray[np.str_],\n) -&gt; npt.NDArray[np.int_]:\n    \"\"\"\n    Factorize categorical covariate values into numerical indices.\n\n    Args:\n        values (np.ndarray): The values to factorize.\n\n    Returns:\n        np.ndarray: The factorized numerical indices for the categories.\n    \"\"\"\n    if self.cov_type != \"categorical\":\n        err = (\n            f\"Covariate '{self.name}' is not a categorical \"\n            \"covariate to be factorized.\"\n        )\n        raise ValueError(err)\n\n    # Create a mapping from category values to indices\n    if self.categories is None:  # to satisfy type checker\n        err = f\"Covariate '{self.name}' does not have categories defined.\"\n        raise ValueError(err)\n    category_mapping: dict[str, int] = {\n        category: idx for idx, category in enumerate(self.categories)\n    }\n    # Factorize the values using the mapping\n    return np.array([category_mapping[val] for val in values], dtype=int)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.CovariateSpec.make_spline_bases","title":"<code>make_spline_bases(values: npt.NDArray[np.floating[Any]], *, include_intercept: bool = True) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Create B-spline basis expansion functions for a given covariate.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>The values to create the spline basis functions for.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray: The B-spline basis function expansion.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def make_spline_bases(\n    self,\n    values: npt.NDArray[np.floating[Any]],\n    *,\n    include_intercept: bool = True,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Create B-spline basis expansion functions for a given covariate.\n\n    Args:\n        values (np.ndarray): The values to create the spline basis functions for.\n\n    Returns:\n        np.ndarray: The B-spline basis function expansion.\n    \"\"\"\n    if self.effect != \"spline\" or self.spline_spec is None:\n        err = f\"Covariate '{self.name}' is not a spline covariate.\"\n        raise ValueError(err)\n\n    # Create B-spline basis functions\n    return np.array(\n        patsy.bs(  # pyright: ignore[reportAttributeAccessIssue]\n            values,\n            knots=self.spline_spec.knots,\n            df=self.spline_spec.df,\n            degree=self.spline_spec.degree,\n            lower_bound=self.spline_spec.lower_bound,\n            upper_bound=self.spline_spec.upper_bound,\n            include_intercept=include_intercept,\n        ),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel","title":"<code>DirectNormativeModel</code>  <code>dataclass</code>","text":"<p>Direct normative model implementation.</p> <p>This class implements the direct normative modeling approach, which directly models the variable of interest using the specified covariates. It can be used to fit a model to data and predict normative centiles.</p> <p>Attributes:</p> Name Type Description <code>spec</code> <code>NormativeModelSpec</code> <p>NormativeModelSpec Specification of the normative model including variable of interest, covariates, and data source.</p> <code>batch_covariates</code> <code>list[str]</code> <p>list[str] List of covariate names that are treated as batch effects.</p> <code>defaults</code> <code>dict[str, Any]</code> <p>dict Default parameters for the model, including spline specifications, ADVI iterations, convergence tolerance, random seed, and Adam optimizer learning rates.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass DirectNormativeModel:\n    \"\"\"\n    Direct normative model implementation.\n\n    This class implements the direct normative modeling approach, which\n    directly models the variable of interest using the specified covariates.\n    It can be used to fit a model to data and predict normative centiles.\n\n    Attributes:\n        spec: NormativeModelSpec\n            Specification of the normative model including variable of interest,\n            covariates, and data source.\n        batch_covariates: list[str]\n            List of covariate names that are treated as batch effects.\n        defaults: dict\n            Default parameters for the model, including spline specifications,\n            ADVI iterations, convergence tolerance, random seed, and Adam optimizer\n            learning rates.\n    \"\"\"\n\n    spec: NormativeModelSpec\n    batch_covariates: list[str]\n    defaults: dict[str, Any] = field(\n        default_factory=lambda: {\n            \"spline_df\": DEFAULT_SPLINE_DF,\n            \"spline_degree\": DEFAULT_SPLINE_DEGREE,\n            \"spline_extrapolation_factor\": DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n            \"advi_iterations\": DEFAULT_ADVI_ITERATIONS,\n            \"advi_convergence_tolerance\": DEFAULT_ADVI_CONVERGENCE_TOLERANCE,\n            \"random_seed\": DEFAULT_RANDOM_SEED,\n            \"adam_learning_rate\": DEFAULT_ADAM_LEARNING_RATE,\n            \"adam_learning_rate_decay\": DEFAULT_ADAM_LEARNING_RATE_DECAY,\n        },\n    )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        String representation of the DirectNormativeModel instance.\n        \"\"\"\n        return (\n            f\"DirectNormativeModel(\\n\\tspec={self.spec}, \\n\\t\"\n            f\"batch_covariates={self.batch_covariates}\\n)\"\n        )\n\n    @staticmethod\n    def _validate_init_args(\n        model_type: ModelType,\n        variable_of_interest: str,\n        numerical_covariates: list[str],\n        categorical_covariates: list[str],\n        batch_covariates: list[str],\n        nonlinear_covariates: list[str],\n    ) -&gt; None:\n        # Validity checks for input parameters\n        if model_type not in {\"HBR\", \"BLR\"}:\n            err = f\"Invalid model type '{model_type}'. Must be 'HBR' or 'BLR'.\"\n            raise ValueError(err)\n        for list_name, covariate_list in [\n            (\"numerical\", numerical_covariates),\n            (\"categorical\", categorical_covariates),\n            (\"batch\", batch_covariates),\n            (\"nonlinear\", nonlinear_covariates),\n        ]:\n            if not all(isinstance(item, str) for item in covariate_list):\n                err = f\"All covariate names must be strings: {list_name} covariates.\"\n                raise TypeError(err)\n        if not isinstance(variable_of_interest, str):\n            err = \"Variable of interest must be a string.\"\n            raise TypeError(err)\n        if not all(col in categorical_covariates for col in batch_covariates):\n            err = \"All batch covariates must be included in categorical covariates.\"\n            raise ValueError(err)\n        if not all(col in numerical_covariates for col in nonlinear_covariates):\n            err = \"All nonlinear covariates must be included in numerical covariates.\"\n            raise ValueError(err)\n\n    @classmethod\n    def from_dataframe(\n        cls,\n        model_type: ModelType,\n        dataframe: pd.DataFrame,\n        variable_of_interest: str,\n        numerical_covariates: list[str] | None = None,\n        categorical_covariates: list[str] | None = None,\n        batch_covariates: list[str] | None = None,\n        nonlinear_covariates: list[str] | None = None,\n        influencing_mean: list[str] | None = None,\n        influencing_variance: list[str] | None = None,\n        spline_kwargs: dict[str, Any] | None = None,\n    ) -&gt; DirectNormativeModel:\n        \"\"\"\n        Initialize a normative model from a pandas DataFrame.\n\n        Args:\n            model_type: ModelType\n                Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n                Regression) or \"BLR\" (Bayesian Linear Regression).\n            dataframe: pd.DataFrame\n                DataFrame containing the data.\n            variable_of_interest: str\n                Name of the target variable to model.\n            numerical_covariates: list[str] | None\n                List of numerical covariate names.\n            categorical_covariates: list[str] | None\n                List of categorical covariate names.\n            batch_covariates: list[str] | None\n                List of batch covariate names which should also be included in\n                categorical_covariates.\n            nonlinear_covariates: list[str] | None\n                List of covariate names to be modeled as nonlinear effects.\n                These should also be included in numerical_covariates.\n            influencing_mean: list[str] | None\n                List of covariate names that influence the mean of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            influencing_variance: list[str] | None\n                List of covariate names that influence the variance of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            spline_kwargs: dict\n                Additional keyword arguments for spline specification, such as\n                `df`, `degree`, and `knots`. These are passed to the\n                `create_spline_spec` method to create spline specifications for\n                nonlinear covariates.\n\n        Returns:\n            DirectNormativeModel\n                An instance of DirectNormativeModel initialized with the provided data.\n        \"\"\"\n        # Set default values for optional parameters\n        numerical_covariates = numerical_covariates or []\n        categorical_covariates = categorical_covariates or []\n        batch_covariates = batch_covariates or []\n        nonlinear_covariates = nonlinear_covariates or []\n        influencing_mean = influencing_mean or []\n        influencing_variance = influencing_variance or []\n        spline_kwargs = spline_kwargs or {}\n\n        # Validity checks for input parameters\n        cls._validate_init_args(\n            model_type,\n            variable_of_interest,\n            numerical_covariates,\n            categorical_covariates,\n            batch_covariates,\n            nonlinear_covariates,\n        )\n        utils.general.validate_dataframe(\n            dataframe,\n            [variable_of_interest, *numerical_covariates, *categorical_covariates],\n        )\n\n        # Create an instance of the class\n        self = cls(\n            spec=NormativeModelSpec(\n                variable_of_interest=variable_of_interest,\n                covariates=[],\n                influencing_mean=influencing_mean,\n                influencing_variance=influencing_variance,\n            ),\n            batch_covariates=batch_covariates,\n        )\n\n        # Populate the spline_kwargs with defaults if not provided\n        spline_kwargs[\"df\"] = spline_kwargs.get(\"df\", self.defaults[\"spline_df\"])\n        spline_kwargs[\"degree\"] = spline_kwargs.get(\n            \"degree\",\n            self.defaults[\"spline_degree\"],\n        )\n        spline_kwargs[\"extrapolation_factor\"] = spline_kwargs.get(\n            \"extrapolation_factor\",\n            self.defaults[\"spline_extrapolation_factor\"],\n        )\n\n        # Start building the model specification\n        # Add categorical covariates\n        for cov_name in categorical_covariates:\n            hierarchical = False\n            if cov_name in batch_covariates and model_type == \"HBR\":\n                hierarchical = True\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"categorical\",\n                    categories=dataframe[cov_name].unique(),\n                    hierarchical=hierarchical,\n                ),\n            )\n        for cov_name in numerical_covariates:\n            if cov_name not in nonlinear_covariates:\n                self.spec.covariates.append(\n                    CovariateSpec(\n                        name=cov_name,\n                        cov_type=\"numerical\",\n                        effect=\"linear\",\n                        moments=(\n                            dataframe[cov_name].mean(),\n                            dataframe[cov_name].std(),\n                        ),\n                    ),\n                )\n            else:\n                self.spec.covariates.append(\n                    CovariateSpec(\n                        name=cov_name,\n                        cov_type=\"numerical\",\n                        effect=\"spline\",\n                        spline_spec=SplineSpec.create_spline_spec(\n                            dataframe[cov_name],\n                            **spline_kwargs,\n                        ),\n                    ),\n                )\n        return self\n\n    def _validate_model(self) -&gt; None:\n        \"\"\"\n        Validate the model instance.\n\n        This method checks if the model instance is complete and valid.\n        It raises errors if any required fields are missing or if there are\n        inconsistencies in the model specification.\n        \"\"\"\n        if self.spec is None:\n            err = (\n                \"Model specification is not set. \"\n                \"Please initialize the model, e.g., with 'from_dataframe'.\"\n            )\n            raise ValueError(err)\n        if len(self.spec.covariates) == 0:\n            err = (\n                \"No covariates specified in the model. \"\n                \"Please add covariates to the specification.\"\n            )\n            raise ValueError(err)\n        if (len(self.spec.influencing_mean) == 0) and (\n            len(self.spec.influencing_variance) == 0\n        ):\n            err = (\n                \"No covariates specified to influence the mean or \"\n                \"variance of the variable of interest.\"\n            )\n            raise ValueError(err)\n\n    def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n        \"\"\"\n        Save the fitted model and it's posterior to a directory.\n        The model will be saved in a subdirectory named 'saved_model'.\n        If this directory is not empty, an error is raised.\n\n        Args:\n            directory: Path\n                Path to a directory to save the model.\n            save_posterior: bool (default=False)\n                If True, save the model's posterior trace inference data.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n        model_dict = {\n            \"spec\": self.spec,\n            \"batch_covariates\": self.batch_covariates,\n            \"defaults\": self.defaults,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n            if hasattr(self, \"model_inference_data\") and save_posterior:\n                self.model_inference_data.to_netcdf(\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n        joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n\n    @classmethod\n    def load_model(\n        cls,\n        directory: Path,\n        *,\n        load_posterior: bool = False,\n    ) -&gt; DirectNormativeModel:\n        \"\"\"\n        Load the model and its posterior from a directory.\n        The model will be loaded from a subdirectory named 'saved_model'.\n\n        Args:\n            directory: Path\n                Path to the directory containing the model.\n            load_posterior: bool (default=False)\n                If True, load the model's posterior trace from the saved inference data.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"saved_model\",\n        )\n\n        # Load the saved model dict\n        model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n        # Create an instance of the class\n        instance = cls(\n            spec=model_dict[\"spec\"],\n            batch_covariates=model_dict[\"batch_covariates\"],\n        )\n\n        # Set the attributes from the loaded model dictionary\n        instance.defaults.update(model_dict[\"defaults\"])\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n            if load_posterior:\n                instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                    saved_model_dir / \"model_inference_data.nc\",\n                )\n\n        return instance\n\n    def _validate_dataframe_for_fitting(self, train_data: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Validate the training DataFrame for fitting.\n        \"\"\"\n        utils.general.validate_dataframe(\n            train_data,\n            (\n                [cov.name for cov in self.spec.covariates]\n                + [self.spec.variable_of_interest]\n            ),\n        )\n\n    def _build_model_coordinates(\n        self,\n        observations: npt.NDArray[np.integer[Any]],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Build the model coordinates for the training DataFrame.\n        \"\"\"\n        # Data coordinates\n        model_coords = {\"observations\": observations, \"scalar\": [0]}\n\n        # Additional coordinates for covariates\n        for cov in self.spec.covariates:\n            if cov.cov_type == \"numerical\":\n                if cov.effect == \"spline\":\n                    if cov.spline_spec is not None:  # to satisfy type checker\n                        model_coords[f\"{cov.name}_splines\"] = np.arange(\n                            cov.spline_spec.df,\n                        )\n                elif cov.effect == \"linear\":\n                    model_coords[f\"{cov.name}_linear\"] = np.arange(1)\n            elif cov.cov_type == \"categorical\":\n                model_coords[cov.name] = cov.categories\n            else:\n                err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                raise ValueError(err)\n        return model_coords\n\n    def _model_linear_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        sigma_prior: float = 10,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate on the mean estimate.\n        \"\"\"\n        # Linear effect\n        linear_beta = pm.Normal(\n            f\"linear_beta_{cov.name}\",\n            mu=0,\n            sigma=sigma_prior,\n            size=1,\n            dims=(f\"{cov.name}_linear\",),\n        )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                ((train_data[cov.name].to_numpy() - cov.moments[0]) / cov.moments[1])\n                * linear_beta,\n            )\n        # Increment parameter count for linear effect\n        self.model_params[\"n_params\"] += 1\n\n    def _model_spline_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 10,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate on the mean estimate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(train_data[cov.name].to_numpy()),\n        )\n        spline_betas = pm.ZeroSumNormal(\n            f\"spline_betas_{cov.name}\",\n            sigma=sigma_prior,\n            shape=spline_bases[cov.name].shape[1],\n            dims=(f\"{cov.name}_splines\",),\n        )\n        # Note ZeroSumNormal imposes a centering constraint (ensuring identifiability)\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))  # type: ignore[no-untyped-call]\n        # Increment parameter count for spline effects\n        if cov.spline_spec is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n\n    def _model_categorical_mean_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 10,\n        hierarchical_sigma_prior: float = 1,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate on the mean estimate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(train_data[cov.name].to_numpy()),\n        )\n        if cov.hierarchical:\n            # Hierarchical categorical effect\n            # Hyperpriors for category (Bayesian equivalent of random effects)\n            sigma_intercept_category = pm.HalfNormal(\n                f\"sigma_intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(\"scalar\",),\n            )\n\n            # Hierarchical intercepts for each category (using reparameterized form)\n            categorical_intercept_offset = pm.ZeroSumNormal(\n                f\"intercept_offset_{cov.name}\",\n                sigma=hierarchical_sigma_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n            categorical_intercept = pm.Deterministic(\n                f\"intercept_{cov.name}\",\n                (\n                    categorical_intercept_offset\n                    * pt.reshape(sigma_intercept_category, (1,))  # type: ignore[attr-defined]\n                ),\n                dims=(cov.name,),\n            )\n\n            # Increment parameter count for hierarchical intercept\n            self.model_params[\"n_params\"] += 1\n\n        else:\n            # Non-hierarchical (linear) categorical effect\n            categorical_intercept = pm.ZeroSumNormal(\n                f\"intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n        # Increment parameter count for categorical effects\n        if cov.categories is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += len(cov.categories) - 1\n\n    def _model_all_mean_effects(\n        self,\n        train_data: pd.DataFrame,\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n    ) -&gt; list[TensorVariable[Any, Any]]:\n        \"\"\"\n        Model all covariate mean effects.\n        \"\"\"\n        mean_effects = []\n        # Model the global intercept\n        global_intercept = pm.Normal(\n            \"global_intercept\",\n            mu=0,\n            sigma=10,\n            dims=(\"scalar\",),\n        )\n        mean_effects.append(global_intercept)\n        # Increment parameter count for global intercept\n        self.model_params[\"n_params\"] += 1\n        # Model additional covariate effects on the mean\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_mean:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_mean_effect(\n                            train_data,\n                            cov,\n                            mean_effects,\n                            sigma_prior=10,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_mean_effect(\n                            train_data,\n                            cov,\n                            mean_effects,\n                            spline_bases,\n                            sigma_prior=10,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_mean_effect(\n                        train_data,\n                        cov,\n                        mean_effects,\n                        category_indices,\n                        sigma_prior=10,\n                        hierarchical_sigma_prior=1,\n                    )\n                else:\n                    err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    raise ValueError(err)\n        return mean_effects\n\n    def _model_linear_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        sigma_prior: float = 0.1,\n    ) -&gt; None:\n        \"\"\"\n        Model a linear effect for a numerical covariate on the variance estimate.\n        \"\"\"\n        # Linear effect\n        linear_beta = pm.Normal(\n            f\"variance_linear_beta_{cov.name}\",\n            mu=0,\n            sigma=sigma_prior,\n            size=1,\n            dims=(f\"{cov.name}_linear\",),\n        )\n        if cov.moments is not None:  # to satisfy type checker\n            effects_list.append(\n                ((train_data[cov.name].to_numpy() - cov.moments[0]) / cov.moments[1])\n                * linear_beta,\n            )\n        # Increment parameter count for linear effect\n        self.model_params[\"n_params\"] += 1\n\n    def _model_spline_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        sigma_prior: float = 0.1,\n    ) -&gt; None:\n        \"\"\"\n        Model a spline effect for a numerical covariate on the variance estimate.\n        \"\"\"\n        # Spline effect\n        spline_bases[cov.name] = spline_bases.get(\n            cov.name,\n            cov.make_spline_bases(train_data[cov.name].to_numpy()),\n        )\n        spline_betas = pm.ZeroSumNormal(\n            f\"variance_spline_betas_{cov.name}\",\n            sigma=sigma_prior,\n            shape=spline_bases[cov.name].shape[1],\n            dims=(f\"{cov.name}_splines\",),\n        )\n        # Note ZeroSumNormal imposes a centering constraint (ensuring identifiability)\n        effects_list.append(pt.dot(spline_bases[cov.name], spline_betas.T))  # type: ignore[no-untyped-call]\n        # Increment parameter count for spline effects\n        if cov.spline_spec is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += cov.spline_spec.df - 1\n\n    def _model_categorical_variance_effect(\n        self,\n        train_data: pd.DataFrame,\n        cov: CovariateSpec,\n        effects_list: list[TensorVariable[Any, Any]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n        sigma_prior: float = 0.1,\n        hierarchical_sigma_prior: float = 0.1,\n    ) -&gt; None:\n        \"\"\"\n        Model the effect of a categorical covariate on the variance estimate.\n        \"\"\"\n        # Factorize categories\n        category_indices[cov.name] = category_indices.get(\n            cov.name,\n            cov.factorize_categories(train_data[cov.name].to_numpy()),\n        )\n        if cov.hierarchical:\n            # Hierarchical categorical effect\n            # Hyperpriors for category (Bayesian equivalent of random effects)\n            sigma_intercept_category = pm.HalfNormal(\n                f\"variance_sigma_intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(\"scalar\",),\n            )\n\n            # Hierarchical intercepts for each category (using reparameterized form)\n            categorical_intercept_offset = pm.ZeroSumNormal(\n                f\"variance_intercept_offset_{cov.name}\",\n                sigma=hierarchical_sigma_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n            categorical_intercept = pm.Deterministic(\n                f\"variance_intercept_{cov.name}\",\n                (\n                    categorical_intercept_offset\n                    * pt.reshape(sigma_intercept_category, (1,))  # type: ignore[attr-defined]\n                ),\n                dims=(cov.name,),\n            )\n\n            # Increment parameter count for hierarchical intercept\n            self.model_params[\"n_params\"] += 1\n\n        else:\n            # Non-hierarchical (linear) categorical effect\n            categorical_intercept = pm.ZeroSumNormal(\n                f\"variance_intercept_{cov.name}\",\n                sigma=sigma_prior,\n                dims=(cov.name,),\n            )\n            # Note ZeroSumNormal imposes a centering constraint\n            # (ensuring identifiability)\n        effects_list.append(\n            categorical_intercept[category_indices[cov.name]],\n        )\n        # Increment parameter count for categorical effects\n        if cov.categories is not None:  # to satisfy type checker\n            self.model_params[\"n_params\"] += len(cov.categories) - 1\n\n    def _model_all_variance_effects(\n        self,\n        train_data: pd.DataFrame,\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]],\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]],\n    ) -&gt; list[TensorVariable[Any, Any]]:\n        \"\"\"\n        Model all covariate variance effects.\n        \"\"\"\n        variance_effects = []\n        # Model the global variance\n        global_variance_baseline = pm.Normal(\n            \"global_variance_baseline\",\n            mu=-0.5,\n            sigma=0.3,\n            dims=(\"scalar\",),\n        )\n        variance_effects.append(global_variance_baseline)\n        # Increment parameter count for global variance\n        self.model_params[\"n_params\"] += 1\n        # Model additional covariate effects on the variance\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_variance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        self._model_linear_variance_effect(\n                            train_data,\n                            cov,\n                            variance_effects,\n                            sigma_prior=0.1,\n                        )\n                    elif cov.effect == \"spline\":\n                        self._model_spline_variance_effect(\n                            train_data,\n                            cov,\n                            variance_effects,\n                            spline_bases,\n                            sigma_prior=0.1,\n                        )\n                elif cov.cov_type == \"categorical\":\n                    self._model_categorical_variance_effect(\n                        train_data,\n                        cov,\n                        variance_effects,\n                        category_indices,\n                        sigma_prior=0.1,\n                        hierarchical_sigma_prior=0.1,\n                    )\n                else:\n                    err = f\"Invalid covariate type '{cov.cov_type}' for '{cov.name}'.\"\n                    raise ValueError(err)\n        return variance_effects\n\n    def _combine_all_effects(\n        self,\n        mean_effects: list[TensorVariable[Any, Any]],\n        variance_effects: list[TensorVariable[Any, Any]],\n        standardized_voi: npt.NDArray[np.floating[Any]],\n    ) -&gt; None:\n        \"\"\"\n        Combine all effects to model the observed data likelihood.\n        \"\"\"\n        # Combine all mean and variance effects\n        mu_estimate = sum(mean_effects)\n        log_sigma_estimate = sum(variance_effects)\n        sigma_estimate = pt.exp(log_sigma_estimate)\n\n        # Model likelihood of the variable of interest\n        _likelihood = pm.Normal(\n            f\"likelihood_{self.spec.variable_of_interest}\",\n            mu=mu_estimate,\n            sigma=sigma_estimate,\n            observed=standardized_voi,\n            total_size=self.model_params[\"sample_size\"],\n        )\n\n    def _fit_model_with_advi(self, *, progress_bar: bool = True) -&gt; None:\n        \"\"\"\n        Fit the model using Automatic Differentiation Variational Inference (ADVI).\n        \"\"\"\n        base_lr = self.defaults[\"adam_learning_rate\"]\n        decay = self.defaults[\"adam_learning_rate_decay\"]\n        lr = shared(base_lr)  # type: ignore[no-untyped-call]\n        optimizer = pm.adam(learning_rate=cast(\"float\", lr))\n\n        # Adaptive learning rate schedule callback\n        def update_learning_rate(_approx: Any, _loss: Any, iteration: int) -&gt; None:\n            lr.set_value(base_lr * (decay**iteration))\n\n        # Run automatic differential variational inference to fit the model\n        self._trace = pm.fit(\n            method=\"advi\",\n            n=self.defaults[\"advi_iterations\"],\n            random_seed=self.defaults[\"random_seed\"],  # For reproducibility\n            obj_optimizer=optimizer,\n            callbacks=[\n                update_learning_rate,\n                pm.callbacks.CheckParametersConvergence(\n                    tolerance=self.defaults[\"advi_convergence_tolerance\"],\n                    diff=\"relative\",\n                ),\n            ],\n            progressbar=progress_bar,\n        )\n\n        # Sample from the posterior distribution and store the results\n        self.model_inference_data = self._trace.sample(\n            2000,\n            random_seed=self.defaults[\"random_seed\"],\n        )\n\n        # Compute posterior means and standard deviations\n        posterior_means = self.model_inference_data.posterior.mean(\n            dim=(\"chain\", \"draw\"),\n        )\n        posterior_stds = self.model_inference_data.posterior.std(dim=(\"chain\", \"draw\"))\n\n        # Store posterior means and stds as a dictionary in model parameters\n        self.model_params[\"posterior_means\"] = {\n            x: posterior_means.data_vars[x].to_numpy()\n            for x in posterior_means.data_vars\n        }\n        self.model_params[\"posterior_stds\"] = {\n            x: posterior_stds.data_vars[x].to_numpy() for x in posterior_stds.data_vars\n        }\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        progress_bar: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Fit the normative model to the training data.\n\n        This method implements the fitting logic for the normative model\n        based on the provided training data and model specification.\n\n        Args:\n            train_data: pd.DataFrame\n                DataFrame containing the training data. It must include the variable\n                of interest and all specified covariates.\n            save_directory: Path | None\n                A path to a directory to save the model. If provided, the fitted model\n                will be saved to this path.\n            progress_bar: bool\n                If True, display a progress bar during fitting. Defaults to True.\n        \"\"\"\n        # Validation checks\n        self._validate_model()\n        self._validate_dataframe_for_fitting(train_data)\n\n        # A dictionary to hold the model parameters after fitting\n        self.model_params = {}\n\n        # Data preparation\n        model_coords = self._build_model_coordinates(\n            observations=np.arange(train_data.shape[0]),\n        )\n\n        # Fitting logic\n        with pm.Model(coords=model_coords) as self._model:\n            # Standardize the variable of interest, and store mean and std\n            # This ensures that the model is not sensitive to the scale of the variable\n            variable_of_interest = train_data[self.spec.variable_of_interest].to_numpy()\n            self.model_params[\"mean_VOI\"] = variable_of_interest.mean()\n            self.model_params[\"std_VOI\"] = variable_of_interest.std()\n            standardized_voi = (\n                variable_of_interest - self.model_params[\"mean_VOI\"]\n            ) / self.model_params[\"std_VOI\"]\n            self.model_params[\"sample_size\"] = variable_of_interest.shape[0]\n\n            # A dictionary for precomputed bspline basis functions\n            spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n            # A dictionary for factorized categories\n            category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n            # Initialize parameter count\n            self.model_params[\"n_params\"] = 0\n\n            # Model the mean of the variable of interest\n            mean_effects = self._model_all_mean_effects(\n                train_data,\n                spline_bases,\n                category_indices,\n            )\n\n            # Model the variance of the variable of interest\n            variance_effects = self._model_all_variance_effects(\n                train_data,\n                spline_bases,\n                category_indices,\n            )\n\n            # Combine all mean and variance effects\n            self._combine_all_effects(mean_effects, variance_effects, standardized_voi)\n\n            # Fit the model using ADVI\n            self._fit_model_with_advi(progress_bar=progress_bar)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(Path(save_directory))\n\n    def _predict_mu(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict the mean of the variable of interest.\n        \"\"\"\n        # Calculate mean effect\n        mu_estimate = np.full(\n            test_covariates.shape[0],\n            model_params[\"posterior_means\"][\"global_intercept\"].item(),\n        )\n\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_mean:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is not None:  # to satisfy type checker\n                            mu_estimate += (\n                                (test_covariates[cov.name].to_numpy() - cov.moments[0])\n                                / cov.moments[1]\n                            ) * model_params[\"posterior_means\"][\n                                f\"linear_beta_{cov.name}\"\n                            ]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            test_covariates[cov.name].to_numpy(),\n                        )\n                        spline_betas = model_params[\"posterior_means\"][\n                            f\"spline_betas_{cov.name}\"\n                        ]\n                        mu_estimate += np.dot(spline_bases, spline_betas)\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        test_covariates[cov.name].to_numpy(),\n                    )\n                    if cov.hierarchical:\n                        categorical_intercept = (\n                            model_params[\"posterior_means\"][\n                                f\"intercept_offset_{cov.name}\"\n                            ]\n                            * model_params[\"posterior_means\"][\n                                f\"sigma_intercept_{cov.name}\"\n                            ]\n                        )\n                    else:\n                        categorical_intercept = model_params[\"posterior_means\"][\n                            f\"intercept_{cov.name}\"\n                        ]\n                    mu_estimate += categorical_intercept[category_indices]\n\n        return np.array(\n            mu_estimate * model_params[\"std_VOI\"] + model_params[\"mean_VOI\"],\n        )\n\n    def _predict_std(\n        self,\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Internal method to predict the standard deviation of the variable of interest.\n        \"\"\"\n        # Calculate deviation effect\n        log_sigma_estimate = np.full(\n            test_covariates.shape[0],\n            model_params[\"posterior_means\"][\"global_variance_baseline\"].item(),\n        )\n\n        for cov in self.spec.covariates:\n            if cov.name in self.spec.influencing_variance:\n                if cov.cov_type == \"numerical\":\n                    if cov.effect == \"linear\":\n                        if cov.moments is not None:  # to satisfy type checker\n                            log_sigma_estimate += (\n                                (test_covariates[cov.name].to_numpy() - cov.moments[0])\n                                / cov.moments[1]\n                            ) * model_params[\"posterior_means\"][\n                                f\"variance_linear_beta_{cov.name}\"\n                            ]\n                    elif cov.effect == \"spline\":\n                        spline_bases = cov.make_spline_bases(\n                            test_covariates[cov.name].to_numpy(),\n                        )\n                        variance_spline_betas = model_params[\"posterior_means\"][\n                            f\"variance_spline_betas_{cov.name}\"\n                        ]\n                        log_sigma_estimate += spline_bases @ variance_spline_betas\n                elif cov.cov_type == \"categorical\":\n                    category_indices = cov.factorize_categories(\n                        test_covariates[cov.name].to_numpy(),\n                    )\n                    if cov.hierarchical:\n                        categorical_variance_intercept = (\n                            model_params[\"posterior_means\"][\n                                f\"variance_intercept_offset_{cov.name}\"\n                            ]\n                            * model_params[\"posterior_means\"][\n                                f\"variance_sigma_intercept_{cov.name}\"\n                            ]\n                        )\n                    else:\n                        categorical_variance_intercept = model_params[\n                            \"posterior_means\"\n                        ][f\"variance_intercept_{cov.name}\"]\n                    log_sigma_estimate += categorical_variance_intercept[\n                        category_indices\n                    ]\n\n        return np.array(np.exp(log_sigma_estimate) * model_params[\"std_VOI\"])\n\n    def predict(\n        self,\n        test_covariates: pd.DataFrame,\n        *,\n        extended: bool = False,\n        model_params: dict[str, Any] | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict normative moments (mean, std) for new data using the fitted model.\n\n        Args:\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n            extended: bool\n                If True, return additional stats such as log-likelihood, centiles, etc.\n                Note that extended predictions require variable_of_interest to be\n                provided in the test_covariates DataFrame.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n\n        Returns:\n            NormativePredictions: Object containing the predicted moments (mean, std)\n                for the variable of interest.\n        \"\"\"\n        # Validate the new data\n        validation_columns = [cov.name for cov in self.spec.covariates]\n        if extended:\n            validation_columns.append(self.spec.variable_of_interest)\n        utils.general.validate_dataframe(test_covariates, validation_columns)\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # Calculate mean and variance effects and store in the predictions object\n        predictions = NormativePredictions(\n            {\n                \"mu_estimate\": self._predict_mu(test_covariates, model_params),\n                \"std_estimate\": self._predict_std(test_covariates, model_params),\n            },\n        )\n\n        # Check if extended predictions are requested\n        if extended:\n            # Add extended statistics to predictions (e.g. centiles, log loss, etc.)\n            predictions.extend_predictions(\n                variable_of_interest=test_covariates[\n                    self.spec.variable_of_interest\n                ].to_numpy(),\n            )\n\n        return predictions\n\n    def evaluate(self, new_data: pd.DataFrame) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the model on new data and return predictions.\n\n        Args:\n            new_data: pd.DataFrame\n                DataFrame containing the new data to evaluate.\n                It must include all specified covariates and the variable of interest.\n\n        Returns:\n            NormativePredictions: Object containing the predictions and evaluation\n            metrics.\n        \"\"\"\n        # Run extended predictions\n        return self.predict(test_covariates=new_data).evaluate_predictions(\n            variable_of_interest=new_data[self.spec.variable_of_interest].to_numpy(),\n            train_mean=self.model_params[\"mean_VOI\"],\n            train_std=self.model_params[\"std_VOI\"],\n            n_params=self.model_params[\"n_params\"],\n        )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>String representation of the DirectNormativeModel instance.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    String representation of the DirectNormativeModel instance.\n    \"\"\"\n    return (\n        f\"DirectNormativeModel(\\n\\tspec={self.spec}, \\n\\t\"\n        f\"batch_covariates={self.batch_covariates}\\n)\"\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.evaluate","title":"<code>evaluate(new_data: pd.DataFrame) -&gt; NormativePredictions</code>","text":"<p>Evaluate the model on new data and return predictions.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new data to evaluate. It must include all specified covariates and the variable of interest.</p> required <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predictions and evaluation</p> <code>NormativePredictions</code> <p>metrics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate(self, new_data: pd.DataFrame) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the model on new data and return predictions.\n\n    Args:\n        new_data: pd.DataFrame\n            DataFrame containing the new data to evaluate.\n            It must include all specified covariates and the variable of interest.\n\n    Returns:\n        NormativePredictions: Object containing the predictions and evaluation\n        metrics.\n    \"\"\"\n    # Run extended predictions\n    return self.predict(test_covariates=new_data).evaluate_predictions(\n        variable_of_interest=new_data[self.spec.variable_of_interest].to_numpy(),\n        train_mean=self.model_params[\"mean_VOI\"],\n        train_std=self.model_params[\"std_VOI\"],\n        n_params=self.model_params[\"n_params\"],\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.fit","title":"<code>fit(train_data: pd.DataFrame, *, save_directory: Path | None = None, progress_bar: bool = True) -&gt; None</code>","text":"<p>Fit the normative model to the training data.</p> <p>This method implements the fitting logic for the normative model based on the provided training data and model specification.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the training data. It must include the variable of interest and all specified covariates.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None A path to a directory to save the model. If provided, the fitted model will be saved to this path.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>bool If True, display a progress bar during fitting. Defaults to True.</p> <code>True</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    train_data: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    progress_bar: bool = True,\n) -&gt; None:\n    \"\"\"\n    Fit the normative model to the training data.\n\n    This method implements the fitting logic for the normative model\n    based on the provided training data and model specification.\n\n    Args:\n        train_data: pd.DataFrame\n            DataFrame containing the training data. It must include the variable\n            of interest and all specified covariates.\n        save_directory: Path | None\n            A path to a directory to save the model. If provided, the fitted model\n            will be saved to this path.\n        progress_bar: bool\n            If True, display a progress bar during fitting. Defaults to True.\n    \"\"\"\n    # Validation checks\n    self._validate_model()\n    self._validate_dataframe_for_fitting(train_data)\n\n    # A dictionary to hold the model parameters after fitting\n    self.model_params = {}\n\n    # Data preparation\n    model_coords = self._build_model_coordinates(\n        observations=np.arange(train_data.shape[0]),\n    )\n\n    # Fitting logic\n    with pm.Model(coords=model_coords) as self._model:\n        # Standardize the variable of interest, and store mean and std\n        # This ensures that the model is not sensitive to the scale of the variable\n        variable_of_interest = train_data[self.spec.variable_of_interest].to_numpy()\n        self.model_params[\"mean_VOI\"] = variable_of_interest.mean()\n        self.model_params[\"std_VOI\"] = variable_of_interest.std()\n        standardized_voi = (\n            variable_of_interest - self.model_params[\"mean_VOI\"]\n        ) / self.model_params[\"std_VOI\"]\n        self.model_params[\"sample_size\"] = variable_of_interest.shape[0]\n\n        # A dictionary for precomputed bspline basis functions\n        spline_bases: dict[str, npt.NDArray[np.floating[Any]]] = {}\n\n        # A dictionary for factorized categories\n        category_indices: dict[str, npt.NDArray[np.integer[Any]]] = {}\n\n        # Initialize parameter count\n        self.model_params[\"n_params\"] = 0\n\n        # Model the mean of the variable of interest\n        mean_effects = self._model_all_mean_effects(\n            train_data,\n            spline_bases,\n            category_indices,\n        )\n\n        # Model the variance of the variable of interest\n        variance_effects = self._model_all_variance_effects(\n            train_data,\n            spline_bases,\n            category_indices,\n        )\n\n        # Combine all mean and variance effects\n        self._combine_all_effects(mean_effects, variance_effects, standardized_voi)\n\n        # Fit the model using ADVI\n        self._fit_model_with_advi(progress_bar=progress_bar)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(Path(save_directory))\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.from_dataframe","title":"<code>from_dataframe(model_type: ModelType, dataframe: pd.DataFrame, variable_of_interest: str, numerical_covariates: list[str] | None = None, categorical_covariates: list[str] | None = None, batch_covariates: list[str] | None = None, nonlinear_covariates: list[str] | None = None, influencing_mean: list[str] | None = None, influencing_variance: list[str] | None = None, spline_kwargs: dict[str, Any] | None = None) -&gt; DirectNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize a normative model from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>ModelType Type of the model to create, either \"HBR\" (Hierarchical Bayesian Regression) or \"BLR\" (Bayesian Linear Regression).</p> required <code>dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the data.</p> required <code>variable_of_interest</code> <code>str</code> <p>str Name of the target variable to model.</p> required <code>numerical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of numerical covariate names.</p> <code>None</code> <code>categorical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of categorical covariate names.</p> <code>None</code> <code>batch_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of batch covariate names which should also be included in categorical_covariates.</p> <code>None</code> <code>nonlinear_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names to be modeled as nonlinear effects. These should also be included in numerical_covariates.</p> <code>None</code> <code>influencing_mean</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the mean of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>influencing_variance</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the variance of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>spline_kwargs</code> <code>dict[str, Any] | None</code> <p>dict Additional keyword arguments for spline specification, such as <code>df</code>, <code>degree</code>, and <code>knots</code>. These are passed to the <code>create_spline_spec</code> method to create spline specifications for nonlinear covariates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirectNormativeModel</code> <p>DirectNormativeModel An instance of DirectNormativeModel initialized with the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef from_dataframe(\n    cls,\n    model_type: ModelType,\n    dataframe: pd.DataFrame,\n    variable_of_interest: str,\n    numerical_covariates: list[str] | None = None,\n    categorical_covariates: list[str] | None = None,\n    batch_covariates: list[str] | None = None,\n    nonlinear_covariates: list[str] | None = None,\n    influencing_mean: list[str] | None = None,\n    influencing_variance: list[str] | None = None,\n    spline_kwargs: dict[str, Any] | None = None,\n) -&gt; DirectNormativeModel:\n    \"\"\"\n    Initialize a normative model from a pandas DataFrame.\n\n    Args:\n        model_type: ModelType\n            Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n            Regression) or \"BLR\" (Bayesian Linear Regression).\n        dataframe: pd.DataFrame\n            DataFrame containing the data.\n        variable_of_interest: str\n            Name of the target variable to model.\n        numerical_covariates: list[str] | None\n            List of numerical covariate names.\n        categorical_covariates: list[str] | None\n            List of categorical covariate names.\n        batch_covariates: list[str] | None\n            List of batch covariate names which should also be included in\n            categorical_covariates.\n        nonlinear_covariates: list[str] | None\n            List of covariate names to be modeled as nonlinear effects.\n            These should also be included in numerical_covariates.\n        influencing_mean: list[str] | None\n            List of covariate names that influence the mean of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        influencing_variance: list[str] | None\n            List of covariate names that influence the variance of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        spline_kwargs: dict\n            Additional keyword arguments for spline specification, such as\n            `df`, `degree`, and `knots`. These are passed to the\n            `create_spline_spec` method to create spline specifications for\n            nonlinear covariates.\n\n    Returns:\n        DirectNormativeModel\n            An instance of DirectNormativeModel initialized with the provided data.\n    \"\"\"\n    # Set default values for optional parameters\n    numerical_covariates = numerical_covariates or []\n    categorical_covariates = categorical_covariates or []\n    batch_covariates = batch_covariates or []\n    nonlinear_covariates = nonlinear_covariates or []\n    influencing_mean = influencing_mean or []\n    influencing_variance = influencing_variance or []\n    spline_kwargs = spline_kwargs or {}\n\n    # Validity checks for input parameters\n    cls._validate_init_args(\n        model_type,\n        variable_of_interest,\n        numerical_covariates,\n        categorical_covariates,\n        batch_covariates,\n        nonlinear_covariates,\n    )\n    utils.general.validate_dataframe(\n        dataframe,\n        [variable_of_interest, *numerical_covariates, *categorical_covariates],\n    )\n\n    # Create an instance of the class\n    self = cls(\n        spec=NormativeModelSpec(\n            variable_of_interest=variable_of_interest,\n            covariates=[],\n            influencing_mean=influencing_mean,\n            influencing_variance=influencing_variance,\n        ),\n        batch_covariates=batch_covariates,\n    )\n\n    # Populate the spline_kwargs with defaults if not provided\n    spline_kwargs[\"df\"] = spline_kwargs.get(\"df\", self.defaults[\"spline_df\"])\n    spline_kwargs[\"degree\"] = spline_kwargs.get(\n        \"degree\",\n        self.defaults[\"spline_degree\"],\n    )\n    spline_kwargs[\"extrapolation_factor\"] = spline_kwargs.get(\n        \"extrapolation_factor\",\n        self.defaults[\"spline_extrapolation_factor\"],\n    )\n\n    # Start building the model specification\n    # Add categorical covariates\n    for cov_name in categorical_covariates:\n        hierarchical = False\n        if cov_name in batch_covariates and model_type == \"HBR\":\n            hierarchical = True\n        self.spec.covariates.append(\n            CovariateSpec(\n                name=cov_name,\n                cov_type=\"categorical\",\n                categories=dataframe[cov_name].unique(),\n                hierarchical=hierarchical,\n            ),\n        )\n    for cov_name in numerical_covariates:\n        if cov_name not in nonlinear_covariates:\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"numerical\",\n                    effect=\"linear\",\n                    moments=(\n                        dataframe[cov_name].mean(),\n                        dataframe[cov_name].std(),\n                    ),\n                ),\n            )\n        else:\n            self.spec.covariates.append(\n                CovariateSpec(\n                    name=cov_name,\n                    cov_type=\"numerical\",\n                    effect=\"spline\",\n                    spline_spec=SplineSpec.create_spline_spec(\n                        dataframe[cov_name],\n                        **spline_kwargs,\n                    ),\n                ),\n            )\n    return self\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.load_model","title":"<code>load_model(directory: Path, *, load_posterior: bool = False) -&gt; DirectNormativeModel</code>  <code>classmethod</code>","text":"<p>Load the model and its posterior from a directory. The model will be loaded from a subdirectory named 'saved_model'.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to the directory containing the model.</p> required <code>load_posterior</code> <code>bool</code> <p>bool (default=False) If True, load the model's posterior trace from the saved inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    directory: Path,\n    *,\n    load_posterior: bool = False,\n) -&gt; DirectNormativeModel:\n    \"\"\"\n    Load the model and its posterior from a directory.\n    The model will be loaded from a subdirectory named 'saved_model'.\n\n    Args:\n        directory: Path\n            Path to the directory containing the model.\n        load_posterior: bool (default=False)\n            If True, load the model's posterior trace from the saved inference data.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"saved_model\",\n    )\n\n    # Load the saved model dict\n    model_dict = joblib.load(saved_model_dir / \"model_dict.joblib\")\n\n    # Create an instance of the class\n    instance = cls(\n        spec=model_dict[\"spec\"],\n        batch_covariates=model_dict[\"batch_covariates\"],\n    )\n\n    # Set the attributes from the loaded model dictionary\n    instance.defaults.update(model_dict[\"defaults\"])\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n        if load_posterior:\n            instance.model_inference_data = az.from_netcdf(  # type: ignore[no-untyped-call]\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n\n    return instance\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.predict","title":"<code>predict(test_covariates: pd.DataFrame, *, extended: bool = False, model_params: dict[str, Any] | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict normative moments (mean, std) for new data using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates.</p> required <code>extended</code> <code>bool</code> <p>bool If True, return additional stats such as log-likelihood, centiles, etc. Note that extended predictions require variable_of_interest to be provided in the test_covariates DataFrame.</p> <code>False</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted moments (mean, std) for the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    test_covariates: pd.DataFrame,\n    *,\n    extended: bool = False,\n    model_params: dict[str, Any] | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict normative moments (mean, std) for new data using the fitted model.\n\n    Args:\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n        extended: bool\n            If True, return additional stats such as log-likelihood, centiles, etc.\n            Note that extended predictions require variable_of_interest to be\n            provided in the test_covariates DataFrame.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n\n    Returns:\n        NormativePredictions: Object containing the predicted moments (mean, std)\n            for the variable of interest.\n    \"\"\"\n    # Validate the new data\n    validation_columns = [cov.name for cov in self.spec.covariates]\n    if extended:\n        validation_columns.append(self.spec.variable_of_interest)\n    utils.general.validate_dataframe(test_covariates, validation_columns)\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # Calculate mean and variance effects and store in the predictions object\n    predictions = NormativePredictions(\n        {\n            \"mu_estimate\": self._predict_mu(test_covariates, model_params),\n            \"std_estimate\": self._predict_std(test_covariates, model_params),\n        },\n    )\n\n    # Check if extended predictions are requested\n    if extended:\n        # Add extended statistics to predictions (e.g. centiles, log loss, etc.)\n        predictions.extend_predictions(\n            variable_of_interest=test_covariates[\n                self.spec.variable_of_interest\n            ].to_numpy(),\n        )\n\n    return predictions\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.DirectNormativeModel.save_model","title":"<code>save_model(directory: Path, *, save_posterior: bool = False) -&gt; None</code>","text":"<p>Save the fitted model and it's posterior to a directory. The model will be saved in a subdirectory named 'saved_model'. If this directory is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Path to a directory to save the model.</p> required <code>save_posterior</code> <code>bool</code> <p>bool (default=False) If True, save the model's posterior trace inference data.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path, *, save_posterior: bool = False) -&gt; None:\n    \"\"\"\n    Save the fitted model and it's posterior to a directory.\n    The model will be saved in a subdirectory named 'saved_model'.\n    If this directory is not empty, an error is raised.\n\n    Args:\n        directory: Path\n            Path to a directory to save the model.\n        save_posterior: bool (default=False)\n            If True, save the model's posterior trace inference data.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(directory, \"saved_model\")\n\n    model_dict = {\n        \"spec\": self.spec,\n        \"batch_covariates\": self.batch_covariates,\n        \"defaults\": self.defaults,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n        if hasattr(self, \"model_inference_data\") and save_posterior:\n            self.model_inference_data.to_netcdf(\n                saved_model_dir / \"model_inference_data.nc\",\n            )\n    joblib.dump(model_dict, saved_model_dir / \"model_dict.joblib\")\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativeModelSpec","title":"<code>NormativeModelSpec</code>  <code>dataclass</code>","text":"<p>General specification of a normative model.</p> <p>Attributes:</p> Name Type Description <code>variable_of_interest</code> <code>str</code> <p>str Name of the target variable to model (e.g., \"thickness\").</p> <code>covariates</code> <code>list[CovariateSpec]</code> <p>list[CovariateSpec] Listing all model covariates and specifying how each covariate is modeled.</p> <code>influencing_mean</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the mean of the variable of interest.</p> <code>influencing_variance</code> <code>list[str]</code> <p>list[str] List of covariate names that influence the variance of the variable of interest.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass NormativeModelSpec:\n    \"\"\"\n    General specification of a normative model.\n\n    Attributes:\n        variable_of_interest: str\n            Name of the target variable to model (e.g., \"thickness\").\n        covariates: list[CovariateSpec]\n            Listing all model covariates and specifying how each covariate is modeled.\n        influencing_mean: list[str]\n            List of covariate names that influence the mean of the variable of interest.\n        influencing_variance: list[str]\n            List of covariate names that influence the variance of the variable of\n            interest.\n    \"\"\"\n\n    variable_of_interest: str\n    covariates: list[CovariateSpec]\n    influencing_mean: list[str]\n    influencing_variance: list[str]\n\n    def __post_init__(self) -&gt; None:\n        if not isinstance(self.variable_of_interest, str):\n            err = \"variable_of_interest must be a string.\"\n            raise TypeError(err)\n        if not isinstance(self.covariates, list):\n            err = \"covariates must be a list of CovariateSpec instances.\"\n            raise TypeError(err)\n        if not all(isinstance(cov, CovariateSpec) for cov in self.covariates):\n            err = \"All items in covariates must be CovariateSpec instances.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_mean, list):\n            err = \"influencing_mean must be a list of covariate names.\"\n            raise TypeError(err)\n        if not isinstance(self.influencing_variance, list):\n            err = \"influencing_variance must be a list of covariate names.\"\n            raise TypeError(err)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativePredictions","title":"<code>NormativePredictions</code>  <code>dataclass</code>","text":"<p>Container for the results of model.predict() function.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>dict[str, NDArray[floating[Any]]]</code> <p>dict Dictionary containing the model's predictions, including - Predictions of mean (mu_estimate). - Predictions of standard deviation (std_estimate). - [Optional] The observed variable of interest (the name of which is   provided in the function argument). - [Optional] Additional evaluation metrics for the predictions.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass NormativePredictions:\n    \"\"\"\n    Container for the results of model.predict() function.\n\n    Attributes:\n        predictions: dict\n            Dictionary containing the model's predictions, including\n            - Predictions of mean (mu_estimate).\n            - Predictions of standard deviation (std_estimate).\n            - [Optional] The observed variable of interest (the name of which is\n              provided in the function argument).\n            - [Optional] Additional evaluation metrics for the predictions.\n    \"\"\"\n\n    predictions: dict[str, npt.NDArray[np.floating[Any]]]\n    evaluations: dict[str, npt.NDArray[np.floating[Any]] | float] = field(\n        default_factory=dict,\n    )\n\n    def extend_predictions(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Extend the NormativePredictions (predictions dictionary) with additional\n        statistics.\n\n        Args:\n            variable_of_interest: np.ndarray\n                The observed values for the variable(s) of interest.\n\n        Returns:\n            NormativePredictions\n                Extended NormativePredictions with additional statistics.\n        \"\"\"\n        self.predictions[\"z-score\"] = (\n            variable_of_interest - self.predictions[\"mu_estimate\"]\n        ) / self.predictions[\"std_estimate\"]\n        self.predictions[\"log-likelihood\"] = (\n            utils.stats.compute_censored_log_likelihood(\n                variable_of_interest,\n                self.predictions[\"mu_estimate\"],\n                self.predictions[\"std_estimate\"],\n            )\n        )\n        self.predictions[\"centiles\"] = utils.stats.compute_centiles_from_z_scores(\n            self.predictions[\"z-score\"],\n        )\n\n        self.predictions[\"variable_of_interest\"] = variable_of_interest\n\n        return self\n\n    def evaluate_predictions(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n        train_mean: npt.NDArray[np.floating[Any]],\n        train_std: npt.NDArray[np.floating[Any]],\n        n_params: int,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the predictions against the observed variable of interest.\n\n        This function computes a battery of evaluation metrics implemented\n        in `snm.utils.metrics`. Namely the evaluations include:\n            - Mean Absolute Error (MAE)\n            - Mean Squared Error (MSE)\n            - Root Mean Squared Error (RMSE)\n            - Mean Absolute Percentage Error (MAPE)\n            - R-squared\n            - Explained Variance Score\n            - Mean Standardized Log Loss (MSLL)\n            - Bayesian Information Criterion (BIC)\n\n        Args:\n            variable_of_interest: np.ndarray\n                The observed values for the variable(s) of interest.\n            train_mean: np.ndarray\n                Mean(s) of the variable(s) of interest from the training data.\n            train_std: np.ndarray\n                Standard deviation(s) of the variable(s) of interest from the training\n                data.\n            n_params: int\n                Number of free parameters in the model.\n\n        Returns:\n            NormativePredictions\n                Object containing the evaluation results.\n        \"\"\"\n        self.extend_predictions(variable_of_interest)\n        # Mean Absolute Error (MAE)\n        self.evaluations[\"MAE\"] = utils.metrics.compute_mae(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Squared Error (MSE)\n        self.evaluations[\"MSE\"] = utils.metrics.compute_mse(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Root Mean Squared Error (RMSE)\n        self.evaluations[\"RMSE\"] = utils.metrics.compute_rmse(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Absolute Percentage Error (MAPE)\n        self.evaluations[\"MAPE\"] = utils.metrics.compute_mape(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # R-squared\n        self.evaluations[\"R-squared\"] = utils.metrics.compute_r2(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Explained Variance Score\n        self.evaluations[\"Explained Variance\"] = utils.metrics.compute_expv(\n            y=self.predictions[\"variable_of_interest\"],\n            y_pred=self.predictions[\"mu_estimate\"],\n        )\n        # Mean Standardized Log Loss (MSLL)\n        self.evaluations[\"MSLL\"] = utils.metrics.compute_msll(\n            model_log_likelihoods=self.predictions[\"log-likelihood\"],\n            baseline_log_likelihoods=utils.stats.compute_censored_log_likelihood(\n                self.predictions[\"variable_of_interest\"],\n                train_mean,\n                train_std,\n            ),\n        )\n        # Bayesian Information Criterion (BIC)\n        self.evaluations[\"BIC\"] = utils.metrics.compute_bic(\n            model_log_likelihoods=self.predictions[\"log-likelihood\"],\n            n_params=n_params,\n            n_samples=self.predictions[\"variable_of_interest\"].shape[0],\n        )\n\n        return self\n\n    def to_array(self, keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Return prediction results as a list of NumPy arrays.\n\n        Args:\n            keys: list[str]\n                Optional list of keys to return.\n                Defaults to [\"mu_estimate\", \"std_estimate\"].\n\n        Returns:\n            list[np.ndarray]\n                NumPy arrays for the requested predictions\n        \"\"\"\n        keys = keys or [\"mu_estimate\", \"std_estimate\"]\n        return np.array([self.predictions[key] for key in keys])\n\n    def to_dataframe(\n        self,\n        index: pd.Index[Any] | list[Any] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Return prediction results as a DataFrame.\n\n        Args:\n            index: pd.Index | list | None\n                Optional index for the DataFrame (defaults to None)\n\n        Returns:\n            pd.DataFrame\n                DataFrame containing the predictions\n        \"\"\"\n        predictions = self.predictions.copy()\n        # Flatten the predictions dictionary if multiple queries are predicted\n        for key in predictions:\n            if predictions[key].ndim &gt; 1:\n                if predictions[key].shape[1] == 1:\n                    predictions[key] = predictions[key].flatten()\n                else:\n                    for i in range(predictions[key].shape[1]):\n                        predictions[f\"{key}_{i + 1}\"] = predictions[key][:, i]\n                    # delete the key\n                    del predictions[key]\n\n        # Make a new DataFrame for the predictions dictionary\n        return pd.DataFrame(predictions, index=index)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativePredictions.evaluate_predictions","title":"<code>evaluate_predictions(variable_of_interest: npt.NDArray[np.floating[Any]], train_mean: npt.NDArray[np.floating[Any]], train_std: npt.NDArray[np.floating[Any]], n_params: int) -&gt; NormativePredictions</code>","text":"<p>Evaluate the predictions against the observed variable of interest.</p> <p>This function computes a battery of evaluation metrics implemented in <code>snm.utils.metrics</code>. Namely the evaluations include:     - Mean Absolute Error (MAE)     - Mean Squared Error (MSE)     - Root Mean Squared Error (RMSE)     - Mean Absolute Percentage Error (MAPE)     - R-squared     - Explained Variance Score     - Mean Standardized Log Loss (MSLL)     - Bayesian Information Criterion (BIC)</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The observed values for the variable(s) of interest.</p> required <code>train_mean</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean(s) of the variable(s) of interest from the training data.</p> required <code>train_std</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Standard deviation(s) of the variable(s) of interest from the training data.</p> required <code>n_params</code> <code>int</code> <p>int Number of free parameters in the model.</p> required <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>NormativePredictions Object containing the evaluation results.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate_predictions(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n    train_mean: npt.NDArray[np.floating[Any]],\n    train_std: npt.NDArray[np.floating[Any]],\n    n_params: int,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the predictions against the observed variable of interest.\n\n    This function computes a battery of evaluation metrics implemented\n    in `snm.utils.metrics`. Namely the evaluations include:\n        - Mean Absolute Error (MAE)\n        - Mean Squared Error (MSE)\n        - Root Mean Squared Error (RMSE)\n        - Mean Absolute Percentage Error (MAPE)\n        - R-squared\n        - Explained Variance Score\n        - Mean Standardized Log Loss (MSLL)\n        - Bayesian Information Criterion (BIC)\n\n    Args:\n        variable_of_interest: np.ndarray\n            The observed values for the variable(s) of interest.\n        train_mean: np.ndarray\n            Mean(s) of the variable(s) of interest from the training data.\n        train_std: np.ndarray\n            Standard deviation(s) of the variable(s) of interest from the training\n            data.\n        n_params: int\n            Number of free parameters in the model.\n\n    Returns:\n        NormativePredictions\n            Object containing the evaluation results.\n    \"\"\"\n    self.extend_predictions(variable_of_interest)\n    # Mean Absolute Error (MAE)\n    self.evaluations[\"MAE\"] = utils.metrics.compute_mae(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Squared Error (MSE)\n    self.evaluations[\"MSE\"] = utils.metrics.compute_mse(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Root Mean Squared Error (RMSE)\n    self.evaluations[\"RMSE\"] = utils.metrics.compute_rmse(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Absolute Percentage Error (MAPE)\n    self.evaluations[\"MAPE\"] = utils.metrics.compute_mape(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # R-squared\n    self.evaluations[\"R-squared\"] = utils.metrics.compute_r2(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Explained Variance Score\n    self.evaluations[\"Explained Variance\"] = utils.metrics.compute_expv(\n        y=self.predictions[\"variable_of_interest\"],\n        y_pred=self.predictions[\"mu_estimate\"],\n    )\n    # Mean Standardized Log Loss (MSLL)\n    self.evaluations[\"MSLL\"] = utils.metrics.compute_msll(\n        model_log_likelihoods=self.predictions[\"log-likelihood\"],\n        baseline_log_likelihoods=utils.stats.compute_censored_log_likelihood(\n            self.predictions[\"variable_of_interest\"],\n            train_mean,\n            train_std,\n        ),\n    )\n    # Bayesian Information Criterion (BIC)\n    self.evaluations[\"BIC\"] = utils.metrics.compute_bic(\n        model_log_likelihoods=self.predictions[\"log-likelihood\"],\n        n_params=n_params,\n        n_samples=self.predictions[\"variable_of_interest\"].shape[0],\n    )\n\n    return self\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativePredictions.extend_predictions","title":"<code>extend_predictions(variable_of_interest: npt.NDArray[np.floating[Any]]) -&gt; NormativePredictions</code>","text":"<p>Extend the NormativePredictions (predictions dictionary) with additional statistics.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The observed values for the variable(s) of interest.</p> required <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>NormativePredictions Extended NormativePredictions with additional statistics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def extend_predictions(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n) -&gt; NormativePredictions:\n    \"\"\"\n    Extend the NormativePredictions (predictions dictionary) with additional\n    statistics.\n\n    Args:\n        variable_of_interest: np.ndarray\n            The observed values for the variable(s) of interest.\n\n    Returns:\n        NormativePredictions\n            Extended NormativePredictions with additional statistics.\n    \"\"\"\n    self.predictions[\"z-score\"] = (\n        variable_of_interest - self.predictions[\"mu_estimate\"]\n    ) / self.predictions[\"std_estimate\"]\n    self.predictions[\"log-likelihood\"] = (\n        utils.stats.compute_censored_log_likelihood(\n            variable_of_interest,\n            self.predictions[\"mu_estimate\"],\n            self.predictions[\"std_estimate\"],\n        )\n    )\n    self.predictions[\"centiles\"] = utils.stats.compute_centiles_from_z_scores(\n        self.predictions[\"z-score\"],\n    )\n\n    self.predictions[\"variable_of_interest\"] = variable_of_interest\n\n    return self\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativePredictions.to_array","title":"<code>to_array(keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Return prediction results as a list of NumPy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str] | None</code> <p>list[str] Optional list of keys to return. Defaults to [\"mu_estimate\", \"std_estimate\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>list[np.ndarray] NumPy arrays for the requested predictions</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def to_array(self, keys: list[str] | None = None) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Return prediction results as a list of NumPy arrays.\n\n    Args:\n        keys: list[str]\n            Optional list of keys to return.\n            Defaults to [\"mu_estimate\", \"std_estimate\"].\n\n    Returns:\n        list[np.ndarray]\n            NumPy arrays for the requested predictions\n    \"\"\"\n    keys = keys or [\"mu_estimate\", \"std_estimate\"]\n    return np.array([self.predictions[key] for key in keys])\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.NormativePredictions.to_dataframe","title":"<code>to_dataframe(index: pd.Index[Any] | list[Any] | None = None) -&gt; pd.DataFrame</code>","text":"<p>Return prediction results as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index[Any] | list[Any] | None</code> <p>pd.Index | list | None Optional index for the DataFrame (defaults to None)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the predictions</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def to_dataframe(\n    self,\n    index: pd.Index[Any] | list[Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return prediction results as a DataFrame.\n\n    Args:\n        index: pd.Index | list | None\n            Optional index for the DataFrame (defaults to None)\n\n    Returns:\n        pd.DataFrame\n            DataFrame containing the predictions\n    \"\"\"\n    predictions = self.predictions.copy()\n    # Flatten the predictions dictionary if multiple queries are predicted\n    for key in predictions:\n        if predictions[key].ndim &gt; 1:\n            if predictions[key].shape[1] == 1:\n                predictions[key] = predictions[key].flatten()\n            else:\n                for i in range(predictions[key].shape[1]):\n                    predictions[f\"{key}_{i + 1}\"] = predictions[key][:, i]\n                # delete the key\n                del predictions[key]\n\n    # Make a new DataFrame for the predictions dictionary\n    return pd.DataFrame(predictions, index=index)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel","title":"<code>SpectralNormativeModel</code>  <code>dataclass</code>","text":"<p>Spectral normative model implementation.</p> <p>This class implements the spectral normative modeling approach, which utilizes a base direct model to generalize normative modeling to any arbitrary variable of interest reconstructed from a graph spectral embedding. It can be used to fit a normative model to high-dimensional data and predict normative centiles for arbitrary variables of interest.</p> <p>Attributes:</p> Name Type Description <code>eigenmode_basis</code> <code>EigenmodeBasis</code> <p>utils.gsp.EigenmodeBasis The eigenmode basis used for spectral normative modeling. This should be an instance of utils.gsp.EigenmodeBasis.</p> <code>base_model</code> <code>DirectNormativeModel</code> <p>DirectNormativeModel The base (direct) normative model used for spectral normative modeling.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass SpectralNormativeModel:\n    \"\"\"\n    Spectral normative model implementation.\n\n    This class implements the spectral normative modeling approach, which\n    utilizes a base direct model to generalize normative modeling to any\n    arbitrary variable of interest reconstructed from a graph spectral\n    embedding. It can be used to fit a normative model to high-dimensional\n    data and predict normative centiles for arbitrary variables of interest.\n\n    Attributes:\n        eigenmode_basis: utils.gsp.EigenmodeBasis\n            The eigenmode basis used for spectral normative modeling. This should be an\n            instance of utils.gsp.EigenmodeBasis.\n        base_model: DirectNormativeModel\n            The base (direct) normative model used for spectral normative modeling.\n    \"\"\"\n\n    eigenmode_basis: utils.gsp.EigenmodeBasis\n    base_model: DirectNormativeModel\n\n    @classmethod\n    def build_from_dataframe(\n        cls,\n        eigenmode_basis: utils.gsp.EigenmodeBasis,\n        model_type: ModelType,\n        covariates_dataframe: pd.DataFrame,\n        numerical_covariates: list[str] | None = None,\n        categorical_covariates: list[str] | None = None,\n        batch_covariates: list[str] | None = None,\n        nonlinear_covariates: list[str] | None = None,\n        influencing_mean: list[str] | None = None,\n        influencing_variance: list[str] | None = None,\n        spline_kwargs: dict[str, Any] | None = None,\n    ) -&gt; SpectralNormativeModel:\n        \"\"\"\n        Initialize SNM with an eigenmode basis and a base direct model built from a\n        pandas DataFrame containing all covariates.\n\n        This uses the from_dataframe method of the DirectNormativeModel class\n        to populate the direct model specification of SNM. Given that SNM does not\n        require a fixed variable of interest, this method assigns a dummy name\n        to the variable_of_interest parameter of the DirectNormativeModel. As such,\n        the provided dataframe should not contain a column with \"dummy_VOI\" as name.\n\n        Essentially, the provided dataframe should contain all covariates as columns.\n\n        Args:\n            eigenmode_basis: utils.gsp.EigenmodeBasis\n                The eigenmode basis to be used for spectral normative modeling.\n            model_type: ModelType\n                Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n                Regression) or \"BLR\" (Bayesian Linear Regression).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the data for all covariates and all samples.\n            numerical_covariates: list[str] | None\n                List of numerical covariate names.\n            categorical_covariates: list[str] | None\n                List of categorical covariate names.\n            batch_covariates: list[str] | None\n                List of batch covariate names which should also be included in\n                categorical_covariates.\n            nonlinear_covariates: list[str] | None\n                List of covariate names to be modeled as nonlinear effects.\n                These should also be included in numerical_covariates.\n            influencing_mean: list[str] | None\n                List of covariate names that influence the mean of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            influencing_variance: list[str] | None\n                List of covariate names that influence the variance of the variable\n                of interest. These should be included in either numerical_covariates\n                or categorical_covariates.\n            spline_kwargs: dict\n                Additional keyword arguments for spline specification, such as\n                `df`, `degree`, and `knots`. These are passed to the\n                `create_spline_spec` method to create spline specifications for\n                nonlinear covariates.\n\n        Returns:\n            SpectralNormativeModel\n                An instance of SpectralNormativeModel with base model specs initialized\n                based on the provided data.\n        \"\"\"\n        # Add a dummy variable of interest to the covariates_dataframe\n        covariates_dataframe = covariates_dataframe.copy()\n        covariates_dataframe[\"dummy_VOI\"] = 0.0  # Dummy variable of interest\n        # Specify the base model from the dataframe\n        return cls(\n            eigenmode_basis=eigenmode_basis,\n            base_model=DirectNormativeModel.from_dataframe(\n                model_type=model_type,\n                dataframe=covariates_dataframe,\n                variable_of_interest=\"dummy_VOI\",  # Dummy variable of interest\n                numerical_covariates=(numerical_covariates or []),\n                categorical_covariates=(categorical_covariates or []),\n                batch_covariates=(batch_covariates or []),\n                nonlinear_covariates=(nonlinear_covariates or []),\n                influencing_mean=(influencing_mean or []),\n                influencing_variance=(influencing_variance or []),\n                spline_kwargs=(spline_kwargs or {}),\n            ),\n        )\n\n    def save_model(self, directory: Path) -&gt; None:\n        \"\"\"\n        Save the fitted spectral normative model to the specified directory.\n\n        Args:\n            directory: Path\n                Directory to save the fitted model. A subdirectory named\n                \"spectral_normative_model\" will be created within this directory.\n        \"\"\"\n        # Prepare the save directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.prepare_save_directory(\n            directory,\n            \"spectral_normative_model\",\n        )\n\n        # Save the model\n        model_dict = {\n            \"spec\": self.base_model.spec,\n            \"batch_covariates\": self.base_model.batch_covariates,\n            \"defaults\": self.base_model.defaults,\n            \"eigenmode_basis\": self.eigenmode_basis,\n        }\n        if hasattr(self, \"model_params\"):\n            model_dict[\"model_params\"] = self.model_params\n        joblib.dump(model_dict, saved_model_dir / \"spectral_model_dict.joblib\")\n\n    def _validate_fit_input(\n        self,\n        encoded_train_data: npt.NDArray[np.floating[Any]],\n        n_modes: int,\n    ) -&gt; None:\n        \"\"\"\n        Internal method to validate input data for fitting the spectral normative model.\n        \"\"\"\n        # Validate the input data\n        if not isinstance(encoded_train_data, np.ndarray):\n            err = \"encoded_train_data must be a numpy array.\"\n            raise TypeError(err)\n        if encoded_train_data.shape[1] &lt; n_modes:\n            err = f\"encoded_train_data must have at least {n_modes} columns (n_modes).\"\n            raise ValueError(err)\n        if self.eigenmode_basis.n_modes &lt; n_modes:\n            err = (\n                f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n                f\" modes, while {n_modes} were requested.\"\n            )\n            raise ValueError(err)\n\n    def identify_sparse_covariance_structure(\n        self,\n        data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        correlation_threshold: float,\n    ) -&gt; npt.NDArray[np.integer[Any]]:\n        \"\"\"\n        Identify the sparse cross-basis covariance structure in the phenotype.\n        This method analyzes the encoded phenotype to determine the covariance\n        pairs that need to be modeled.\n\n        Note: if the batches become too small, this estimate can become less stable\n        in which case it is recommended to provide the sparse covariance structure\n        to the model instead.\n\n        Args:\n            data: np.ndarray\n                The encoded training data representing the phenotype in the graph\n                frequency domain.\n            covariates_dataframe: pd.DataFrame\n                The DataFrame containing covariates for the samples. The batch\n                covariates will be used to group the samples and identify the\n                important covariance pairs.\n            correlation_threshold: float\n                The threshold to include covariance pairs if significantly correlated.\n                Should be between 0 and 1.\n\n        Returns:\n            np.ndarray:\n                A (N, 2) array: the rows and columns of the\n                identified sparse covariance structure.\n        \"\"\"\n        # Start with correlation structure across the whole sample\n        corr_max = utils.stats.compute_correlation_significance_by_fisher_z(\n            np.corrcoef(data.T),\n            n_samples=data.shape[0],\n            correlation_threshold=correlation_threshold,\n        )\n\n        # Now iterate over all batch covariates\n        for batch_effect in self.base_model.batch_covariates:\n            # For every batch re-evaluate correlations\n            for batch in pd.unique(covariates_dataframe[batch_effect]):\n                # Make a mask for the current batch\n                batch_mask = covariates_dataframe[batch_effect] == batch\n                # Update maximums\n                corr_max = np.maximum.reduce(\n                    [\n                        corr_max,\n                        utils.stats.compute_correlation_significance_by_fisher_z(\n                            np.corrcoef(data[batch_mask].T),\n                            n_samples=data[batch_mask].shape[0],\n                            correlation_threshold=correlation_threshold,\n                        ),\n                    ],\n                )\n\n        # Now compute the sparsity structure based on the resulting matrix\n        rows, cols = np.where(np.abs(corr_max) &gt; 0)\n\n        # Remove redundant and duplicate pairs\n        rows_lim = rows[rows &lt; cols]\n        cols_lim = cols[rows &lt; cols]\n\n        return np.array([rows_lim, cols_lim]).T\n\n    @staticmethod\n    def _is_valid_covariance_structure(\n        covariance_structure: npt.NDArray[np.integer[Any]] | float,\n    ) -&gt; bool:\n        \"\"\"\n        Verify the validity of the sparse covariance structure.\n        \"\"\"\n        # Check it's a 2D array with two columns\n        expected_ndims = 2\n        expected_ncols = 2\n        if not (\n            isinstance(covariance_structure, np.ndarray)\n            and covariance_structure.ndim == expected_ndims\n            and covariance_structure.shape[1] == expected_ncols\n        ):\n            return False\n        return np.issubdtype(covariance_structure.dtype, np.integer)\n\n    def fit_single_direct(\n        self,\n        variable_of_interest: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        return_model_params: bool = True,\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Fit a direct normative model for a single spectral eigenmode.\n        This method fits the base direct model to the provided variable of interest\n        and covariates dataframe, allowing for the model to be trained on a specific\n        eigenmode of the spectral embedding.\n\n        Args:\n            variable_of_interest: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a single eigenmode.\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n            return_model_params: bool\n                If True, return the fitted model parameters.\n\n        Returns:\n            dict:\n                If `return_model_params` is True, return the fitted model parameters\n                in a dictionary.\n        \"\"\"\n        # Prepare the data for fitting\n        train_data = covariates_dataframe.copy()\n        # Add the mode loading as the variable of interest\n        train_data[\"VOI\"] = variable_of_interest\n\n        # Instantiate a direct normative model from the base model\n        direct_model = DirectNormativeModel(\n            spec=NormativeModelSpec(\n                variable_of_interest=\"VOI\",  # Use the added VOI column\n                covariates=self.base_model.spec.covariates,\n                influencing_mean=self.base_model.spec.influencing_mean,\n                influencing_variance=self.base_model.spec.influencing_variance,\n            ),\n            batch_covariates=self.base_model.batch_covariates,\n            defaults=self.base_model.defaults,\n        )\n\n        # Fit the model silently\n        with utils.general.suppress_output():\n            direct_model.fit(\n                train_data=train_data,\n                save_directory=save_directory,\n                progress_bar=False,\n            )\n\n        # Return the fitted model parameters if requested\n        if return_model_params:\n            return direct_model.model_params\n\n        # If not returning model parameters, return None\n        return None\n\n    def fit_single_covariance(\n        self,\n        variable_of_interest_1: npt.NDArray[np.floating[Any]],\n        variable_of_interest_2: npt.NDArray[np.floating[Any]],\n        direct_model_params_1: dict[str, Any],\n        direct_model_params_2: dict[str, Any],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        save_directory: Path | None = None,\n        return_model_params: bool = True,\n        defaults_overwrite: dict[str, Any] | None = None,\n    ) -&gt; dict[str, Any] | None:\n        \"\"\"\n        Fit a covariance normative model between a single pair of eigenmodes.\n        This method fits a covariance model to the provided pair of variables\n        and covariates dataframe, considering the direct model fits for each\n        eigenmode, while allowing for the cross-eigenmode covariance to vary\n        normatively.\n\n        Args:\n            variable_of_interest_1: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a single eigenmode.\n            variable_of_interest_2: np.ndarray\n                The loading vector capturing the variance within training data that\n                corresponds to a second eigenmode.\n            direct_model_params_1: dict\n                The parameters of the direct model fitted to the first eigenmode.\n            direct_model_params_2: dict\n                The parameters of the direct model fitted to the second eigenmode.\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n            return_model_params: bool\n                If True, return the fitted model parameters.\n            defaults_overwrite: dict (default={})\n                Dictionary of default values to overwrite in the model fitting process.\n\n        Returns:\n            dict:\n                If `return_model_params` is True, return the fitted model parameters\n                in a dictionary.\n        \"\"\"\n        # Prepare the data for fitting\n        train_data = covariates_dataframe.copy()\n        # Add the respective mode loadings as the variables of interest\n        train_data[\"VOI_1\"] = variable_of_interest_1\n        train_data[\"VOI_2\"] = variable_of_interest_2\n        train_data[[\"VOI_1_mu_estimate\", \"VOI_1_std_estimate\"]] = (\n            self.base_model.predict(\n                train_data,\n                model_params=direct_model_params_1,\n            )\n            .to_array()\n            .T\n        )  # Add the direct model predictions\n        train_data[[\"VOI_2_mu_estimate\", \"VOI_2_std_estimate\"]] = (\n            self.base_model.predict(\n                train_data,\n                model_params=direct_model_params_2,\n            )\n            .to_array()\n            .T\n        )  # Add the direct model predictions\n\n        # Instantiate a covariance normative model from the base model\n        covariance_model = CovarianceNormativeModel.from_direct_model(\n            self.base_model,\n            variable_of_interest_1=\"VOI_1\",\n            variable_of_interest_2=\"VOI_2\",\n            defaults_overwrite=(defaults_overwrite or {}),\n        )\n\n        # Fit the model silently\n        with utils.general.suppress_output():\n            covariance_model.fit(\n                train_data=train_data,\n                save_directory=save_directory,\n                progress_bar=False,\n            )\n\n        # Return the fitted model parameters if requested\n        if return_model_params:\n            return covariance_model.model_params\n\n        # If not returning model parameters, return None\n        return None\n\n    def fit_all_direct(\n        self,\n        encoded_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_modes: int = -1,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Fit the direct models for all specified eigenmodes.\n\n        Args:\n            encoded_train_data: np.ndarray\n                Encoded training data as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_modes: int (default=-1)\n                Number of eigenmodes to fit the model to. If -1, all modes are\n                used. If a positive integer, only the first n_modes are used.\n                Note that the encoded_train_data and the eigenmode basis should have\n                at least n_modes columns/eigenvectors.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n        \"\"\"\n        # Setup the save directory if needed\n        if save_directory is not None:\n            save_directory = Path(save_directory)\n\n        # Fit the base direct model for each eigenmode using parallel processing\n        tasks = (\n            joblib.delayed(self.fit_single_direct)(\n                variable_of_interest=encoded_train_data[:, i],\n                covariates_dataframe=covariates_dataframe,\n                save_directory=(\n                    utils.general.ensure_dir(\n                        save_directory\n                        / \"spectral_normative_model\"\n                        / \"direct_models\"\n                        / f\"mode_{i + 1}\",\n                    )\n                    if save_directory is not None and save_separate\n                    else None\n                ),\n            )\n            for i in range(n_modes)\n        )\n        self.direct_model_params = list(\n            utils.parallel.ParallelTqdm(\n                n_jobs=n_jobs,\n                total_tasks=n_modes,\n                desc=\"Fitting direct models\",\n            )(tasks),  # pyright: ignore[reportCallIssue]\n        )\n\n    def fit_all_covariance(\n        self,\n        encoded_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Fit the direct models for all specified eigenmodes.\n\n        Args:\n            encoded_train_data: np.ndarray\n                Encoded training data as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n        \"\"\"\n        # Setup the save directory if needed\n        if save_directory is not None:\n            save_directory = Path(save_directory)\n\n        # Fit the base covariance models for selected eigenmode pairs in parallel\n        tasks = (\n            joblib.delayed(self.fit_single_covariance)(\n                variable_of_interest_1=encoded_train_data[\n                    :,\n                    self.sparse_covariance_structure[i, 0],\n                ],\n                variable_of_interest_2=encoded_train_data[\n                    :,\n                    self.sparse_covariance_structure[i, 1],\n                ],\n                direct_model_params_1=self.direct_model_params[\n                    self.sparse_covariance_structure[i, 0]\n                ],\n                direct_model_params_2=self.direct_model_params[\n                    self.sparse_covariance_structure[i, 1]\n                ],\n                covariates_dataframe=covariates_dataframe,\n                save_directory=(\n                    utils.general.ensure_dir(\n                        save_directory\n                        / \"spectral_normative_model\"\n                        / \"covariance_models\"\n                        / (\n                            f\"mode_{self.sparse_covariance_structure[i, 0] + 1},\"\n                            f\"mode_{self.sparse_covariance_structure[i, 1] + 1}\"\n                        ),\n                    )\n                    if save_directory is not None and save_separate\n                    else None\n                ),\n            )\n            for i in range(self.sparse_covariance_structure.shape[0])\n        )\n        self.covariance_model_params = utils.parallel.ParallelTqdm(\n            n_jobs=n_jobs,\n            total_tasks=self.sparse_covariance_structure.shape[0],\n            desc=\"Fitting covariance models\",\n        )(tasks)  # pyright: ignore[reportCallIssue]\n\n    def fit(\n        self,\n        encoded_train_data: npt.NDArray[np.floating[Any]],\n        covariates_dataframe: pd.DataFrame,\n        *,\n        n_modes: int = -1,\n        n_jobs: int = -1,\n        save_directory: Path | None = None,\n        save_separate: bool = False,\n        covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.25,\n    ) -&gt; None:\n        \"\"\"\n        Fit the spectral normative model to the provided encoded training data.\n\n        Args:\n            encoded_train_data: np.ndarray\n                Encoded training data as a numpy array (n_samples, n_modes).\n            covariates_dataframe: pd.DataFrame\n                DataFrame containing the covariates for the samples.\n                It must include all specified covariates in the model specification.\n            n_modes: int (default=-1)\n                Number of eigenmodes to fit the model to. If -1, all modes are\n                used. If a positive integer, only the first n_modes are used.\n                Note that the encoded_train_data and the eigenmode basis should have\n                at least n_modes columns/eigenvectors.\n            n_jobs: int (default=-1)\n                Number of parallel jobs to use for fitting the model. If -1, all\n                available CPU cores are used. If 1, no parallelization is used.\n            save_directory: Path | None\n                Directory to save the fitted model. If None, the model is not saved.\n                A subdirectory named \"spectral_normative_model\" will be created\n                within the specified save_directory.\n            save_separate: bool (default=False)\n                Whether to save the fitted direct model parameters separately for each\n                eigenmode as individual files. This is only applicable if\n                `save_directory` is provided.\n            covariance_structure: np.ndarray | float\n                Sparse covariance structure to use for the model fitting. If a\n                (2, n_pairs) array of row and column indices are provided, the model\n                will use this structure. If float, the model will estimate the\n                covariance structure based on the training data and the float value\n                will be used as the threshold to exclude small correlations to form a\n                sparse covariance structure.\n        \"\"\"\n        logger.info(\"Starting SNM model fitting:\")\n        # Evaluate the number of modes to fit\n        if n_modes == -1:\n            n_modes = self.eigenmode_basis.n_modes\n        # Validate the input data\n        if not isinstance(encoded_train_data, np.ndarray):\n            err = \"encoded_train_data must be a numpy array.\"\n            raise TypeError(err)\n        if encoded_train_data.shape[1] &lt; n_modes:\n            err = f\"encoded_train_data must have at least {n_modes} columns (n_modes).\"\n            raise ValueError(err)\n        if self.eigenmode_basis.n_modes &lt; n_modes:\n            err = (\n                f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n                f\" modes, while {n_modes} were requested.\"\n            )\n            raise ValueError(err)\n\n        # Setup the save directory if needed\n        if save_directory is not None:\n            # Prepare the save directory\n            save_directory = Path(save_directory)\n            utils.general.prepare_save_directory(\n                save_directory,\n                \"spectral_normative_model\",\n            )\n\n        logger.info(\"Step 1; direct models for each eigenmode (%s modes)\", n_modes)\n\n        self.fit_all_direct(\n            encoded_train_data=encoded_train_data,\n            covariates_dataframe=covariates_dataframe,\n            n_modes=n_modes,\n            n_jobs=n_jobs,\n            save_directory=save_directory,\n            save_separate=save_separate,\n        )\n\n        logger.info(\"Step 2; identify sparse covariance structure\")\n\n        # Identify sparse covariance structure if a float value is given\n        if isinstance(covariance_structure, float):\n            # Use trained models to compute z-scores\n            encoded_train_z_scores = np.array(\n                [\n                    self.base_model.predict(\n                        test_covariates=covariates_dataframe,\n                        model_params=self.direct_model_params[x],\n                    )\n                    .extend_predictions(\n                        variable_of_interest=encoded_train_data[:, x],\n                    )\n                    .predictions[\"z-score\"]\n                    for x in range(n_modes)\n                ],\n            ).T\n\n            self.sparse_covariance_structure = (\n                self.identify_sparse_covariance_structure(\n                    encoded_train_z_scores,\n                    covariates_dataframe,\n                    covariance_structure,\n                )\n            )\n        else:\n            self.sparse_covariance_structure = np.array(covariance_structure)\n\n        # Verify that the covariance structure is valid\n        if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n            err = \"Invalid sparse covariance structure.\"\n            raise ValueError(err)\n\n        # Model cross basis sparse covariance structure\n        logger.info(\n            \"Step 3; cross-eigenmode dependency modeling (%s pairs)\",\n            self.sparse_covariance_structure.shape[0],\n        )\n\n        self.fit_all_covariance(\n            encoded_train_data=encoded_train_data,\n            covariates_dataframe=covariates_dataframe,\n            n_jobs=n_jobs,\n            save_directory=save_directory,\n            save_separate=save_separate,\n        )\n\n        # Save SNM model parameters\n        self.model_params = {\n            \"n_modes\": n_modes,\n            \"sample_size\": encoded_train_data.shape[0],\n            \"direct_model_params\": self.direct_model_params,\n            \"sparse_covariance_structure\": self.sparse_covariance_structure,\n            \"covariance_model_params\": self.covariance_model_params,\n        }\n        if (self.direct_model_params[0] is not None) and (\n            \"n_params\" in self.direct_model_params[0]\n        ):\n            self.model_params[\"n_params\"] = self.direct_model_params[0][\"n_params\"]\n        else:\n            err = \"Direct model parameters are not valid.\"\n            raise ValueError(err)\n\n        # Save the model if a save path is provided\n        if save_directory is not None:\n            self.save_model(save_directory)\n\n    def _predict_from_spectral_estimates(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        eigenmode_mu_estimates: npt.NDArray[np.floating[Any]],\n        eigenmode_std_estimates: npt.NDArray[np.floating[Any]],\n        rho_estimates: npt.NDArray[np.floating[Any]],\n        test_covariates: pd.DataFrame,\n        model_params: dict[str, Any],\n        n_modes: int,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Internal method to predict only the mean and sigma for new data using the fitted\n        spectral moments.\n        \"\"\"\n        # Prepare the predictions\n        predictions_dict = {}\n        predictions_dict[\"mu_estimate\"] = eigenmode_mu_estimates @ encoded_query\n        # empty initialization of std estimates\n        predictions_dict[\"std_estimate\"] = predictions_dict[\"mu_estimate\"] * np.nan\n\n        # Load sparse covariance structure\n        row_indices = model_params[\"sparse_covariance_structure\"][:, 0]\n        col_indices = model_params[\"sparse_covariance_structure\"][:, 1]\n        # Select indices that are both within n_modes\n        corr_index_valid = (row_indices &lt; n_modes) &amp; (col_indices &lt; n_modes)\n\n        # Estimate query variance for each sample\n        for sample_idx in range(test_covariates.shape[0]):\n            # Build sparse correlation matrix\n            sparse_correlations = sparse.coo_matrix(\n                (\n                    rho_estimates[sample_idx, corr_index_valid],  # sparse data values\n                    (  # row, column indices\n                        row_indices[corr_index_valid],\n                        col_indices[corr_index_valid],\n                    ),\n                ),\n                shape=(n_modes, n_modes),\n            ).tocsr()\n            # Make it symmetric\n            sparse_correlations = sparse_correlations + sparse_correlations.T\n            # Set diagonal to 1\n            sparse_correlations.setdiag(np.array(1))\n            # Weight mode stds by encoding\n            weighted_mode_stds = (\n                np.asarray(\n                    eigenmode_std_estimates[sample_idx],\n                ).reshape(-1, 1)\n                * encoded_query\n            )\n            # Compute the variance estimate\n            predictions_dict[\"std_estimate\"][sample_idx] = np.sqrt(\n                np.sum(\n                    weighted_mode_stds * (sparse_correlations @ weighted_mode_stds),\n                    axis=0,\n                ),\n            )\n\n        # Create a the predictions object\n        return NormativePredictions(predictions=predictions_dict)\n\n    def predict(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        test_covariates: pd.DataFrame,\n        *,\n        extended: bool = False,\n        model_params: dict[str, Any] | None = None,\n        encoded_test_data: npt.NDArray[np.floating[Any]] | None = None,\n        n_modes: int | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Predict normative moments (mean, std) for new data using the fitted spectral\n        normative model.\n        Spectral normative modeling can estimate the normative distribution of any\n        variable of interest defined as a spatial query encoded in the latent low-pass\n        graph spectral space.\n\n        Args:\n            encoded_query: np.ndarray\n                Encoded query data defining the normative variable of interest.\n                Can be provided as:\n                - shape = (n_modes) for a single query vector\n                - shape = (n_modes, n_queries) for multiple queries predicted at once\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n            extended: bool (default: False)\n                If True, return additional stats such as log-likelihood, centiles, etc.\n                Note that extended predictions require encoded_test_data to be\n                provided in addition to the covariates.\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            encoded_test_data: np.ndarray | None\n                Optional encoded test data for the phenotype being modeled (only\n                required for extended predictions).\n                Expects a numpy array (n_samples, n_modes)\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the stored number of modes from model.fit() will be used.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the predicted moments (mean, std) for\n                the variable of interest defined by the encoded query.\n        \"\"\"\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(self.model_params[\"n_modes\"])\n\n        if self.base_model.spec is None:\n            err = \"The base model is not specified. Cannot predict new data.\"\n            raise ValueError(err)\n        # Validate the new data\n        self._validate_fit_input(encoded_train_data=encoded_query, n_modes=n_modes)\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # direct normative predictions for each eigenmode\n        eigenmode_mu_estimates, eigenmode_std_estimates = np.array(\n            [\n                self.base_model.predict(\n                    test_covariates,\n                    model_params=direct_model_params,\n                )\n                .to_array([\"mu_estimate\", \"std_estimate\"])\n                .T\n                for direct_model_params in model_params[\"direct_model_params\"]\n            ],\n        ).T  # estimates have a shape of (n_samples, n_modes)\n\n        # create a dummy covariance model\n        covariance_model = CovarianceNormativeModel.from_direct_model(\n            self.base_model,\n            variable_of_interest_1=\"dummy_VOI_1\",  # Dummy variable of interest\n            variable_of_interest_2=\"dummy_VOI_2\",  # Dummy variable of interest\n        )\n\n        # cross-mode dependence structure\n        rho_estimates = np.array(\n            [\n                covariance_model.predict(\n                    test_covariates,\n                    model_params=covariance_model_params,\n                )\n                .to_array([\"correlation_estimate\"])\n                .T\n                for covariance_model_params in model_params[\"covariance_model_params\"]\n            ],\n        ).T[0]  # estimates have a shape of (n_samples, n_covariance_pairs)\n\n        # reformat encoded queries\n        encoded_query = np.asarray(encoded_query[:n_modes]).reshape(n_modes, -1)\n\n        # Compute the predictions\n        predictions = self._predict_from_spectral_estimates(\n            encoded_query=encoded_query,\n            eigenmode_mu_estimates=eigenmode_mu_estimates,\n            eigenmode_std_estimates=eigenmode_std_estimates,\n            rho_estimates=rho_estimates,\n            test_covariates=test_covariates,\n            model_params=model_params,\n            n_modes=n_modes,\n        )\n\n        # Check if extended predictions are requested\n        if extended:\n            if encoded_test_data is None:\n                err = \"Extended predictions require encoded_test_data to be provided.\"\n                raise ValueError(err)\n            # Add extended statistics to predictions (e.g. centiles, log-loss, etc.)\n            predictions.extend_predictions(\n                variable_of_interest=encoded_test_data @ encoded_query,\n            )\n\n        return predictions\n\n    def evaluate(\n        self,\n        encoded_query: npt.NDArray[np.floating[Any]],\n        test_covariates: pd.DataFrame,\n        encoded_test_data: npt.NDArray[np.floating[Any]],\n        query_train_moments: npt.NDArray[np.floating[Any]] | None = None,\n        model_params: dict[str, Any] | None = None,\n        n_modes: int | None = None,\n    ) -&gt; NormativePredictions:\n        \"\"\"\n        Evaluate the model on new data and return predictions along with evaluation\n        metrics.\n\n        Args:\n            encoded_query: np.ndarray\n                Encoded query data defining the normative variable of interest.\n                Can be provided as:\n                - shape = (n_modes) for a single query vector\n                - shape = (n_modes, n_queries) for multiple queries predicted at once\n            test_covariates: pd.DataFrame\n                DataFrame containing the new covariate data to predict.\n                This must include all specified covariates.\n            encoded_test_data: np.ndarray | None\n                Encoded test data for the phenotype being modeled. Expects a numpy array\n                of shape: (n_test, n_modes).\n            query_train_moments: np.ndarray | None\n                A (2, n_queries) array containing the query moments (mean, std) directly\n                measured in the training data. While optional, providing these moments\n                is strongly recommended for accurate evaluation of the model's MSLL.\n                If not provided, the model will use the test data moments as an\n                approximation, which may lead to overestimating MSLL. This is made\n                optional to allow evaluating MSLL when the training data is not\n                accessible (e.g. using a pre-trained model).\n            model_params: dict | None\n                Optional dictionary of model parameters to use. If not provided,\n                the stored parameters from model.fit() will be used.\n            n_modes: int | None\n                Optional number of modes to use for the prediction. If not provided,\n                the stored number of modes from model.fit() will be used.\n\n        Returns:\n            NormativePredictions:\n                Object containing the predicted moments (mean, std) for\n                the variable of interest defined by the encoded query, along with\n                evaluation metrics.\n        \"\"\"\n        # Find n_modes\n        if n_modes is None:\n            n_modes = int(self.model_params[\"n_modes\"])\n\n        # Parameters\n        if model_params is None:\n            model_params = self.model_params\n\n        # reformat encoded queries\n        encoded_query = np.asarray(encoded_query[:n_modes]).reshape(n_modes, -1)\n\n        # Run extended predictions\n        predictions = self.predict(\n            encoded_query=encoded_query,\n            test_covariates=test_covariates,\n            extended=True,\n            model_params=model_params,\n            encoded_test_data=encoded_test_data,\n            n_modes=n_modes,\n        )\n        if query_train_moments is None:\n            logger.warning(\n                \"Query moments not provided. Using test data moments as an\"\n                \" approximation, which may lead to overestimating MSLL.\",\n            )\n            query_train_moments = np.array(\n                [\n                    np.mean(encoded_test_data @ encoded_query, axis=0),\n                    np.std(encoded_test_data @ encoded_query, axis=0, ddof=1),\n                ],\n            )\n        return predictions.evaluate_predictions(\n            variable_of_interest=encoded_test_data @ encoded_query,\n            train_mean=query_train_moments[0],\n            train_std=query_train_moments[1],\n            n_params=model_params[\"n_params\"],\n        )\n\n    @classmethod\n    def load_model(cls, directory: Path) -&gt; SpectralNormativeModel:\n        \"\"\"\n        Load a spectral normative model instance from the specified save directory.\n\n        Args:\n            directory: Path\n                Directory to load the fitted model from. A subdirectory named\n                \"spectral_normative_model\" will be searched within this directory.\n        \"\"\"\n        # Validate the load directory\n        directory = Path(directory)\n        saved_model_dir = utils.general.validate_load_directory(\n            directory,\n            \"spectral_normative_model\",\n        )\n\n        # Check if the pickled joblib file exists in this directory\n        pickled_file = saved_model_dir / \"spectral_model_dict.joblib\"\n        if not pickled_file.exists():\n            err = f\"Model Load Error: Pickled file '{pickled_file}' does not exist.\"\n            raise FileNotFoundError(err)\n\n        model_dict = joblib.load(pickled_file)\n\n        # Create an instance of the class\n        instance = cls(\n            eigenmode_basis=model_dict[\"eigenmode_basis\"],\n            base_model=DirectNormativeModel(\n                spec=model_dict[\"spec\"],\n                batch_covariates=model_dict[\"batch_covariates\"],\n                defaults=model_dict[\"defaults\"],\n            ),\n        )\n\n        if \"model_params\" in model_dict:\n            instance.model_params = model_dict[\"model_params\"]\n\n        return instance\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.build_from_dataframe","title":"<code>build_from_dataframe(eigenmode_basis: utils.gsp.EigenmodeBasis, model_type: ModelType, covariates_dataframe: pd.DataFrame, numerical_covariates: list[str] | None = None, categorical_covariates: list[str] | None = None, batch_covariates: list[str] | None = None, nonlinear_covariates: list[str] | None = None, influencing_mean: list[str] | None = None, influencing_variance: list[str] | None = None, spline_kwargs: dict[str, Any] | None = None) -&gt; SpectralNormativeModel</code>  <code>classmethod</code>","text":"<p>Initialize SNM with an eigenmode basis and a base direct model built from a pandas DataFrame containing all covariates.</p> <p>This uses the from_dataframe method of the DirectNormativeModel class to populate the direct model specification of SNM. Given that SNM does not require a fixed variable of interest, this method assigns a dummy name to the variable_of_interest parameter of the DirectNormativeModel. As such, the provided dataframe should not contain a column with \"dummy_VOI\" as name.</p> <p>Essentially, the provided dataframe should contain all covariates as columns.</p> <p>Parameters:</p> Name Type Description Default <code>eigenmode_basis</code> <code>EigenmodeBasis</code> <p>utils.gsp.EigenmodeBasis The eigenmode basis to be used for spectral normative modeling.</p> required <code>model_type</code> <code>ModelType</code> <p>ModelType Type of the model to create, either \"HBR\" (Hierarchical Bayesian Regression) or \"BLR\" (Bayesian Linear Regression).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the data for all covariates and all samples.</p> required <code>numerical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of numerical covariate names.</p> <code>None</code> <code>categorical_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of categorical covariate names.</p> <code>None</code> <code>batch_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of batch covariate names which should also be included in categorical_covariates.</p> <code>None</code> <code>nonlinear_covariates</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names to be modeled as nonlinear effects. These should also be included in numerical_covariates.</p> <code>None</code> <code>influencing_mean</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the mean of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>influencing_variance</code> <code>list[str] | None</code> <p>list[str] | None List of covariate names that influence the variance of the variable of interest. These should be included in either numerical_covariates or categorical_covariates.</p> <code>None</code> <code>spline_kwargs</code> <code>dict[str, Any] | None</code> <p>dict Additional keyword arguments for spline specification, such as <code>df</code>, <code>degree</code>, and <code>knots</code>. These are passed to the <code>create_spline_spec</code> method to create spline specifications for nonlinear covariates.</p> <code>None</code> <p>Returns:</p> Type Description <code>SpectralNormativeModel</code> <p>SpectralNormativeModel An instance of SpectralNormativeModel with base model specs initialized based on the provided data.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef build_from_dataframe(\n    cls,\n    eigenmode_basis: utils.gsp.EigenmodeBasis,\n    model_type: ModelType,\n    covariates_dataframe: pd.DataFrame,\n    numerical_covariates: list[str] | None = None,\n    categorical_covariates: list[str] | None = None,\n    batch_covariates: list[str] | None = None,\n    nonlinear_covariates: list[str] | None = None,\n    influencing_mean: list[str] | None = None,\n    influencing_variance: list[str] | None = None,\n    spline_kwargs: dict[str, Any] | None = None,\n) -&gt; SpectralNormativeModel:\n    \"\"\"\n    Initialize SNM with an eigenmode basis and a base direct model built from a\n    pandas DataFrame containing all covariates.\n\n    This uses the from_dataframe method of the DirectNormativeModel class\n    to populate the direct model specification of SNM. Given that SNM does not\n    require a fixed variable of interest, this method assigns a dummy name\n    to the variable_of_interest parameter of the DirectNormativeModel. As such,\n    the provided dataframe should not contain a column with \"dummy_VOI\" as name.\n\n    Essentially, the provided dataframe should contain all covariates as columns.\n\n    Args:\n        eigenmode_basis: utils.gsp.EigenmodeBasis\n            The eigenmode basis to be used for spectral normative modeling.\n        model_type: ModelType\n            Type of the model to create, either \"HBR\" (Hierarchical Bayesian\n            Regression) or \"BLR\" (Bayesian Linear Regression).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the data for all covariates and all samples.\n        numerical_covariates: list[str] | None\n            List of numerical covariate names.\n        categorical_covariates: list[str] | None\n            List of categorical covariate names.\n        batch_covariates: list[str] | None\n            List of batch covariate names which should also be included in\n            categorical_covariates.\n        nonlinear_covariates: list[str] | None\n            List of covariate names to be modeled as nonlinear effects.\n            These should also be included in numerical_covariates.\n        influencing_mean: list[str] | None\n            List of covariate names that influence the mean of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        influencing_variance: list[str] | None\n            List of covariate names that influence the variance of the variable\n            of interest. These should be included in either numerical_covariates\n            or categorical_covariates.\n        spline_kwargs: dict\n            Additional keyword arguments for spline specification, such as\n            `df`, `degree`, and `knots`. These are passed to the\n            `create_spline_spec` method to create spline specifications for\n            nonlinear covariates.\n\n    Returns:\n        SpectralNormativeModel\n            An instance of SpectralNormativeModel with base model specs initialized\n            based on the provided data.\n    \"\"\"\n    # Add a dummy variable of interest to the covariates_dataframe\n    covariates_dataframe = covariates_dataframe.copy()\n    covariates_dataframe[\"dummy_VOI\"] = 0.0  # Dummy variable of interest\n    # Specify the base model from the dataframe\n    return cls(\n        eigenmode_basis=eigenmode_basis,\n        base_model=DirectNormativeModel.from_dataframe(\n            model_type=model_type,\n            dataframe=covariates_dataframe,\n            variable_of_interest=\"dummy_VOI\",  # Dummy variable of interest\n            numerical_covariates=(numerical_covariates or []),\n            categorical_covariates=(categorical_covariates or []),\n            batch_covariates=(batch_covariates or []),\n            nonlinear_covariates=(nonlinear_covariates or []),\n            influencing_mean=(influencing_mean or []),\n            influencing_variance=(influencing_variance or []),\n            spline_kwargs=(spline_kwargs or {}),\n        ),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.evaluate","title":"<code>evaluate(encoded_query: npt.NDArray[np.floating[Any]], test_covariates: pd.DataFrame, encoded_test_data: npt.NDArray[np.floating[Any]], query_train_moments: npt.NDArray[np.floating[Any]] | None = None, model_params: dict[str, Any] | None = None, n_modes: int | None = None) -&gt; NormativePredictions</code>","text":"<p>Evaluate the model on new data and return predictions along with evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_query</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded query data defining the normative variable of interest. Can be provided as: - shape = (n_modes) for a single query vector - shape = (n_modes, n_queries) for multiple queries predicted at once</p> required <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates.</p> required <code>encoded_test_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray | None Encoded test data for the phenotype being modeled. Expects a numpy array of shape: (n_test, n_modes).</p> required <code>query_train_moments</code> <code>NDArray[floating[Any]] | None</code> <p>np.ndarray | None A (2, n_queries) array containing the query moments (mean, std) directly measured in the training data. While optional, providing these moments is strongly recommended for accurate evaluation of the model's MSLL. If not provided, the model will use the test data moments as an approximation, which may lead to overestimating MSLL. This is made optional to allow evaluating MSLL when the training data is not accessible (e.g. using a pre-trained model).</p> <code>None</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the stored number of modes from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>NormativePredictions</code> <code>NormativePredictions</code> <p>Object containing the predicted moments (mean, std) for the variable of interest defined by the encoded query, along with evaluation metrics.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def evaluate(\n    self,\n    encoded_query: npt.NDArray[np.floating[Any]],\n    test_covariates: pd.DataFrame,\n    encoded_test_data: npt.NDArray[np.floating[Any]],\n    query_train_moments: npt.NDArray[np.floating[Any]] | None = None,\n    model_params: dict[str, Any] | None = None,\n    n_modes: int | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Evaluate the model on new data and return predictions along with evaluation\n    metrics.\n\n    Args:\n        encoded_query: np.ndarray\n            Encoded query data defining the normative variable of interest.\n            Can be provided as:\n            - shape = (n_modes) for a single query vector\n            - shape = (n_modes, n_queries) for multiple queries predicted at once\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n        encoded_test_data: np.ndarray | None\n            Encoded test data for the phenotype being modeled. Expects a numpy array\n            of shape: (n_test, n_modes).\n        query_train_moments: np.ndarray | None\n            A (2, n_queries) array containing the query moments (mean, std) directly\n            measured in the training data. While optional, providing these moments\n            is strongly recommended for accurate evaluation of the model's MSLL.\n            If not provided, the model will use the test data moments as an\n            approximation, which may lead to overestimating MSLL. This is made\n            optional to allow evaluating MSLL when the training data is not\n            accessible (e.g. using a pre-trained model).\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the stored number of modes from model.fit() will be used.\n\n    Returns:\n        NormativePredictions:\n            Object containing the predicted moments (mean, std) for\n            the variable of interest defined by the encoded query, along with\n            evaluation metrics.\n    \"\"\"\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(self.model_params[\"n_modes\"])\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # reformat encoded queries\n    encoded_query = np.asarray(encoded_query[:n_modes]).reshape(n_modes, -1)\n\n    # Run extended predictions\n    predictions = self.predict(\n        encoded_query=encoded_query,\n        test_covariates=test_covariates,\n        extended=True,\n        model_params=model_params,\n        encoded_test_data=encoded_test_data,\n        n_modes=n_modes,\n    )\n    if query_train_moments is None:\n        logger.warning(\n            \"Query moments not provided. Using test data moments as an\"\n            \" approximation, which may lead to overestimating MSLL.\",\n        )\n        query_train_moments = np.array(\n            [\n                np.mean(encoded_test_data @ encoded_query, axis=0),\n                np.std(encoded_test_data @ encoded_query, axis=0, ddof=1),\n            ],\n        )\n    return predictions.evaluate_predictions(\n        variable_of_interest=encoded_test_data @ encoded_query,\n        train_mean=query_train_moments[0],\n        train_std=query_train_moments[1],\n        n_params=model_params[\"n_params\"],\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.fit","title":"<code>fit(encoded_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_modes: int = -1, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False, covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.25) -&gt; None</code>","text":"<p>Fit the spectral normative model to the provided encoded training data.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded training data as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_modes</code> <code>int</code> <p>int (default=-1) Number of eigenmodes to fit the model to. If -1, all modes are used. If a positive integer, only the first n_modes are used. Note that the encoded_train_data and the eigenmode basis should have at least n_modes columns/eigenvectors.</p> <code>-1</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> <code>covariance_structure</code> <code>NDArray[floating[Any]] | float</code> <p>np.ndarray | float Sparse covariance structure to use for the model fitting. If a (2, n_pairs) array of row and column indices are provided, the model will use this structure. If float, the model will estimate the covariance structure based on the training data and the float value will be used as the threshold to exclude small correlations to form a sparse covariance structure.</p> <code>0.25</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit(\n    self,\n    encoded_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_modes: int = -1,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n    covariance_structure: npt.NDArray[np.floating[Any]] | float = 0.25,\n) -&gt; None:\n    \"\"\"\n    Fit the spectral normative model to the provided encoded training data.\n\n    Args:\n        encoded_train_data: np.ndarray\n            Encoded training data as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_modes: int (default=-1)\n            Number of eigenmodes to fit the model to. If -1, all modes are\n            used. If a positive integer, only the first n_modes are used.\n            Note that the encoded_train_data and the eigenmode basis should have\n            at least n_modes columns/eigenvectors.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n        covariance_structure: np.ndarray | float\n            Sparse covariance structure to use for the model fitting. If a\n            (2, n_pairs) array of row and column indices are provided, the model\n            will use this structure. If float, the model will estimate the\n            covariance structure based on the training data and the float value\n            will be used as the threshold to exclude small correlations to form a\n            sparse covariance structure.\n    \"\"\"\n    logger.info(\"Starting SNM model fitting:\")\n    # Evaluate the number of modes to fit\n    if n_modes == -1:\n        n_modes = self.eigenmode_basis.n_modes\n    # Validate the input data\n    if not isinstance(encoded_train_data, np.ndarray):\n        err = \"encoded_train_data must be a numpy array.\"\n        raise TypeError(err)\n    if encoded_train_data.shape[1] &lt; n_modes:\n        err = f\"encoded_train_data must have at least {n_modes} columns (n_modes).\"\n        raise ValueError(err)\n    if self.eigenmode_basis.n_modes &lt; n_modes:\n        err = (\n            f\"Eigenmode basis has only {self.eigenmode_basis.n_modes}\"\n            f\" modes, while {n_modes} were requested.\"\n        )\n        raise ValueError(err)\n\n    # Setup the save directory if needed\n    if save_directory is not None:\n        # Prepare the save directory\n        save_directory = Path(save_directory)\n        utils.general.prepare_save_directory(\n            save_directory,\n            \"spectral_normative_model\",\n        )\n\n    logger.info(\"Step 1; direct models for each eigenmode (%s modes)\", n_modes)\n\n    self.fit_all_direct(\n        encoded_train_data=encoded_train_data,\n        covariates_dataframe=covariates_dataframe,\n        n_modes=n_modes,\n        n_jobs=n_jobs,\n        save_directory=save_directory,\n        save_separate=save_separate,\n    )\n\n    logger.info(\"Step 2; identify sparse covariance structure\")\n\n    # Identify sparse covariance structure if a float value is given\n    if isinstance(covariance_structure, float):\n        # Use trained models to compute z-scores\n        encoded_train_z_scores = np.array(\n            [\n                self.base_model.predict(\n                    test_covariates=covariates_dataframe,\n                    model_params=self.direct_model_params[x],\n                )\n                .extend_predictions(\n                    variable_of_interest=encoded_train_data[:, x],\n                )\n                .predictions[\"z-score\"]\n                for x in range(n_modes)\n            ],\n        ).T\n\n        self.sparse_covariance_structure = (\n            self.identify_sparse_covariance_structure(\n                encoded_train_z_scores,\n                covariates_dataframe,\n                covariance_structure,\n            )\n        )\n    else:\n        self.sparse_covariance_structure = np.array(covariance_structure)\n\n    # Verify that the covariance structure is valid\n    if not self._is_valid_covariance_structure(self.sparse_covariance_structure):\n        err = \"Invalid sparse covariance structure.\"\n        raise ValueError(err)\n\n    # Model cross basis sparse covariance structure\n    logger.info(\n        \"Step 3; cross-eigenmode dependency modeling (%s pairs)\",\n        self.sparse_covariance_structure.shape[0],\n    )\n\n    self.fit_all_covariance(\n        encoded_train_data=encoded_train_data,\n        covariates_dataframe=covariates_dataframe,\n        n_jobs=n_jobs,\n        save_directory=save_directory,\n        save_separate=save_separate,\n    )\n\n    # Save SNM model parameters\n    self.model_params = {\n        \"n_modes\": n_modes,\n        \"sample_size\": encoded_train_data.shape[0],\n        \"direct_model_params\": self.direct_model_params,\n        \"sparse_covariance_structure\": self.sparse_covariance_structure,\n        \"covariance_model_params\": self.covariance_model_params,\n    }\n    if (self.direct_model_params[0] is not None) and (\n        \"n_params\" in self.direct_model_params[0]\n    ):\n        self.model_params[\"n_params\"] = self.direct_model_params[0][\"n_params\"]\n    else:\n        err = \"Direct model parameters are not valid.\"\n        raise ValueError(err)\n\n    # Save the model if a save path is provided\n    if save_directory is not None:\n        self.save_model(save_directory)\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.fit_all_covariance","title":"<code>fit_all_covariance(encoded_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False) -&gt; None</code>","text":"<p>Fit the direct models for all specified eigenmodes.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded training data as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_all_covariance(\n    self,\n    encoded_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Fit the direct models for all specified eigenmodes.\n\n    Args:\n        encoded_train_data: np.ndarray\n            Encoded training data as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n    \"\"\"\n    # Setup the save directory if needed\n    if save_directory is not None:\n        save_directory = Path(save_directory)\n\n    # Fit the base covariance models for selected eigenmode pairs in parallel\n    tasks = (\n        joblib.delayed(self.fit_single_covariance)(\n            variable_of_interest_1=encoded_train_data[\n                :,\n                self.sparse_covariance_structure[i, 0],\n            ],\n            variable_of_interest_2=encoded_train_data[\n                :,\n                self.sparse_covariance_structure[i, 1],\n            ],\n            direct_model_params_1=self.direct_model_params[\n                self.sparse_covariance_structure[i, 0]\n            ],\n            direct_model_params_2=self.direct_model_params[\n                self.sparse_covariance_structure[i, 1]\n            ],\n            covariates_dataframe=covariates_dataframe,\n            save_directory=(\n                utils.general.ensure_dir(\n                    save_directory\n                    / \"spectral_normative_model\"\n                    / \"covariance_models\"\n                    / (\n                        f\"mode_{self.sparse_covariance_structure[i, 0] + 1},\"\n                        f\"mode_{self.sparse_covariance_structure[i, 1] + 1}\"\n                    ),\n                )\n                if save_directory is not None and save_separate\n                else None\n            ),\n        )\n        for i in range(self.sparse_covariance_structure.shape[0])\n    )\n    self.covariance_model_params = utils.parallel.ParallelTqdm(\n        n_jobs=n_jobs,\n        total_tasks=self.sparse_covariance_structure.shape[0],\n        desc=\"Fitting covariance models\",\n    )(tasks)  # pyright: ignore[reportCallIssue]\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.fit_all_direct","title":"<code>fit_all_direct(encoded_train_data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, n_modes: int = -1, n_jobs: int = -1, save_directory: Path | None = None, save_separate: bool = False) -&gt; None</code>","text":"<p>Fit the direct models for all specified eigenmodes.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_train_data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded training data as a numpy array (n_samples, n_modes).</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples. It must include all specified covariates in the model specification.</p> required <code>n_modes</code> <code>int</code> <p>int (default=-1) Number of eigenmodes to fit the model to. If -1, all modes are used. If a positive integer, only the first n_modes are used. Note that the encoded_train_data and the eigenmode basis should have at least n_modes columns/eigenvectors.</p> <code>-1</code> <code>n_jobs</code> <code>int</code> <p>int (default=-1) Number of parallel jobs to use for fitting the model. If -1, all available CPU cores are used. If 1, no parallelization is used.</p> <code>-1</code> <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved. A subdirectory named \"spectral_normative_model\" will be created within the specified save_directory.</p> <code>None</code> <code>save_separate</code> <code>bool</code> <p>bool (default=False) Whether to save the fitted direct model parameters separately for each eigenmode as individual files. This is only applicable if <code>save_directory</code> is provided.</p> <code>False</code> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_all_direct(\n    self,\n    encoded_train_data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    n_modes: int = -1,\n    n_jobs: int = -1,\n    save_directory: Path | None = None,\n    save_separate: bool = False,\n) -&gt; None:\n    \"\"\"\n    Fit the direct models for all specified eigenmodes.\n\n    Args:\n        encoded_train_data: np.ndarray\n            Encoded training data as a numpy array (n_samples, n_modes).\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n            It must include all specified covariates in the model specification.\n        n_modes: int (default=-1)\n            Number of eigenmodes to fit the model to. If -1, all modes are\n            used. If a positive integer, only the first n_modes are used.\n            Note that the encoded_train_data and the eigenmode basis should have\n            at least n_modes columns/eigenvectors.\n        n_jobs: int (default=-1)\n            Number of parallel jobs to use for fitting the model. If -1, all\n            available CPU cores are used. If 1, no parallelization is used.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n            A subdirectory named \"spectral_normative_model\" will be created\n            within the specified save_directory.\n        save_separate: bool (default=False)\n            Whether to save the fitted direct model parameters separately for each\n            eigenmode as individual files. This is only applicable if\n            `save_directory` is provided.\n    \"\"\"\n    # Setup the save directory if needed\n    if save_directory is not None:\n        save_directory = Path(save_directory)\n\n    # Fit the base direct model for each eigenmode using parallel processing\n    tasks = (\n        joblib.delayed(self.fit_single_direct)(\n            variable_of_interest=encoded_train_data[:, i],\n            covariates_dataframe=covariates_dataframe,\n            save_directory=(\n                utils.general.ensure_dir(\n                    save_directory\n                    / \"spectral_normative_model\"\n                    / \"direct_models\"\n                    / f\"mode_{i + 1}\",\n                )\n                if save_directory is not None and save_separate\n                else None\n            ),\n        )\n        for i in range(n_modes)\n    )\n    self.direct_model_params = list(\n        utils.parallel.ParallelTqdm(\n            n_jobs=n_jobs,\n            total_tasks=n_modes,\n            desc=\"Fitting direct models\",\n        )(tasks),  # pyright: ignore[reportCallIssue]\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.fit_single_covariance","title":"<code>fit_single_covariance(variable_of_interest_1: npt.NDArray[np.floating[Any]], variable_of_interest_2: npt.NDArray[np.floating[Any]], direct_model_params_1: dict[str, Any], direct_model_params_2: dict[str, Any], covariates_dataframe: pd.DataFrame, *, save_directory: Path | None = None, return_model_params: bool = True, defaults_overwrite: dict[str, Any] | None = None) -&gt; dict[str, Any] | None</code>","text":"<p>Fit a covariance normative model between a single pair of eigenmodes. This method fits a covariance model to the provided pair of variables and covariates dataframe, considering the direct model fits for each eigenmode, while allowing for the cross-eigenmode covariance to vary normatively.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest_1</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a single eigenmode.</p> required <code>variable_of_interest_2</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a second eigenmode.</p> required <code>direct_model_params_1</code> <code>dict[str, Any]</code> <p>dict The parameters of the direct model fitted to the first eigenmode.</p> required <code>direct_model_params_2</code> <code>dict[str, Any]</code> <p>dict The parameters of the direct model fitted to the second eigenmode.</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved.</p> <code>None</code> <code>return_model_params</code> <code>bool</code> <p>bool If True, return the fitted model parameters.</p> <code>True</code> <code>defaults_overwrite</code> <code>dict[str, Any] | None</code> <p>dict (default={}) Dictionary of default values to overwrite in the model fitting process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any] | None</code> <p>If <code>return_model_params</code> is True, return the fitted model parameters in a dictionary.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_single_covariance(\n    self,\n    variable_of_interest_1: npt.NDArray[np.floating[Any]],\n    variable_of_interest_2: npt.NDArray[np.floating[Any]],\n    direct_model_params_1: dict[str, Any],\n    direct_model_params_2: dict[str, Any],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    return_model_params: bool = True,\n    defaults_overwrite: dict[str, Any] | None = None,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Fit a covariance normative model between a single pair of eigenmodes.\n    This method fits a covariance model to the provided pair of variables\n    and covariates dataframe, considering the direct model fits for each\n    eigenmode, while allowing for the cross-eigenmode covariance to vary\n    normatively.\n\n    Args:\n        variable_of_interest_1: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a single eigenmode.\n        variable_of_interest_2: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a second eigenmode.\n        direct_model_params_1: dict\n            The parameters of the direct model fitted to the first eigenmode.\n        direct_model_params_2: dict\n            The parameters of the direct model fitted to the second eigenmode.\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n        return_model_params: bool\n            If True, return the fitted model parameters.\n        defaults_overwrite: dict (default={})\n            Dictionary of default values to overwrite in the model fitting process.\n\n    Returns:\n        dict:\n            If `return_model_params` is True, return the fitted model parameters\n            in a dictionary.\n    \"\"\"\n    # Prepare the data for fitting\n    train_data = covariates_dataframe.copy()\n    # Add the respective mode loadings as the variables of interest\n    train_data[\"VOI_1\"] = variable_of_interest_1\n    train_data[\"VOI_2\"] = variable_of_interest_2\n    train_data[[\"VOI_1_mu_estimate\", \"VOI_1_std_estimate\"]] = (\n        self.base_model.predict(\n            train_data,\n            model_params=direct_model_params_1,\n        )\n        .to_array()\n        .T\n    )  # Add the direct model predictions\n    train_data[[\"VOI_2_mu_estimate\", \"VOI_2_std_estimate\"]] = (\n        self.base_model.predict(\n            train_data,\n            model_params=direct_model_params_2,\n        )\n        .to_array()\n        .T\n    )  # Add the direct model predictions\n\n    # Instantiate a covariance normative model from the base model\n    covariance_model = CovarianceNormativeModel.from_direct_model(\n        self.base_model,\n        variable_of_interest_1=\"VOI_1\",\n        variable_of_interest_2=\"VOI_2\",\n        defaults_overwrite=(defaults_overwrite or {}),\n    )\n\n    # Fit the model silently\n    with utils.general.suppress_output():\n        covariance_model.fit(\n            train_data=train_data,\n            save_directory=save_directory,\n            progress_bar=False,\n        )\n\n    # Return the fitted model parameters if requested\n    if return_model_params:\n        return covariance_model.model_params\n\n    # If not returning model parameters, return None\n    return None\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.fit_single_direct","title":"<code>fit_single_direct(variable_of_interest: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, *, save_directory: Path | None = None, return_model_params: bool = True) -&gt; dict[str, Any] | None</code>","text":"<p>Fit a direct normative model for a single spectral eigenmode. This method fits the base direct model to the provided variable of interest and covariates dataframe, allowing for the model to be trained on a specific eigenmode of the spectral embedding.</p> <p>Parameters:</p> Name Type Description Default <code>variable_of_interest</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The loading vector capturing the variance within training data that corresponds to a single eigenmode.</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the covariates for the samples.</p> required <code>save_directory</code> <code>Path | None</code> <p>Path | None Directory to save the fitted model. If None, the model is not saved.</p> <code>None</code> <code>return_model_params</code> <code>bool</code> <p>bool If True, return the fitted model parameters.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any] | None</code> <p>If <code>return_model_params</code> is True, return the fitted model parameters in a dictionary.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def fit_single_direct(\n    self,\n    variable_of_interest: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    *,\n    save_directory: Path | None = None,\n    return_model_params: bool = True,\n) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Fit a direct normative model for a single spectral eigenmode.\n    This method fits the base direct model to the provided variable of interest\n    and covariates dataframe, allowing for the model to be trained on a specific\n    eigenmode of the spectral embedding.\n\n    Args:\n        variable_of_interest: np.ndarray\n            The loading vector capturing the variance within training data that\n            corresponds to a single eigenmode.\n        covariates_dataframe: pd.DataFrame\n            DataFrame containing the covariates for the samples.\n        save_directory: Path | None\n            Directory to save the fitted model. If None, the model is not saved.\n        return_model_params: bool\n            If True, return the fitted model parameters.\n\n    Returns:\n        dict:\n            If `return_model_params` is True, return the fitted model parameters\n            in a dictionary.\n    \"\"\"\n    # Prepare the data for fitting\n    train_data = covariates_dataframe.copy()\n    # Add the mode loading as the variable of interest\n    train_data[\"VOI\"] = variable_of_interest\n\n    # Instantiate a direct normative model from the base model\n    direct_model = DirectNormativeModel(\n        spec=NormativeModelSpec(\n            variable_of_interest=\"VOI\",  # Use the added VOI column\n            covariates=self.base_model.spec.covariates,\n            influencing_mean=self.base_model.spec.influencing_mean,\n            influencing_variance=self.base_model.spec.influencing_variance,\n        ),\n        batch_covariates=self.base_model.batch_covariates,\n        defaults=self.base_model.defaults,\n    )\n\n    # Fit the model silently\n    with utils.general.suppress_output():\n        direct_model.fit(\n            train_data=train_data,\n            save_directory=save_directory,\n            progress_bar=False,\n        )\n\n    # Return the fitted model parameters if requested\n    if return_model_params:\n        return direct_model.model_params\n\n    # If not returning model parameters, return None\n    return None\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.identify_sparse_covariance_structure","title":"<code>identify_sparse_covariance_structure(data: npt.NDArray[np.floating[Any]], covariates_dataframe: pd.DataFrame, correlation_threshold: float) -&gt; npt.NDArray[np.integer[Any]]</code>","text":"<p>Identify the sparse cross-basis covariance structure in the phenotype. This method analyzes the encoded phenotype to determine the covariance pairs that need to be modeled.</p> <p>Note: if the batches become too small, this estimate can become less stable in which case it is recommended to provide the sparse covariance structure to the model instead.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray The encoded training data representing the phenotype in the graph frequency domain.</p> required <code>covariates_dataframe</code> <code>DataFrame</code> <p>pd.DataFrame The DataFrame containing covariates for the samples. The batch covariates will be used to group the samples and identify the important covariance pairs.</p> required <code>correlation_threshold</code> <code>float</code> <p>float The threshold to include covariance pairs if significantly correlated. Should be between 0 and 1.</p> required <p>Returns:</p> Type Description <code>NDArray[integer[Any]]</code> <p>np.ndarray: A (N, 2) array: the rows and columns of the identified sparse covariance structure.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def identify_sparse_covariance_structure(\n    self,\n    data: npt.NDArray[np.floating[Any]],\n    covariates_dataframe: pd.DataFrame,\n    correlation_threshold: float,\n) -&gt; npt.NDArray[np.integer[Any]]:\n    \"\"\"\n    Identify the sparse cross-basis covariance structure in the phenotype.\n    This method analyzes the encoded phenotype to determine the covariance\n    pairs that need to be modeled.\n\n    Note: if the batches become too small, this estimate can become less stable\n    in which case it is recommended to provide the sparse covariance structure\n    to the model instead.\n\n    Args:\n        data: np.ndarray\n            The encoded training data representing the phenotype in the graph\n            frequency domain.\n        covariates_dataframe: pd.DataFrame\n            The DataFrame containing covariates for the samples. The batch\n            covariates will be used to group the samples and identify the\n            important covariance pairs.\n        correlation_threshold: float\n            The threshold to include covariance pairs if significantly correlated.\n            Should be between 0 and 1.\n\n    Returns:\n        np.ndarray:\n            A (N, 2) array: the rows and columns of the\n            identified sparse covariance structure.\n    \"\"\"\n    # Start with correlation structure across the whole sample\n    corr_max = utils.stats.compute_correlation_significance_by_fisher_z(\n        np.corrcoef(data.T),\n        n_samples=data.shape[0],\n        correlation_threshold=correlation_threshold,\n    )\n\n    # Now iterate over all batch covariates\n    for batch_effect in self.base_model.batch_covariates:\n        # For every batch re-evaluate correlations\n        for batch in pd.unique(covariates_dataframe[batch_effect]):\n            # Make a mask for the current batch\n            batch_mask = covariates_dataframe[batch_effect] == batch\n            # Update maximums\n            corr_max = np.maximum.reduce(\n                [\n                    corr_max,\n                    utils.stats.compute_correlation_significance_by_fisher_z(\n                        np.corrcoef(data[batch_mask].T),\n                        n_samples=data[batch_mask].shape[0],\n                        correlation_threshold=correlation_threshold,\n                    ),\n                ],\n            )\n\n    # Now compute the sparsity structure based on the resulting matrix\n    rows, cols = np.where(np.abs(corr_max) &gt; 0)\n\n    # Remove redundant and duplicate pairs\n    rows_lim = rows[rows &lt; cols]\n    cols_lim = cols[rows &lt; cols]\n\n    return np.array([rows_lim, cols_lim]).T\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.load_model","title":"<code>load_model(directory: Path) -&gt; SpectralNormativeModel</code>  <code>classmethod</code>","text":"<p>Load a spectral normative model instance from the specified save directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Directory to load the fitted model from. A subdirectory named \"spectral_normative_model\" will be searched within this directory.</p> required Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef load_model(cls, directory: Path) -&gt; SpectralNormativeModel:\n    \"\"\"\n    Load a spectral normative model instance from the specified save directory.\n\n    Args:\n        directory: Path\n            Directory to load the fitted model from. A subdirectory named\n            \"spectral_normative_model\" will be searched within this directory.\n    \"\"\"\n    # Validate the load directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.validate_load_directory(\n        directory,\n        \"spectral_normative_model\",\n    )\n\n    # Check if the pickled joblib file exists in this directory\n    pickled_file = saved_model_dir / \"spectral_model_dict.joblib\"\n    if not pickled_file.exists():\n        err = f\"Model Load Error: Pickled file '{pickled_file}' does not exist.\"\n        raise FileNotFoundError(err)\n\n    model_dict = joblib.load(pickled_file)\n\n    # Create an instance of the class\n    instance = cls(\n        eigenmode_basis=model_dict[\"eigenmode_basis\"],\n        base_model=DirectNormativeModel(\n            spec=model_dict[\"spec\"],\n            batch_covariates=model_dict[\"batch_covariates\"],\n            defaults=model_dict[\"defaults\"],\n        ),\n    )\n\n    if \"model_params\" in model_dict:\n        instance.model_params = model_dict[\"model_params\"]\n\n    return instance\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.predict","title":"<code>predict(encoded_query: npt.NDArray[np.floating[Any]], test_covariates: pd.DataFrame, *, extended: bool = False, model_params: dict[str, Any] | None = None, encoded_test_data: npt.NDArray[np.floating[Any]] | None = None, n_modes: int | None = None) -&gt; NormativePredictions</code>","text":"<p>Predict normative moments (mean, std) for new data using the fitted spectral normative model. Spectral normative modeling can estimate the normative distribution of any variable of interest defined as a spatial query encoded in the latent low-pass graph spectral space.</p> <p>Parameters:</p> Name Type Description Default <code>encoded_query</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded query data defining the normative variable of interest. Can be provided as: - shape = (n_modes) for a single query vector - shape = (n_modes, n_queries) for multiple queries predicted at once</p> required <code>test_covariates</code> <code>DataFrame</code> <p>pd.DataFrame DataFrame containing the new covariate data to predict. This must include all specified covariates.</p> required <code>extended</code> <code>bool</code> <p>bool (default: False) If True, return additional stats such as log-likelihood, centiles, etc. Note that extended predictions require encoded_test_data to be provided in addition to the covariates.</p> <code>False</code> <code>model_params</code> <code>dict[str, Any] | None</code> <p>dict | None Optional dictionary of model parameters to use. If not provided, the stored parameters from model.fit() will be used.</p> <code>None</code> <code>encoded_test_data</code> <code>NDArray[floating[Any]] | None</code> <p>np.ndarray | None Optional encoded test data for the phenotype being modeled (only required for extended predictions). Expects a numpy array (n_samples, n_modes)</p> <code>None</code> <code>n_modes</code> <code>int | None</code> <p>int | None Optional number of modes to use for the prediction. If not provided, the stored number of modes from model.fit() will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>NormativePredictions</code> <p>pd.DataFrame: DataFrame containing the predicted moments (mean, std) for the variable of interest defined by the encoded query.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>def predict(\n    self,\n    encoded_query: npt.NDArray[np.floating[Any]],\n    test_covariates: pd.DataFrame,\n    *,\n    extended: bool = False,\n    model_params: dict[str, Any] | None = None,\n    encoded_test_data: npt.NDArray[np.floating[Any]] | None = None,\n    n_modes: int | None = None,\n) -&gt; NormativePredictions:\n    \"\"\"\n    Predict normative moments (mean, std) for new data using the fitted spectral\n    normative model.\n    Spectral normative modeling can estimate the normative distribution of any\n    variable of interest defined as a spatial query encoded in the latent low-pass\n    graph spectral space.\n\n    Args:\n        encoded_query: np.ndarray\n            Encoded query data defining the normative variable of interest.\n            Can be provided as:\n            - shape = (n_modes) for a single query vector\n            - shape = (n_modes, n_queries) for multiple queries predicted at once\n        test_covariates: pd.DataFrame\n            DataFrame containing the new covariate data to predict.\n            This must include all specified covariates.\n        extended: bool (default: False)\n            If True, return additional stats such as log-likelihood, centiles, etc.\n            Note that extended predictions require encoded_test_data to be\n            provided in addition to the covariates.\n        model_params: dict | None\n            Optional dictionary of model parameters to use. If not provided,\n            the stored parameters from model.fit() will be used.\n        encoded_test_data: np.ndarray | None\n            Optional encoded test data for the phenotype being modeled (only\n            required for extended predictions).\n            Expects a numpy array (n_samples, n_modes)\n        n_modes: int | None\n            Optional number of modes to use for the prediction. If not provided,\n            the stored number of modes from model.fit() will be used.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the predicted moments (mean, std) for\n            the variable of interest defined by the encoded query.\n    \"\"\"\n    # Find n_modes\n    if n_modes is None:\n        n_modes = int(self.model_params[\"n_modes\"])\n\n    if self.base_model.spec is None:\n        err = \"The base model is not specified. Cannot predict new data.\"\n        raise ValueError(err)\n    # Validate the new data\n    self._validate_fit_input(encoded_train_data=encoded_query, n_modes=n_modes)\n\n    # Parameters\n    if model_params is None:\n        model_params = self.model_params\n\n    # direct normative predictions for each eigenmode\n    eigenmode_mu_estimates, eigenmode_std_estimates = np.array(\n        [\n            self.base_model.predict(\n                test_covariates,\n                model_params=direct_model_params,\n            )\n            .to_array([\"mu_estimate\", \"std_estimate\"])\n            .T\n            for direct_model_params in model_params[\"direct_model_params\"]\n        ],\n    ).T  # estimates have a shape of (n_samples, n_modes)\n\n    # create a dummy covariance model\n    covariance_model = CovarianceNormativeModel.from_direct_model(\n        self.base_model,\n        variable_of_interest_1=\"dummy_VOI_1\",  # Dummy variable of interest\n        variable_of_interest_2=\"dummy_VOI_2\",  # Dummy variable of interest\n    )\n\n    # cross-mode dependence structure\n    rho_estimates = np.array(\n        [\n            covariance_model.predict(\n                test_covariates,\n                model_params=covariance_model_params,\n            )\n            .to_array([\"correlation_estimate\"])\n            .T\n            for covariance_model_params in model_params[\"covariance_model_params\"]\n        ],\n    ).T[0]  # estimates have a shape of (n_samples, n_covariance_pairs)\n\n    # reformat encoded queries\n    encoded_query = np.asarray(encoded_query[:n_modes]).reshape(n_modes, -1)\n\n    # Compute the predictions\n    predictions = self._predict_from_spectral_estimates(\n        encoded_query=encoded_query,\n        eigenmode_mu_estimates=eigenmode_mu_estimates,\n        eigenmode_std_estimates=eigenmode_std_estimates,\n        rho_estimates=rho_estimates,\n        test_covariates=test_covariates,\n        model_params=model_params,\n        n_modes=n_modes,\n    )\n\n    # Check if extended predictions are requested\n    if extended:\n        if encoded_test_data is None:\n            err = \"Extended predictions require encoded_test_data to be provided.\"\n            raise ValueError(err)\n        # Add extended statistics to predictions (e.g. centiles, log-loss, etc.)\n        predictions.extend_predictions(\n            variable_of_interest=encoded_test_data @ encoded_query,\n        )\n\n    return predictions\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SpectralNormativeModel.save_model","title":"<code>save_model(directory: Path) -&gt; None</code>","text":"<p>Save the fitted spectral normative model to the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path Directory to save the fitted model. A subdirectory named \"spectral_normative_model\" will be created within this directory.</p> required Source code in <code>src/spectranorm/snm.py</code> <pre><code>def save_model(self, directory: Path) -&gt; None:\n    \"\"\"\n    Save the fitted spectral normative model to the specified directory.\n\n    Args:\n        directory: Path\n            Directory to save the fitted model. A subdirectory named\n            \"spectral_normative_model\" will be created within this directory.\n    \"\"\"\n    # Prepare the save directory\n    directory = Path(directory)\n    saved_model_dir = utils.general.prepare_save_directory(\n        directory,\n        \"spectral_normative_model\",\n    )\n\n    # Save the model\n    model_dict = {\n        \"spec\": self.base_model.spec,\n        \"batch_covariates\": self.base_model.batch_covariates,\n        \"defaults\": self.base_model.defaults,\n        \"eigenmode_basis\": self.eigenmode_basis,\n    }\n    if hasattr(self, \"model_params\"):\n        model_dict[\"model_params\"] = self.model_params\n    joblib.dump(model_dict, saved_model_dir / \"spectral_model_dict.joblib\")\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SplineSpec","title":"<code>SplineSpec</code>  <code>dataclass</code>","text":"<p>Specification for spline basis construction.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>int</code> <p>int Degrees of freedom (number of basis functions).</p> <code>degree</code> <code>int</code> <p>int Degree of the spline (e.g., 3 for cubic splines).</p> <code>lower_bound</code> <code>float</code> <p>float Lower boundary for the spline domain.</p> <code>upper_bound</code> <code>float</code> <p>float Upper boundary for the spline domain.</p> <code>knots</code> <code>list[float] | None</code> <p>Optional[List[float]] Optional list of internal knot locations within the spline domain (excluding the boundary knots). Must be strictly increasing and contain exactly <code>df - degree - 1</code> values. If unspecified, then equally spaced quantiles of the input data are used.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@dataclass\nclass SplineSpec:\n    \"\"\"\n    Specification for spline basis construction.\n\n    Attributes:\n        df: int\n            Degrees of freedom (number of basis functions).\n        degree: int\n            Degree of the spline (e.g., 3 for cubic splines).\n        lower_bound: float\n            Lower boundary for the spline domain.\n        upper_bound: float\n            Upper boundary for the spline domain.\n        knots: Optional[List[float]]\n            Optional list of internal knot locations within the spline domain\n            (excluding the boundary knots). Must be strictly increasing and\n            contain exactly `df - degree - 1` values. If unspecified, then\n            equally spaced quantiles of the input data are used.\n    \"\"\"\n\n    lower_bound: float\n    upper_bound: float\n    df: int = DEFAULT_SPLINE_DF\n    degree: int = DEFAULT_SPLINE_DEGREE\n    knots: list[float] | None = None\n\n    # Validation checks for the spline specification.\n    def __post_init__(self) -&gt; None:\n        # Check that df (degrees of freedom) is greater than degree\n        if self.df &lt;= self.degree:\n            err = \"df (degrees of freedom) must be greater than degree.\"\n            raise ValueError(err)\n        # Check that degree is at least 1\n        if self.degree &lt; 1:\n            err = \"degree must be at least 1.\"\n            raise ValueError(err)\n        # Check that lower_bound and upper_bound are numeric\n        if self.lower_bound &gt;= self.upper_bound:\n            err = \"lower_bound must be less than upper_bound.\"\n            raise ValueError(err)\n        if self.knots is not None:\n            if not all(isinstance(k, (int, float)) for k in self.knots):\n                err = \"All knots must be numeric (int or float).\"\n                raise TypeError(err)\n            # Check if knots are strictly increasing\n            if not all(x &lt; y for x, y in zip(self.knots, self.knots[1:])):\n                err = \"Knots must be strictly increasing.\"\n                raise ValueError(err)\n            # Check if knots are within bounds\n            if any(k &lt; self.lower_bound or k &gt; self.upper_bound for k in self.knots):\n                err = (\n                    \"All knots must be within the bounds defined by \"\n                    \"lower_bound and upper_bound.\"\n                )\n                raise ValueError(err)\n            # Check if the number of knots is correct\n            if len(self.knots) != (self.df - self.degree - 1):\n                err = (\n                    f\"knots must contain exactly {self.df - self.degree - 1} \"\n                    f\"values, got {len(self.knots)}.\"\n                )\n                raise ValueError(err)\n\n    @classmethod\n    def create_spline_spec(\n        cls,\n        values: pd.Series[float],\n        df: int = DEFAULT_SPLINE_DF,\n        degree: int = DEFAULT_SPLINE_DEGREE,\n        knots: list[float] | None = None,\n        extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n        lower_bound: float | None = None,\n        upper_bound: float | None = None,\n    ) -&gt; SplineSpec:\n        \"\"\"\n        Create a spline specification from a pandas Series.\n\n        Args:\n            values: pd.Series\n                The list of input values to make the spline.\n            df: int\n                Degrees of freedom for the spline (default is 5).\n            degree: int\n                Degree of the spline (default is 3).\n            knots: list[float] | None\n                [Optional] List of internal knot locations within the spline domain.\n                If None, equally spaced quantiles of the input data are used.\n            extrapolation_factor: float, positive, default is 0.1\n                [Optional] Factor to extend the lower and upper bounds of the spline\n                domain.\n            lower_bound: float | None\n                [Optional] Lower boundary for the spline domain. If None, it is set to\n                `values.min() - extrapolation_factor * (values.max() - values.min())`.\n            upper_bound: float | None\n                [Optional] Upper boundary for the spline domain. If None, it is set to\n                `values.max() + extrapolation_factor * (values.max() - values.min())`.\n\n        Returns:\n            SplineSpec\n                The created spline specification.\n        \"\"\"\n        extrapolation = extrapolation_factor * (values.max() - values.min())\n        if lower_bound is None:\n            lower_bound = values.min() - extrapolation\n        if upper_bound is None:\n            upper_bound = values.max() + extrapolation\n        if knots is None:\n            # Use equally spaced quantiles as knots\n            knots = np.linspace(lower_bound, upper_bound, df - degree + 1)[\n                1:-1\n            ].tolist()\n        return cls(\n            lower_bound=lower_bound,\n            upper_bound=upper_bound,\n            df=df,\n            degree=degree,\n            knots=knots,\n        )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.SplineSpec.create_spline_spec","title":"<code>create_spline_spec(values: pd.Series[float], df: int = DEFAULT_SPLINE_DF, degree: int = DEFAULT_SPLINE_DEGREE, knots: list[float] | None = None, extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR, lower_bound: float | None = None, upper_bound: float | None = None) -&gt; SplineSpec</code>  <code>classmethod</code>","text":"<p>Create a spline specification from a pandas Series.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series[float]</code> <p>pd.Series The list of input values to make the spline.</p> required <code>df</code> <code>int</code> <p>int Degrees of freedom for the spline (default is 5).</p> <code>DEFAULT_SPLINE_DF</code> <code>degree</code> <code>int</code> <p>int Degree of the spline (default is 3).</p> <code>DEFAULT_SPLINE_DEGREE</code> <code>knots</code> <code>list[float] | None</code> <p>list[float] | None [Optional] List of internal knot locations within the spline domain. If None, equally spaced quantiles of the input data are used.</p> <code>None</code> <code>extrapolation_factor</code> <code>float</code> <p>float, positive, default is 0.1 [Optional] Factor to extend the lower and upper bounds of the spline domain.</p> <code>DEFAULT_SPLINE_EXTRAPOLATION_FACTOR</code> <code>lower_bound</code> <code>float | None</code> <p>float | None [Optional] Lower boundary for the spline domain. If None, it is set to <code>values.min() - extrapolation_factor * (values.max() - values.min())</code>.</p> <code>None</code> <code>upper_bound</code> <code>float | None</code> <p>float | None [Optional] Upper boundary for the spline domain. If None, it is set to <code>values.max() + extrapolation_factor * (values.max() - values.min())</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>SplineSpec</code> <p>SplineSpec The created spline specification.</p> Source code in <code>src/spectranorm/snm.py</code> <pre><code>@classmethod\ndef create_spline_spec(\n    cls,\n    values: pd.Series[float],\n    df: int = DEFAULT_SPLINE_DF,\n    degree: int = DEFAULT_SPLINE_DEGREE,\n    knots: list[float] | None = None,\n    extrapolation_factor: float = DEFAULT_SPLINE_EXTRAPOLATION_FACTOR,\n    lower_bound: float | None = None,\n    upper_bound: float | None = None,\n) -&gt; SplineSpec:\n    \"\"\"\n    Create a spline specification from a pandas Series.\n\n    Args:\n        values: pd.Series\n            The list of input values to make the spline.\n        df: int\n            Degrees of freedom for the spline (default is 5).\n        degree: int\n            Degree of the spline (default is 3).\n        knots: list[float] | None\n            [Optional] List of internal knot locations within the spline domain.\n            If None, equally spaced quantiles of the input data are used.\n        extrapolation_factor: float, positive, default is 0.1\n            [Optional] Factor to extend the lower and upper bounds of the spline\n            domain.\n        lower_bound: float | None\n            [Optional] Lower boundary for the spline domain. If None, it is set to\n            `values.min() - extrapolation_factor * (values.max() - values.min())`.\n        upper_bound: float | None\n            [Optional] Upper boundary for the spline domain. If None, it is set to\n            `values.max() + extrapolation_factor * (values.max() - values.min())`.\n\n    Returns:\n        SplineSpec\n            The created spline specification.\n    \"\"\"\n    extrapolation = extrapolation_factor * (values.max() - values.min())\n    if lower_bound is None:\n        lower_bound = values.min() - extrapolation\n    if upper_bound is None:\n        upper_bound = values.max() + extrapolation\n    if knots is None:\n        # Use equally spaced quantiles as knots\n        knots = np.linspace(lower_bound, upper_bound, df - degree + 1)[\n            1:-1\n        ].tolist()\n    return cls(\n        lower_bound=lower_bound,\n        upper_bound=upper_bound,\n        df=df,\n        degree=degree,\n        knots=knots,\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.snm.utils","title":"<code>utils</code>","text":""},{"location":"api_docs/#spectranorm.utils","title":"<code>utils</code>","text":""},{"location":"api_docs/#spectranorm.utils.general","title":"<code>general</code>","text":"<p>utils/general.py</p> <p>General utility functions for spectranorm.</p>"},{"location":"api_docs/#spectranorm.utils.general.ReportTimeFormatter","title":"<code>ReportTimeFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom log formatter that injects general.report_time().</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>class ReportTimeFormatter(logging.Formatter):\n    \"\"\"Custom log formatter that injects general.report_time().\"\"\"\n\n    def format(self, record: logging.LogRecord) -&gt; str:\n        record.report_time = report_time()\n        return super().format(record)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.ensure_dir","title":"<code>ensure_dir(file_name: Path) -&gt; Path</code>","text":"<p>Ensure that the directory for the given file name exists.</p>"},{"location":"api_docs/#spectranorm.utils.general.ensure_dir--parameters","title":"Parameters","text":"<p>file_name : Path     The file name for which to ensure the directory exists.</p>"},{"location":"api_docs/#spectranorm.utils.general.ensure_dir--returns","title":"Returns","text":"<p>Path     The original file name.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def ensure_dir(file_name: Path) -&gt; Path:\n    \"\"\"\n    Ensure that the directory for the given file name exists.\n\n    Parameters\n    ----------\n    file_name : Path\n        The file name for which to ensure the directory exists.\n\n    Returns\n    -------\n    Path\n        The original file name.\n    \"\"\"\n    file_name.parent.mkdir(parents=True, exist_ok=True)\n    return file_name\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.get_logger","title":"<code>get_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger</code>","text":"<p>Get a logger with standardized formatting and time reporting.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (usually name).</p> required <code>level</code> <code>int</code> <p>Logging level, default INFO.</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger with a custom ReportTimeFormatter.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def get_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger with standardized formatting and time reporting.\n\n    Args:\n        name: Logger name (usually __name__).\n        level: Logging level, default INFO.\n\n    Returns:\n        logging.Logger with a custom ReportTimeFormatter.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if not logger.hasHandlers():\n        handler = logging.StreamHandler()\n        formatter = ReportTimeFormatter(\n            fmt=\"%(report_time)s : [%(levelname)s] - %(name)s - %(message)s\",\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(level)\n    return logger\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.prepare_save_directory","title":"<code>prepare_save_directory(directory: Path, subdirectory: str = 'saved_model') -&gt; Path</code>","text":"<p>Prepare a directory to save a model.</p> <p>A subdirectory named 'saved_model' will be created if it does not exist. If this directory exists, but is not empty, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to a directory to save the model.</p> required <code>subdirectory</code> <code>str</code> <p>Name of the subdirectory to create (default: \"saved_model\").</p> <code>'saved_model'</code>"},{"location":"api_docs/#spectranorm.utils.general.prepare_save_directory--returns","title":"Returns","text":"<p>Path     The path to the created subdirectory.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def prepare_save_directory(directory: Path, subdirectory: str = \"saved_model\") -&gt; Path:\n    \"\"\"\n    Prepare a directory to save a model.\n\n    A subdirectory named 'saved_model' will be created if it does not exist.\n    If this directory exists, but is not empty, an error is raised.\n\n    Args:\n        directory (Path): Path to a directory to save the model.\n        subdirectory (str): Name of the subdirectory to create (default: \"saved_model\").\n\n    Returns\n    -------\n    Path\n        The path to the created subdirectory.\n    \"\"\"\n    # Check if the directory exists\n    if not directory.exists():\n        err = f\"Model Save Error: Directory '{directory}' does not exist.\"\n        raise FileNotFoundError(err)\n    # Check if the subdirectory exists and is empty\n    saved_model_dir = directory / subdirectory\n    if saved_model_dir.exists():\n        if any(saved_model_dir.iterdir()):\n            err = f\"Model Save Error: Directory '{saved_model_dir}' is not empty.\"\n            raise ValueError(err)\n    else:\n        saved_model_dir.mkdir(parents=True, exist_ok=True)\n\n    return saved_model_dir\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.report_time","title":"<code>report_time(*, relative_to: float | None = None, absolute: bool = False) -&gt; str | float</code>","text":"<p>Report the current time or the time elapsed since a given reference point.</p>"},{"location":"api_docs/#spectranorm.utils.general.report_time--parameters","title":"Parameters","text":"<p>relative_to : float | None     The reference time in seconds since the epoch. If None, report the current time. absolute : bool     If True, report the absolute time format (float). If False, report the time in a     human-readable format (str).</p>"},{"location":"api_docs/#spectranorm.utils.general.report_time--returns","title":"Returns","text":"<p>str | float     The current time or the elapsed time since the reference point.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def report_time(\n    *,\n    relative_to: float | None = None,\n    absolute: bool = False,\n) -&gt; str | float:\n    \"\"\"\n    Report the current time or the time elapsed since a given reference point.\n\n    Parameters\n    ----------\n    relative_to : float | None\n        The reference time in seconds since the epoch. If None, report the current time.\n    absolute : bool\n        If True, report the absolute time format (float). If False, report the time in a\n        human-readable format (str).\n\n    Returns\n    -------\n    str | float\n        The current time or the elapsed time since the reference point.\n    \"\"\"\n    if relative_to is not None:\n        elapsed = time.time() - relative_to\n        return elapsed if absolute else str(datetime.timedelta(seconds=elapsed))\n\n    now = time.time()\n    return (\n        now\n        if absolute\n        else datetime.datetime.fromtimestamp(\n            now,\n            tz=datetime.datetime.now().astimezone().tzinfo,\n        ).strftime(\"%Y-%m-%d %H:%M:%S\")\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.suppress_output","title":"<code>suppress_output() -&gt; Iterator[None]</code>","text":"<p>Context manager to suppress stdout and stderr output.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>@contextmanager\ndef suppress_output() -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to suppress stdout and stderr output.\n    \"\"\"\n    with Path(os.devnull).open(\"w\") as fnull:\n        old_stdout, old_stderr = os.dup(1), os.dup(2)\n        try:\n            os.dup2(fnull.fileno(), 1)  # Redirect stdout\n            os.dup2(fnull.fileno(), 2)  # Redirect stderr\n            yield\n        finally:\n            os.dup2(old_stdout, 1)  # Restore\n            os.dup2(old_stderr, 2)  # Restore\n            os.close(old_stdout)\n            os.close(old_stderr)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.validate_dataframe","title":"<code>validate_dataframe(dataframe: pd.DataFrame, column_names: list[str]) -&gt; None</code>","text":"<p>Validate the input DataFrame to ensure all required columns are available.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>pd.DataFrame The DataFrame to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the DataFrame is not valid.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def validate_dataframe(dataframe: pd.DataFrame, column_names: list[str]) -&gt; None:\n    \"\"\"\n    Validate the input DataFrame to ensure all required columns are available.\n\n    Args:\n        dataframe: pd.DataFrame\n            The DataFrame to validate.\n\n    Raises:\n        TypeError: If the DataFrame is not valid.\n    \"\"\"\n    if not isinstance(dataframe, pd.DataFrame):\n        err = \"Input data must be a pandas DataFrame.\"\n        raise TypeError(err)\n    # report if a column is missing\n    missing_columns = [col for col in column_names if col not in dataframe.columns]\n    if missing_columns:\n        err = f\"Missing columns in DataFrame: {', '.join(missing_columns)}\"\n        raise ValueError(err)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.general.validate_load_directory","title":"<code>validate_load_directory(directory: Path, subdirectory: str = 'saved_model') -&gt; Path</code>","text":"<p>Validate the directory structure for loading a model.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Path to the main directory.</p> required <code>subdirectory</code> <code>str</code> <p>Name of the subdirectory to check (default: \"saved_model\").</p> <code>'saved_model'</code>"},{"location":"api_docs/#spectranorm.utils.general.validate_load_directory--returns","title":"Returns","text":"<p>Path     The path to the validated subdirectory.</p> Source code in <code>src/spectranorm/utils/general.py</code> <pre><code>def validate_load_directory(directory: Path, subdirectory: str = \"saved_model\") -&gt; Path:\n    \"\"\"\n    Validate the directory structure for loading a model.\n\n    Args:\n        directory (Path): Path to the main directory.\n        subdirectory (str): Name of the subdirectory to check (default: \"saved_model\").\n\n    Returns\n    -------\n    Path\n        The path to the validated subdirectory.\n    \"\"\"\n    # Check if the directory exists\n    if not directory.exists():\n        err = f\"Model Load Error: Directory '{directory}' does not exist.\"\n        raise FileNotFoundError(err)\n    # Check if the subdirectory exists\n    saved_model_dir = directory / subdirectory\n    if not saved_model_dir.exists():\n        err = f\"Model Load Error: Directory '{saved_model_dir}' does not exist.\"\n        raise FileNotFoundError(err)\n\n    return saved_model_dir\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp","title":"<code>gsp</code>","text":"<p>utils/gsp.py</p> <p>Graph Signal Processing (GSP) functions for the Spectranorm package.</p>"},{"location":"api_docs/#spectranorm.utils.gsp.EigenmodeBasis","title":"<code>EigenmodeBasis</code>  <code>dataclass</code>","text":"<p>Data class to hold eigenmode basis information.</p> <p>Attributes:</p> Name Type Description <code>eigenvalues</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Eigenvalues of the Basis (n_modes,).</p> <code>eigenvectors</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Eigenvectors corresponding to the eigenvalues (n_modes, n_features).</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>@dataclass\nclass EigenmodeBasis:\n    \"\"\"\n    Data class to hold eigenmode basis information.\n\n    Attributes:\n        eigenvalues: np.ndarray\n            Eigenvalues of the Basis (n_modes,).\n        eigenvectors: np.ndarray\n            Eigenvectors corresponding to the eigenvalues (n_modes, n_features).\n    \"\"\"\n\n    eigenvalues: npt.NDArray[np.floating[Any]]\n    eigenvectors: npt.NDArray[np.floating[Any]]\n\n    # Additional attributes\n    def __post_init__(self) -&gt; None:\n        self.n_modes = self.eigenvalues.shape[0]\n        self.n_features = self.eigenvectors.shape[1]\n\n        if self.eigenvectors.shape[0] != self.n_modes:\n            err = (\n                f\"Eigenvectors must have {self.n_modes} modes, \"\n                f\"got {self.eigenvectors.shape[0]}.\"\n            )\n            raise ValueError(err)\n\n    @classmethod\n    def load(cls, filepath: str) -&gt; EigenmodeBasis:\n        \"\"\"\n        Load an EigenmodeBasis instance from a joblib file.\n\n        Args:\n            filepath: str\n                Path to the joblib file.\n\n        Returns:\n            EigenmodeBasis instance\n        \"\"\"\n        data = joblib.load(filepath)\n        # Expecting the saved file to contain a dict with these keys:\n        # 'eigenvalues', 'eigenvectors'\n        return cls(eigenvalues=data[\"eigenvalues\"], eigenvectors=data[\"eigenvectors\"])\n\n    def save(self, filepath: str) -&gt; None:\n        \"\"\"\n        Save the EigenmodeBasis instance to a joblib file.\n\n        Args:\n            filepath: str\n                Path to save the joblib file.\n        \"\"\"\n        data = {\n            \"eigenvalues\": self.eigenvalues,\n            \"eigenvectors\": self.eigenvectors,\n        }\n        joblib.dump(data, filepath)\n\n    def encode(\n        self,\n        signals: npt.NDArray[np.floating[Any]],\n        n_modes: int | None = None,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Encode a signal using the eigenmode basis.\n\n        Args:\n            signals: np.ndarray\n                Signals to encode (n_signals, n_features).\n            n_modes: int | None\n                Number of modes to use for encoding. If None, use all available modes.\n\n        Returns:\n            np.ndarray\n                Encoded signal in the eigenmode basis (n_signals, n_modes).\n        \"\"\"\n        if signals.shape[-1] != self.n_features:\n            err = (\n                f\"Signal must have {self.n_features} features, got {signals.shape[-1]}.\"\n            )\n            raise ValueError(err)\n\n        if n_modes is None:\n            n_modes = self.n_modes\n\n        return signals @ self.eigenvectors[:n_modes].T\n\n    def load_and_encode_data_list(\n        self,\n        data_paths: list[str],\n        output_path: str | None,\n        n_modes: int | None = None,\n        data_loader: Callable[[str], npt.NDArray[np.floating[Any]]] = np.load,\n    ) -&gt; npt.NDArray[np.floating[Any]]:\n        \"\"\"\n        Load and encode a list of data using the eigenmode basis.\n\n        This function uses the data_loader to load data from the provided paths\n        (for each sample), and then encodes the data using the eigenmode basis.\n        Finally, it can save the encoded data to a specified output path, enabling\n        efficient reuse of the encoded data in subsequent analyses.\n\n        Args:\n            data_paths: list[str]\n                List of data identifier strings to load data using the data_loader.\n            output_path: str | None\n                Path to save the encoded data (as a .npy file). If None, the encoded\n                data is not saved.\n            n_modes: int | None\n                Number of modes to use for encoding. If None, all modes are used.\n            data_loader: Callable[[str], np.ndarray] = np.load\n\n        data_loader: Callable[[str], np.ndarray]\n            A callable function to load data from a string identifier (e.g., file path).\n            Note that by default, this is set to load numpy arrays from .npy files.\n            However, you may want to override this with a custom data loader\n            that fits your data format and loading requirements.\n\n            For instance, `snm.utils.nitools.compute_fslr_thickness` is an example of a\n            custom data loader which can be used to load fs-LR thickness values from a\n            path to a subject's FreeSurfer directory.\n\n            If you don't want to use a custom data loader, you can alternatively convert\n            all individual data files to numpy arrays and use the default data loader,\n            which loads numpy arrays from .npy files.\n\n        Returns:\n            np.ndarray: (n_samples, n_modes)\n                Encoded data of all individuals as a single numpy array.\n        \"\"\"\n        if n_modes is None:\n            n_modes = self.n_modes\n        # Load and encode data\n        encoded_data = np.nan * np.zeros(\n            (len(data_paths), n_modes),\n            dtype=self.eigenvectors.dtype,\n        )\n        for i, data_path in enumerate(data_paths):\n            # Load (using data_loader) and Encode\n            encoded_data[i, :] = self.encode(\n                data_loader(data_path),\n                n_modes=n_modes,\n            )\n\n        if output_path is not None:\n            np.save(output_path, encoded_data)\n\n        return encoded_data\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.EigenmodeBasis.encode","title":"<code>encode(signals: npt.NDArray[np.floating[Any]], n_modes: int | None = None) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Encode a signal using the eigenmode basis.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Signals to encode (n_signals, n_features).</p> required <code>n_modes</code> <code>int | None</code> <p>int | None Number of modes to use for encoding. If None, use all available modes.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Encoded signal in the eigenmode basis (n_signals, n_modes).</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def encode(\n    self,\n    signals: npt.NDArray[np.floating[Any]],\n    n_modes: int | None = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Encode a signal using the eigenmode basis.\n\n    Args:\n        signals: np.ndarray\n            Signals to encode (n_signals, n_features).\n        n_modes: int | None\n            Number of modes to use for encoding. If None, use all available modes.\n\n    Returns:\n        np.ndarray\n            Encoded signal in the eigenmode basis (n_signals, n_modes).\n    \"\"\"\n    if signals.shape[-1] != self.n_features:\n        err = (\n            f\"Signal must have {self.n_features} features, got {signals.shape[-1]}.\"\n        )\n        raise ValueError(err)\n\n    if n_modes is None:\n        n_modes = self.n_modes\n\n    return signals @ self.eigenvectors[:n_modes].T\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.EigenmodeBasis.load","title":"<code>load(filepath: str) -&gt; EigenmodeBasis</code>  <code>classmethod</code>","text":"<p>Load an EigenmodeBasis instance from a joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>str Path to the joblib file.</p> required <p>Returns:</p> Type Description <code>EigenmodeBasis</code> <p>EigenmodeBasis instance</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str) -&gt; EigenmodeBasis:\n    \"\"\"\n    Load an EigenmodeBasis instance from a joblib file.\n\n    Args:\n        filepath: str\n            Path to the joblib file.\n\n    Returns:\n        EigenmodeBasis instance\n    \"\"\"\n    data = joblib.load(filepath)\n    # Expecting the saved file to contain a dict with these keys:\n    # 'eigenvalues', 'eigenvectors'\n    return cls(eigenvalues=data[\"eigenvalues\"], eigenvectors=data[\"eigenvectors\"])\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.EigenmodeBasis.load_and_encode_data_list","title":"<code>load_and_encode_data_list(data_paths: list[str], output_path: str | None, n_modes: int | None = None, data_loader: Callable[[str], npt.NDArray[np.floating[Any]]] = np.load) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Load and encode a list of data using the eigenmode basis.</p> <p>This function uses the data_loader to load data from the provided paths (for each sample), and then encodes the data using the eigenmode basis. Finally, it can save the encoded data to a specified output path, enabling efficient reuse of the encoded data in subsequent analyses.</p> <p>Parameters:</p> Name Type Description Default <code>data_paths</code> <code>list[str]</code> <p>list[str] List of data identifier strings to load data using the data_loader.</p> required <code>output_path</code> <code>str | None</code> <p>str | None Path to save the encoded data (as a .npy file). If None, the encoded data is not saved.</p> required <code>n_modes</code> <code>int | None</code> <p>int | None Number of modes to use for encoding. If None, all modes are used.</p> <code>None</code> <code>data_loader</code> <code>Callable[[str], NDArray[floating[Any]]]</code> <p>Callable[[str], np.ndarray] = np.load</p> <code>load</code> Callable[[str], np.ndarray] <p>A callable function to load data from a string identifier (e.g., file path). Note that by default, this is set to load numpy arrays from .npy files. However, you may want to override this with a custom data loader that fits your data format and loading requirements.</p> <p>For instance, <code>snm.utils.nitools.compute_fslr_thickness</code> is an example of a custom data loader which can be used to load fs-LR thickness values from a path to a subject's FreeSurfer directory.</p> <p>If you don't want to use a custom data loader, you can alternatively convert all individual data files to numpy arrays and use the default data loader, which loads numpy arrays from .npy files.</p> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray: (n_samples, n_modes) Encoded data of all individuals as a single numpy array.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def load_and_encode_data_list(\n    self,\n    data_paths: list[str],\n    output_path: str | None,\n    n_modes: int | None = None,\n    data_loader: Callable[[str], npt.NDArray[np.floating[Any]]] = np.load,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Load and encode a list of data using the eigenmode basis.\n\n    This function uses the data_loader to load data from the provided paths\n    (for each sample), and then encodes the data using the eigenmode basis.\n    Finally, it can save the encoded data to a specified output path, enabling\n    efficient reuse of the encoded data in subsequent analyses.\n\n    Args:\n        data_paths: list[str]\n            List of data identifier strings to load data using the data_loader.\n        output_path: str | None\n            Path to save the encoded data (as a .npy file). If None, the encoded\n            data is not saved.\n        n_modes: int | None\n            Number of modes to use for encoding. If None, all modes are used.\n        data_loader: Callable[[str], np.ndarray] = np.load\n\n    data_loader: Callable[[str], np.ndarray]\n        A callable function to load data from a string identifier (e.g., file path).\n        Note that by default, this is set to load numpy arrays from .npy files.\n        However, you may want to override this with a custom data loader\n        that fits your data format and loading requirements.\n\n        For instance, `snm.utils.nitools.compute_fslr_thickness` is an example of a\n        custom data loader which can be used to load fs-LR thickness values from a\n        path to a subject's FreeSurfer directory.\n\n        If you don't want to use a custom data loader, you can alternatively convert\n        all individual data files to numpy arrays and use the default data loader,\n        which loads numpy arrays from .npy files.\n\n    Returns:\n        np.ndarray: (n_samples, n_modes)\n            Encoded data of all individuals as a single numpy array.\n    \"\"\"\n    if n_modes is None:\n        n_modes = self.n_modes\n    # Load and encode data\n    encoded_data = np.nan * np.zeros(\n        (len(data_paths), n_modes),\n        dtype=self.eigenvectors.dtype,\n    )\n    for i, data_path in enumerate(data_paths):\n        # Load (using data_loader) and Encode\n        encoded_data[i, :] = self.encode(\n            data_loader(data_path),\n            n_modes=n_modes,\n        )\n\n    if output_path is not None:\n        np.save(output_path, encoded_data)\n\n    return encoded_data\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.EigenmodeBasis.save","title":"<code>save(filepath: str) -&gt; None</code>","text":"<p>Save the EigenmodeBasis instance to a joblib file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>str Path to save the joblib file.</p> required Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def save(self, filepath: str) -&gt; None:\n    \"\"\"\n    Save the EigenmodeBasis instance to a joblib file.\n\n    Args:\n        filepath: str\n            Path to save the joblib file.\n    \"\"\"\n    data = {\n        \"eigenvalues\": self.eigenvalues,\n        \"eigenvectors\": self.eigenvectors,\n    }\n    joblib.dump(data, filepath)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.compute_random_walk_laplacian_eigenmodes","title":"<code>compute_random_walk_laplacian_eigenmodes(adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]], num_eigenvalues: int = 100) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]</code>","text":"<p>Compute the eigenvalues of the random walk Laplacian.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix | NDArray[floating[Any]]</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <code>num_eigenvalues</code> <code>int</code> <p>int Number of eigenvalues to compute.</p> <code>100</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>eigenvalues, eigenvectors: (np.ndarray, np.ndarray) Eigenvalues and eigenvectors of the random walk Laplacian.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def compute_random_walk_laplacian_eigenmodes(\n    adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]],\n    num_eigenvalues: int = 100,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]:\n    \"\"\"\n    Compute the eigenvalues of the random walk Laplacian.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n        num_eigenvalues: int\n            Number of eigenvalues to compute.\n\n    Returns:\n        eigenvalues, eigenvectors: (np.ndarray, np.ndarray)\n            Eigenvalues and eigenvectors of the random walk Laplacian.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Ensure the adjacency matrix is symmetric\n    adjacency_matrix = sparse.csr_matrix((adjacency_matrix + adjacency_matrix.T) * 0.5)\n\n    # Convert to transition matrix\n    transition_matrix = convert_adjacency_to_transition_matrix(adjacency_matrix)\n\n    # Use shift invert mode to compute the eigenvalues for the transition matrix,\n    # starting with the trivial eigenvalue 1\n    initial_vector = np.ones(adjacency_matrix.shape[0])\n    lambdas, vectors = sparse.linalg.eigsh(\n        transition_matrix,\n        k=num_eigenvalues + 1,\n        which=\"LM\",\n        v0=initial_vector,\n    )\n\n    # Convert to Laplacian eigenvalues\n    lambdas = 1 - lambdas\n    lambda_idx = np.argsort(lambdas)\n    lambdas = lambdas[lambda_idx]\n    vectors = vectors[:, lambda_idx]\n\n    return (lambdas, np.asarray(vectors).T)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.compute_symmetric_normalized_laplacian_eigenmodes","title":"<code>compute_symmetric_normalized_laplacian_eigenmodes(adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]], num_eigenvalues: int = 100) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]</code>","text":"<p>Compute the eigenvalues of the symmetric normalized Laplacian.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix | NDArray[floating[Any]]</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <code>num_eigenvalues</code> <code>int</code> <p>int Number of eigenvalues to compute.</p> <code>100</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>eigenvalues, eigenvectors: (np.ndarray, np.ndarray) Eigenvalues and eigenvectors of the symmetric normalized Laplacian.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def compute_symmetric_normalized_laplacian_eigenmodes(\n    adjacency_matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]],\n    num_eigenvalues: int = 100,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]:\n    \"\"\"\n    Compute the eigenvalues of the symmetric normalized Laplacian.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n        num_eigenvalues: int\n            Number of eigenvalues to compute.\n\n    Returns:\n        eigenvalues, eigenvectors: (np.ndarray, np.ndarray)\n            Eigenvalues and eigenvectors of the symmetric normalized Laplacian.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Ensure the adjacency matrix is symmetric\n    adjacency_matrix = sparse.csr_matrix((adjacency_matrix + adjacency_matrix.T) * 0.5)\n\n    # Perform symmetric normalization\n    normalized_matrix = perform_symmetric_normalization(adjacency_matrix)\n\n    # Use shift invert mode to compute the eigenvalues for the\n    # normalized adjacency matrix\n    lambdas, vectors = sparse.linalg.eigsh(\n        normalized_matrix,\n        k=num_eigenvalues + 1,\n        which=\"LM\",\n    )\n    # Note the largest eigenvalues of the adjacency matrix correspond to\n    # the smallest eigenvalues of the Laplacian\n\n    # Convert to Laplacian eigenvalues\n    lambdas = 1 - lambdas\n    lambda_idx = np.argsort(lambdas)\n    lambdas = lambdas[lambda_idx]\n    vectors = vectors[:, lambda_idx]\n\n    return (lambdas, vectors.T)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.convert_adjacency_to_transition_matrix","title":"<code>convert_adjacency_to_transition_matrix(adjacency_matrix: sparse.spmatrix) -&gt; sparse.csr_matrix</code>","text":"<p>Convert an adjacency matrix to a transition matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.spmatrix Transition matrix.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def convert_adjacency_to_transition_matrix(\n    adjacency_matrix: sparse.spmatrix,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Convert an adjacency matrix to a transition matrix.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n\n    Returns:\n        sparse.spmatrix\n            Transition matrix.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n\n    # Compute the degree matrix\n    degrees = np.array(adjacency_matrix.sum(axis=1)).flatten()\n    degree_matrix_inv = sparse.diags(1.0 / degrees)\n\n    # Create the transition matrix\n    return sparse.csr_matrix(degree_matrix_inv @ adjacency_matrix)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.make_csr_matrix","title":"<code>make_csr_matrix(matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]]) -&gt; sparse.csr_matrix</code>","text":"<p>Ensure the input matrix is in CSR format.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def make_csr_matrix(\n    matrix: sparse.spmatrix | npt.NDArray[np.floating[Any]],\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Ensure the input matrix is in CSR format.\n    \"\"\"\n    if not sparse.issparse(matrix):\n        matrix = sparse.csr_matrix(np.array(matrix))\n    if not sparse.isspmatrix_csr(matrix):\n        matrix = sparse.csr_matrix(matrix)\n    return matrix\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.gsp.perform_symmetric_normalization","title":"<code>perform_symmetric_normalization(adjacency_matrix: sparse.spmatrix) -&gt; sparse.csr_matrix</code>","text":"<p>Perform symmetric normalization on the adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>spmatrix</code> <p>sparse.spmatrix The adjacency matrix of the graph.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>sparse.spmatrix Symmetrically normalized adjacency matrix.</p> Source code in <code>src/spectranorm/utils/gsp.py</code> <pre><code>def perform_symmetric_normalization(\n    adjacency_matrix: sparse.spmatrix,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Perform symmetric normalization on the adjacency matrix.\n\n    Args:\n        adjacency_matrix: sparse.spmatrix\n            The adjacency matrix of the graph.\n\n    Returns:\n        sparse.spmatrix\n            Symmetrically normalized adjacency matrix.\n    \"\"\"\n    # Convert to CSR format if in a different format\n    adjacency_matrix = make_csr_matrix(adjacency_matrix)\n    # Compute the degree matrix\n    degrees = np.array(adjacency_matrix.sum(axis=1)).flatten()\n    degree_matrix_inv_sqrt = sparse.diags(1.0 / np.sqrt(degrees))\n\n    # Perform symmetric normalization\n    return sparse.csr_matrix(\n        degree_matrix_inv_sqrt @ adjacency_matrix @ degree_matrix_inv_sqrt,\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics","title":"<code>metrics</code>","text":"<p>utils/metrics.py</p> <p>Utility functions for computing various model metrics in the Spectranorm package.</p>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_bic","title":"<code>compute_bic(model_log_likelihoods: npt.NDArray[np.floating[Any]], n_params: int, n_samples: int) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Bayesian Information Criterion (BIC) for model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>model_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the model. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>n_params</code> <code>int</code> <p>int Number of parameters in the model.</p> required <code>n_samples</code> <code>int</code> <p>int Number of samples used in the model.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Bayesian Information Criterion. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_bic(\n    model_log_likelihoods: npt.NDArray[np.floating[Any]],\n    n_params: int,\n    n_samples: int,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Bayesian Information Criterion (BIC) for model evaluation.\n\n    Args:\n        model_log_likelihoods: np.ndarray\n            Log likelihoods from the model. (n_samples) or (n_samples, n_outputs)\n            if multiple outputs are being assessed.\n        n_params: int\n            Number of parameters in the model.\n        n_samples: int\n            Number of samples used in the model.\n\n    Returns:\n        np.ndarray\n            Bayesian Information Criterion. (n_outputs,) if multiple outputs are\n            assessed, otherwise a scalar.\n    \"\"\"\n    return np.asarray(\n        -2 * np.mean(model_log_likelihoods, axis=0) + n_params * np.log(n_samples),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_expv","title":"<code>compute_expv(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Explained Variance (EXPV) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Explained Variance. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_expv(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Explained Variance (EXPV) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Explained Variance. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(1 - np.var(y - y_pred, axis=0) / np.var(y, axis=0))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_mae","title":"<code>compute_mae(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Absolute Error (MAE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Absolute Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mae(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Absolute Error (MAE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Absolute Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean(np.abs(y - y_pred), axis=0))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_mape","title":"<code>compute_mape(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Absolute Percentage Error (MAPE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Absolute Percentage Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mape(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Absolute Percentage Error (MAPE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Absolute Percentage Error. (n_outputs,) if multiple outputs are\n            assessed, otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean(np.abs((y - y_pred) / y), axis=0) * 100)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_mse","title":"<code>compute_mse(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Squared Error (MSE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Squared Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_mse(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Squared Error (MSE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Mean Squared Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.mean((y - y_pred) ** 2, axis=0))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_msll","title":"<code>compute_msll(model_log_likelihoods: npt.NDArray[np.floating[Any]], baseline_log_likelihoods: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Mean Standardized Log Loss (MSLL) based on model and baseline log likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>model_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the model. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>baseline_log_likelihoods</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihoods from the baseline model (a trivial model). Same shape as model_log_likelihoods.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Mean Standardized Log Loss. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_msll(\n    model_log_likelihoods: npt.NDArray[np.floating[Any]],\n    baseline_log_likelihoods: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Mean Standardized Log Loss (MSLL) based on model and baseline log\n    likelihoods.\n\n    Args:\n        model_log_likelihoods: np.ndarray\n            Log likelihoods from the model. (n_samples) or (n_samples, n_outputs)\n            if multiple outputs are being assessed.\n        baseline_log_likelihoods: np.ndarray\n            Log likelihoods from the baseline model (a trivial model). Same shape as\n            model_log_likelihoods.\n\n    Returns:\n        np.ndarray\n            Mean Standardized Log Loss. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(\n        -(\n            np.mean(model_log_likelihoods, axis=0)\n            - np.mean(baseline_log_likelihoods, axis=0)\n        ),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_r2","title":"<code>compute_r2(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute R-squared (coefficient of determination) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray R-squared value. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_r2(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute R-squared (coefficient of determination) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            R-squared value. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    ss_res = np.sum((y - y_pred) ** 2, axis=0)\n    ss_tot = np.sum((y - np.mean(y, axis=0)) ** 2, axis=0)\n    return np.asarray(1 - (ss_res / ss_tot))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.metrics.compute_rmse","title":"<code>compute_rmse(y: npt.NDArray[np.floating[Any]], y_pred: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute Root Mean Squared Error (RMSE) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray True values. (n_samples) or (n_samples, n_outputs) if multiple outputs are being assessed.</p> required <code>y_pred</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Root Mean Squared Error. (n_outputs,) if multiple outputs are assessed, otherwise a scalar.</p> Source code in <code>src/spectranorm/utils/metrics.py</code> <pre><code>def compute_rmse(\n    y: npt.NDArray[np.floating[Any]],\n    y_pred: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute Root Mean Squared Error (RMSE) between true and predicted values.\n\n    Args:\n        y: np.ndarray\n            True values. (n_samples) or (n_samples, n_outputs) if multiple outputs\n            are being assessed.\n        y_pred: np.ndarray\n            Predicted values. (n_samples) or (n_samples, n_outputs) same shape as y.\n\n    Returns:\n        np.ndarray\n            Root Mean Squared Error. (n_outputs,) if multiple outputs are assessed,\n            otherwise a scalar.\n    \"\"\"\n    return np.asarray(np.sqrt(compute_mse(y, y_pred)))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools","title":"<code>nitools</code>","text":"<p>utils/nitools.py</p> <p>Neuroimaging functions to deal with imaging files for spectranorm.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_adaptive_area_barycentric_transformation","title":"<code>compute_adaptive_area_barycentric_transformation(source_vertices: npt.NDArray[np.floating[Any]], source_triangles: npt.NDArray[np.integer[Any]], source_vertex_areas: npt.NDArray[np.floating[Any]], target_vertices: npt.NDArray[np.floating[Any]], target_triangles: npt.NDArray[np.integer[Any]], target_vertex_areas: npt.NDArray[np.floating[Any]], source_vertex_mask: npt.NDArray[np.bool_] | None = None, k_candidates: int = 10, tol: float = 1e-10) -&gt; sparse.csr_matrix</code>","text":"<p>Computes an adaptive area barycentric transformation matrix from the source mesh to the target mesh (similar to workbench command's ADAP_BARY_AREA option).</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_adaptive_area_barycentric_transformation--parameters","title":"Parameters","text":"<p>source_vertices : np.ndarray     Array of shape (N1, 3) containing the coordinates of the source vertices. source_triangles : np.ndarray     Array of shape (M1, 3) containing the indices of the source triangles. source_vertex_areas : np.ndarray     Array of shape (N1,) containing the areas of the source vertices. target_vertices : np.ndarray     Array of shape (N2, 3) containing the coordinates of the target vertices. target_triangles : np.ndarray     Array of shape (M2, 3) containing the indices of the target triangles. target_vertex_areas : np.ndarray     Array of shape (N2,) containing the areas of the target vertices. source_vertex_mask : np.ndarray | None     (N1,) Optional mask for source vertices. k_candidates : int     Number of candidate triangles to consider for each target vertex. tol : float     Tolerance for barycentric coordinate computation.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_adaptive_area_barycentric_transformation--returns","title":"Returns","text":"<p>sparse.csr_matrix     Sparse matrix of shape (N2, N1) representing the transformation.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_adaptive_area_barycentric_transformation(\n    source_vertices: npt.NDArray[np.floating[Any]],  # (n_source, 3)\n    source_triangles: npt.NDArray[np.integer[Any]],  # (n_tri_source, 3)\n    source_vertex_areas: npt.NDArray[np.floating[Any]],  # (n_source,)\n    target_vertices: npt.NDArray[np.floating[Any]],  # (n_target, 3)\n    target_triangles: npt.NDArray[np.integer[Any]],  # (n_tri_target, 3)\n    target_vertex_areas: npt.NDArray[np.floating[Any]],  # (n_target,)\n    source_vertex_mask: npt.NDArray[np.bool_] | None = None,  # (n_source,)\n    k_candidates: int = 10,  # number of candidate triangles to check\n    tol: float = 1e-10,  # tolerance for barycentric coords\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Computes an adaptive area barycentric transformation matrix from the source mesh to\n    the target mesh (similar to workbench command's ADAP_BARY_AREA option).\n\n    Parameters\n    ----------\n    source_vertices : np.ndarray\n        Array of shape (N1, 3) containing the coordinates of the source vertices.\n    source_triangles : np.ndarray\n        Array of shape (M1, 3) containing the indices of the source triangles.\n    source_vertex_areas : np.ndarray\n        Array of shape (N1,) containing the areas of the source vertices.\n    target_vertices : np.ndarray\n        Array of shape (N2, 3) containing the coordinates of the target vertices.\n    target_triangles : np.ndarray\n        Array of shape (M2, 3) containing the indices of the target triangles.\n    target_vertex_areas : np.ndarray\n        Array of shape (N2,) containing the areas of the target vertices.\n    source_vertex_mask : np.ndarray | None\n        (N1,) Optional mask for source vertices.\n    k_candidates : int\n        Number of candidate triangles to consider for each target vertex.\n    tol : float\n        Tolerance for barycentric coordinate computation.\n\n    Returns\n    -------\n    sparse.csr_matrix\n        Sparse matrix of shape (N2, N1) representing the transformation.\n    \"\"\"\n    # --- Step 1: Compute forward and reverse barycentric transformations ---\n    forward_transform = compute_barycentric_transformation(\n        source_vertices=source_vertices,\n        source_triangles=source_triangles,\n        target_vertices=target_vertices,\n        k_candidates=k_candidates,\n        tol=tol,\n    ).tocsr()\n\n    reverse_transform = compute_barycentric_transformation(\n        source_vertices=target_vertices,\n        source_triangles=target_triangles,\n        target_vertices=source_vertices,\n        k_candidates=k_candidates,\n        tol=tol,\n    ).tocsr()\n\n    # Convert reverse scatter to gather (transpose)\n    reverse_gather_transform = reverse_transform.T.tocsr()\n\n    # --- Step 2: Decide forward vs reverse for each target vertex\n    # Choose the mapping with higher number of sources involved\n    # Count non-zero entries in each row of the forward and reverse matrices\n    # Note: Since we have CSR format, we can use indptr for efficiency\n    forward_sources = np.diff(forward_transform.indptr)\n    reverse_sources = np.diff(reverse_gather_transform.indptr)\n\n    # Pick forward if it has &gt;= as many sources as reverse\n    use_forward = forward_sources &gt;= reverse_sources\n\n    # --- Step 3: Build the adaptive gather matrix\n    adap_gather = (\n        sparse.diags(use_forward) @ forward_transform\n        + sparse.diags(~use_forward) @ reverse_gather_transform\n    )\n\n    # --- Step 4: Multiply each row by target vertex area (area contributions)\n    adap_gather = adap_gather.tocsr()\n    adap_gather = sparse.csr_matrix(sparse.diags(target_vertex_areas) @ adap_gather)\n\n    # --- Step 5: Correction sum (sum over target vertices for each source vertex)\n    correction_sum = np.asarray(adap_gather.sum(axis=0)).ravel()\n\n    # --- Step 6: Scale by source_areas / correction_sum (scatter normalization)\n    scale = np.zeros_like(correction_sum)\n    valid = correction_sum &gt; 0\n    scale[valid] = source_vertex_areas[valid] / correction_sum[valid]\n    adap_gather = sparse.csr_matrix(adap_gather @ sparse.diags(scale))\n\n    # --- Step 7: ROI masking\n    # (remove columns corresponding to masked-out source vertices)\n    if source_vertex_mask is not None:\n        adap_gather = sparse.csr_matrix(\n            adap_gather.multiply(\n                source_vertex_mask[None, :],\n            ),\n        )\n\n    # --- Step 8: Normalize each row to sum to 1\n    row_sums = np.asarray(adap_gather.sum(axis=1)).ravel()\n    row_scale = np.ones_like(row_sums)\n    valid_rows = row_sums &gt; 0\n    row_scale[valid_rows] = 1.0 / row_sums[valid_rows]\n    adap_gather = sparse.diags(row_scale) @ adap_gather\n\n    return sparse.csr_matrix(adap_gather)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_barycentric_transformation","title":"<code>compute_barycentric_transformation(source_vertices: npt.NDArray[np.floating[Any]], source_triangles: npt.NDArray[np.integer[Any]], target_vertices: npt.NDArray[np.floating[Any]], k_candidates: int = 10, tol: float = 1e-10) -&gt; sparse.coo_matrix</code>","text":"<p>Compute sparse barycentric transformation matrix from target_vertices to source_vertices.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_barycentric_transformation--parameters","title":"Parameters","text":"<p>source_vertices : numpy.ndarray     Array of shape (N1, 3) containing the coordinates of the source vertices. source_triangles : numpy.ndarray     Array of shape (M1, 3) containing the indices of the source triangles. target_vertices : numpy.ndarray     Array of shape (N2, 3) containing the coordinates of the target vertices. k_candidates : int     Number of candidate triangles to consider for each target vertex. (default: 10) tol : float     Tolerance for inside-triangle check. (default: 1e-10)</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_barycentric_transformation--returns","title":"Returns","text":"<p>barycentric_transformation : scipy.sparse.csr_matrix     Sparse matrix of shape (N2, N1) representing the barycentric transformation.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_barycentric_transformation(\n    source_vertices: npt.NDArray[np.floating[Any]],\n    source_triangles: npt.NDArray[np.integer[Any]],\n    target_vertices: npt.NDArray[np.floating[Any]],\n    k_candidates: int = 10,\n    tol: float = 1e-10,\n) -&gt; sparse.coo_matrix:\n    \"\"\"\n    Compute sparse barycentric transformation matrix from target_vertices to\n    source_vertices.\n\n    Parameters\n    ----------\n    source_vertices : numpy.ndarray\n        Array of shape (N1, 3) containing the coordinates of the source vertices.\n    source_triangles : numpy.ndarray\n        Array of shape (M1, 3) containing the indices of the source triangles.\n    target_vertices : numpy.ndarray\n        Array of shape (N2, 3) containing the coordinates of the target vertices.\n    k_candidates : int\n        Number of candidate triangles to consider for each target vertex. (default: 10)\n    tol : float\n        Tolerance for inside-triangle check. (default: 1e-10)\n\n    Returns\n    -------\n    barycentric_transformation : scipy.sparse.csr_matrix\n        Sparse matrix of shape (N2, N1) representing the barycentric transformation.\n    \"\"\"\n    # Compute source triangle centroids\n    source_triangle_centroids = np.mean(source_vertices[source_triangles], axis=1)\n\n    # Build KD-tree on triangle centroids to get a quick nearest triangle search\n    triangle_tree = spatial.cKDTree(source_triangle_centroids)\n\n    # Find the K nearest source triangles for each target vertex\n    _, candidate_triangles = triangle_tree.query(target_vertices, k=k_candidates)\n\n    # Reshape candidate_triangles to ensure it's 2D\n    candidate_triangles = candidate_triangles.reshape(-1, k_candidates)\n\n    # Compute coordinates of all vertices in the candidate triangles\n    a = source_vertices[source_triangles][candidate_triangles, 0, :]\n    b = source_vertices[source_triangles][candidate_triangles, 1, :]\n    c = source_vertices[source_triangles][candidate_triangles, 2, :]\n\n    # Compute vectors for barycentric coordinates\n    # v0 = b - a, v1 = c - a, v2 = target - a\n    v0 = b - a\n    v1 = c - a\n    v2 = target_vertices[:, None, :] - a\n\n    # Compute vector dot products\n    d00 = np.einsum(\"ijk,ijk-&gt;ij\", v0, v0)\n    d01 = np.einsum(\"ijk,ijk-&gt;ij\", v0, v1)\n    d11 = np.einsum(\"ijk,ijk-&gt;ij\", v1, v1)\n    d20 = np.einsum(\"ijk,ijk-&gt;ij\", v2, v0)\n    d21 = np.einsum(\"ijk,ijk-&gt;ij\", v2, v1)\n\n    # Compute barycentric coordinates\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        den = d00 * d11 - d01 * d01\n        beta = (d11 * d20 - d01 * d21) / den\n        gamma = (d00 * d21 - d01 * d20) / den\n        alpha = 1 - beta - gamma\n\n    # Check if target is inside any of the candidate triangles\n    is_inside = (alpha &gt;= -tol) &amp; (beta &gt;= -tol) &amp; (gamma &gt;= -tol) &amp; (np.abs(den) &gt; tol)\n    found_valid = is_inside.any(axis=1)\n\n    # Choose the optimal candidate\n    selected_candidate = np.argmax(is_inside, axis=1)\n    selected_candidate[~found_valid] = -1  # Mark cases with no valid candidates\n\n    # Start preparing the sparse transformation matrix\n    rows: list[int] = []\n    cols: list[int] = []\n    data: list[float] = []\n\n    valid_idx = np.where(selected_candidate &gt;= 0)[0]\n    if valid_idx.size &gt; 0:\n        selected_triangles = candidate_triangles[\n            valid_idx,\n            selected_candidate[valid_idx],\n        ]\n        selected_vertex_indices = source_triangles[selected_triangles]\n        alpha_values = alpha[valid_idx, selected_candidate[valid_idx]]\n        beta_values = beta[valid_idx, selected_candidate[valid_idx]]\n        gamma_values = gamma[valid_idx, selected_candidate[valid_idx]]\n\n        rows.extend(np.repeat(valid_idx, 3))\n        cols.extend(selected_vertex_indices.flatten())\n        bary_coords = np.stack([alpha_values, beta_values, gamma_values], axis=1)\n        data.extend(bary_coords.flatten())\n\n    # Fallback: nearest vertex for points without valid candidates\n    invalid_idx = np.where(selected_candidate &lt; 0)[0]\n    if invalid_idx.size &gt; 0:\n        vertex_tree = spatial.cKDTree(source_vertices)\n\n        _, nearest_vertices = vertex_tree.query(target_vertices[invalid_idx], k=1)\n        rows.extend(invalid_idx)\n        cols.extend(nearest_vertices.reshape(-1))\n        data.extend(np.ones(invalid_idx.size, dtype=float))\n\n    # Build and return the barycentric transformation matrix\n    return sparse.coo_matrix(\n        (np.asarray(data), (np.asarray(rows), np.asarray(cols))),\n        shape=(target_vertices.shape[0], source_vertices.shape[0]),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fslr_thickness","title":"<code>compute_fslr_thickness(subject_freesurfer_directory: str) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the fs_LR thickness from FreeSurfer output directory.</p> <p>This function can be passed to a <code>snm.utils.gsp.EigenmodeBasis</code> as a dataloader for the <code>load_and_encode_data_list</code> method.</p> <p>Note: Assuming FreeSurfer's recon-all is completed, this function automatically maps thickness estimates from subject's native cortical surface space onto the standard fs_LR space.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fslr_thickness--parameters","title":"Parameters","text":"<p>subject_freesurfer_directory : str     Path to the FreeSurfer subject directory.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fslr_thickness--returns","title":"Returns","text":"<p>np.ndarray     Array of shape (59412,) containing the thickness values for fs_LR vertices     (excluding the medial wall).</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_fslr_thickness(\n    subject_freesurfer_directory: str,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the fs_LR thickness from FreeSurfer output directory.\n\n    This function can be passed to a `snm.utils.gsp.EigenmodeBasis` as a dataloader\n    for the `load_and_encode_data_list` method.\n\n    Note: Assuming FreeSurfer's recon-all is completed, this function automatically\n    maps thickness estimates from subject's native cortical surface space onto the\n    standard fs_LR space.\n\n    Parameters\n    ----------\n    subject_freesurfer_directory : str\n        Path to the FreeSurfer subject directory.\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (59412,) containing the thickness values for fs_LR vertices\n        (excluding the medial wall).\n    \"\"\"\n    cifti_mask = get_fslr_surface_indices_from_cifti()\n    adap_area_barycentric_transformation = compute_fsnative_to_fslr32k_transformation(\n        subject_freesurfer_directory,\n    )[cifti_mask, :]\n    fsnative_thickness = np.concatenate(\n        [\n            nib.freesurfer.io.read_morph_data(  # type: ignore[no-untyped-call]\n                f\"{subject_freesurfer_directory}/surf/lh.thickness\",\n            ),\n            nib.freesurfer.io.read_morph_data(  # type: ignore[no-untyped-call]\n                f\"{subject_freesurfer_directory}/surf/rh.thickness\",\n            ),\n        ],\n    )\n    return np.asarray(adap_area_barycentric_transformation @ fsnative_thickness)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fsnative_to_fslr32k_transformation","title":"<code>compute_fsnative_to_fslr32k_transformation(subject_freesurfer_directory: str) -&gt; sparse.csr_matrix</code>","text":"<p>Compute the transformation matrix from FreeSurfer native space to fs_LR-32k space using an adaptive area barycentric transformation.</p> <p>Note: This function combines the vertices across left and right hemispheres to create a unified transformation matrix (left first, then right).</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fsnative_to_fslr32k_transformation--parameters","title":"Parameters","text":"<p>subject_freesurfer_directory : str     Path to the FreeSurfer subject directory.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_fsnative_to_fslr32k_transformation--returns","title":"Returns","text":"<p>sparse.csr_matrix     Sparse matrix representing the transformation from fsnative space to fs_LR-32k.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_fsnative_to_fslr32k_transformation(\n    subject_freesurfer_directory: str,\n) -&gt; sparse.csr_matrix:\n    \"\"\"\n    Compute the transformation matrix from FreeSurfer native space to fs_LR-32k\n    space using an adaptive area barycentric transformation.\n\n    Note: This function combines the vertices across left and right hemispheres\n    to create a unified transformation matrix (left first, then right).\n\n    Parameters\n    ----------\n    subject_freesurfer_directory : str\n        Path to the FreeSurfer subject directory.\n\n    Returns\n    -------\n    sparse.csr_matrix\n        Sparse matrix representing the transformation from fsnative space to fs_LR-32k.\n    \"\"\"\n    # Load fs_LR 32k files\n    left_fslr_sphere_vertices, left_fslr_triangles = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.fs_LR_32k\")\n                / \"fs_LR-deformed_to-fsaverage.L.sphere.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    left_fslr_midthickness_vertices, _ = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.HCP\")\n                / \"S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    left_fslr_vertex_areas = compute_vertex_areas(\n        left_fslr_midthickness_vertices,\n        left_fslr_triangles,\n    )\n    right_fslr_sphere_vertices, right_fslr_triangles = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.fs_LR_32k\")\n                / \"fs_LR-deformed_to-fsaverage.R.sphere.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    right_fslr_midthickness_vertices, _ = load_gifti_surface(\n        Path(\n            str(\n                resources.files(\"spectranorm.data.templates.HCP\")\n                / \"S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii\",\n            ),\n        ),\n    )\n    right_fslr_vertex_areas = compute_vertex_areas(\n        right_fslr_midthickness_vertices,\n        right_fslr_triangles,\n    )\n\n    # Load subject native files\n    left_fsnative_sphere_vertices, left_fsnative_triangles = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.sphere.reg\",\n    )\n    left_fsnative_white_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.white\",\n    )\n    left_fsnative_pial_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/lh.pial\",\n    )\n    left_fsnative_midthickness_vertices = np.asarray(\n        (left_fsnative_white_vertices + left_fsnative_pial_vertices) * 0.5,\n    )\n    left_fsnative_vertex_areas = compute_vertex_areas(\n        left_fsnative_midthickness_vertices,\n        left_fsnative_triangles,\n    )\n    right_fsnative_sphere_vertices, right_fsnative_triangles = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.sphere.reg\",\n    )\n    right_fsnative_white_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.white\",\n    )\n    right_fsnative_pial_vertices, _ = load_freesurfer_surface(\n        f\"{subject_freesurfer_directory}/surf/rh.pial\",\n    )\n    right_fsnative_midthickness_vertices = np.asarray(\n        (right_fsnative_white_vertices + right_fsnative_pial_vertices) * 0.5,\n    )\n    right_fsnative_vertex_areas = compute_vertex_areas(\n        right_fsnative_midthickness_vertices,\n        right_fsnative_triangles,\n    )\n\n    # Compute transformations\n    left_surface_transformation = compute_adaptive_area_barycentric_transformation(\n        source_vertices=left_fsnative_sphere_vertices,\n        source_triangles=left_fsnative_triangles,\n        source_vertex_areas=left_fsnative_vertex_areas,\n        target_vertices=left_fslr_sphere_vertices,\n        target_triangles=left_fslr_triangles,\n        target_vertex_areas=left_fslr_vertex_areas,\n    )\n    right_surface_transformation = compute_adaptive_area_barycentric_transformation(\n        source_vertices=right_fsnative_sphere_vertices,\n        source_triangles=right_fsnative_triangles,\n        source_vertex_areas=right_fsnative_vertex_areas,\n        target_vertices=right_fslr_sphere_vertices,\n        target_triangles=right_fslr_triangles,\n        target_vertex_areas=right_fslr_vertex_areas,\n    )\n\n    # Combine the transformations and return\n    return sparse.csr_matrix(\n        sparse.block_diag(\n            [\n                left_surface_transformation,\n                right_surface_transformation,\n            ],\n        ),\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_total_euler_number","title":"<code>compute_total_euler_number(subject_freesurfer_directory: str) -&gt; int</code>","text":"<p>Compute the total Euler number from FreeSurfer output directory. Note: This function assumes FreeSurfer's recon-all is completed.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_total_euler_number--parameters","title":"Parameters","text":"<p>subject_freesurfer_directory : str     Path to the FreeSurfer subject directory.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_total_euler_number--returns","title":"Returns","text":"<p>int     The total Euler number (sum over left and right surfaces).</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_total_euler_number(subject_freesurfer_directory: str) -&gt; int:\n    \"\"\"\n    Compute the total Euler number from FreeSurfer output directory.\n    Note: This function assumes FreeSurfer's recon-all is completed.\n\n    Parameters\n    ----------\n    subject_freesurfer_directory : str\n        Path to the FreeSurfer subject directory.\n\n    Returns\n    -------\n    int\n        The total Euler number (sum over left and right surfaces).\n    \"\"\"\n    left_surface = f\"{subject_freesurfer_directory}/surf/lh.orig.nofix\"\n    right_surface = f\"{subject_freesurfer_directory}/surf/rh.orig.nofix\"\n\n    # Compute the Euler characteristic for each surface\n    left_euler = get_euler_number(*load_freesurfer_surface(left_surface))\n    right_euler = get_euler_number(*load_freesurfer_surface(right_surface))\n\n    # Return the total Euler number\n    return left_euler + right_euler\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_vertex_areas","title":"<code>compute_vertex_areas(vertices: npt.NDArray[np.floating[Any]], triangles: npt.NDArray[np.integer[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the surface area of each vertex in a triangular mesh.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_vertex_areas--parameters","title":"Parameters","text":"<p>vertices : numpy.ndarray     Array of shape (n_vertices, 3) containing vertex coordinates. triangles : numpy.ndarray     Array of shape (n_triangles, 3) containing vertex indices for each triangular     face.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.compute_vertex_areas--returns","title":"Returns","text":"<p>vertex_areas : numpy.ndarray     Array of shape (n_vertices,) containing the area associated with each vertex.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def compute_vertex_areas(\n    vertices: npt.NDArray[np.floating[Any]],\n    triangles: npt.NDArray[np.integer[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the surface area of each vertex in a triangular mesh.\n\n    Parameters\n    ----------\n    vertices : numpy.ndarray\n        Array of shape (n_vertices, 3) containing vertex coordinates.\n    triangles : numpy.ndarray\n        Array of shape (n_triangles, 3) containing vertex indices for each triangular\n        face.\n\n    Returns\n    -------\n    vertex_areas : numpy.ndarray\n        Array of shape (n_vertices,) containing the area associated with each vertex.\n    \"\"\"\n    # Compute the area of each triangle\n    v0 = vertices[triangles[:, 0]]\n    v1 = vertices[triangles[:, 1]]\n    v2 = vertices[triangles[:, 2]]\n    triangle_areas = 0.5 * np.linalg.norm(np.cross(v1 - v0, v2 - v0), axis=1)\n\n    # Accumulate areas for each vertex\n    # One third of each triangle's area is attributed to each of its vertices\n    vertex_areas = np.zeros(vertices.shape[0])\n    np.add.at(vertex_areas, triangles[:, 0], triangle_areas / 3)\n    np.add.at(vertex_areas, triangles[:, 1], triangle_areas / 3)\n    np.add.at(vertex_areas, triangles[:, 2], triangle_areas / 3)\n\n    return vertex_areas\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.get_euler_number","title":"<code>get_euler_number(vertices: npt.NDArray[np.floating[Any]], triangles: npt.NDArray[np.integer[Any]]) -&gt; int</code>","text":"<p>Calculate the Euler number of a surface.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.get_euler_number--parameters","title":"Parameters","text":"<p>vertices : numpy.ndarray     Array of shape (n_vertices, 3) containing vertex coordinates. triangles : numpy.ndarray     Array of shape (n_triangles, 3) containing vertex indices for each triangular     face.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.get_euler_number--returns","title":"Returns","text":"<p>euler_number : int     The Euler number of the surface.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def get_euler_number(\n    vertices: npt.NDArray[np.floating[Any]],\n    triangles: npt.NDArray[np.integer[Any]],\n) -&gt; int:\n    \"\"\"\n    Calculate the Euler number of a surface.\n\n    Parameters\n    ----------\n    vertices : numpy.ndarray\n        Array of shape (n_vertices, 3) containing vertex coordinates.\n    triangles : numpy.ndarray\n        Array of shape (n_triangles, 3) containing vertex indices for each triangular\n        face.\n\n    Returns\n    -------\n    euler_number : int\n        The Euler number of the surface.\n    \"\"\"\n    # Euler characteristic: V - E + F = 2 for closed surfaces\n    # where V = number of vertices, E = number of edges, F = number of faces\n\n    n_vertices = vertices.shape[0]\n    n_faces = triangles.shape[0]\n\n    # Compute number of unique edges\n    # Extract all edges from triangles\n    edges = np.vstack(\n        [\n            triangles[:, [0, 1]],\n            triangles[:, [1, 2]],\n            triangles[:, [2, 0]],\n        ],\n    )\n    # Sort edges so that [1,2] and [2,1] are considered the same\n    edges = np.sort(edges, axis=1)\n    # Count unique edges\n    n_edges = len(np.unique(edges, axis=0))\n\n    # Compute and return the Euler number\n    return int(n_vertices - n_edges + n_faces)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.get_fslr_surface_indices_from_cifti","title":"<code>get_fslr_surface_indices_from_cifti(cifti_file: Path | None = None) -&gt; npt.NDArray[np.integer[Any]]</code>","text":"<p>Get the fs_LR surface indices from a CIFTI file (excluding the medial wall).</p>"},{"location":"api_docs/#spectranorm.utils.nitools.get_fslr_surface_indices_from_cifti--parameters","title":"Parameters","text":"<p>cifti_file : str     Path to the CIFTI file. By default a ones.dscalar.nii template will be used.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.get_fslr_surface_indices_from_cifti--returns","title":"Returns","text":"<p>np.ndarray     The indices indicating of vertices present in the CIFTI format.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def get_fslr_surface_indices_from_cifti(\n    cifti_file: Path | None = None,\n) -&gt; npt.NDArray[np.integer[Any]]:\n    \"\"\"\n    Get the fs_LR surface indices from a CIFTI file (excluding the medial wall).\n\n    Parameters\n    ----------\n    cifti_file : str\n        Path to the CIFTI file. By default a ones.dscalar.nii template will be used.\n\n    Returns\n    -------\n    np.ndarray\n        The indices indicating of vertices present in the CIFTI format.\n    \"\"\"\n    # Use default ones.dscalar.nii if no file is provided\n    if cifti_file is None:\n        cifti_file = Path(\n            str(\n                resources.files(\"spectranorm.data.templates.CIFTI\")\n                / \"ones.dscalar.nii\",\n            ),\n        )\n    cifti = nib.loadsave.load(str(cifti_file))\n    if not isinstance(cifti, nib.cifti2.cifti2.Cifti2Image):\n        err = f\"File {cifti_file} is not a valid CIFTI file.\"\n        raise TypeError(err)\n\n    # Extract the brain models for left and right cortical surfaces\n    brain_models = list(cifti.header.get_index_map(1).brain_models)  # type: ignore[no-untyped-call]\n    left_surface_model, right_surface_model = brain_models[0], brain_models[1]\n\n    # Return the indices for left and right surfaces\n    return np.concatenate(\n        [\n            np.asarray(left_surface_model.vertex_indices),\n            (\n                np.asarray(right_surface_model.vertex_indices)\n                + left_surface_model.surface_number_of_vertices\n            ),\n        ],\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.load_freesurfer_surface","title":"<code>load_freesurfer_surface(file: str) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]</code>","text":"<p>Load a FreeSurfer surface file.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.load_freesurfer_surface--parameters","title":"Parameters","text":"<p>file : str     Path to the FreeSurfer surface file to be loaded.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.load_freesurfer_surface--returns","title":"Returns","text":"<p>vertices : numpy.ndarray     Array of shape (n_vertices, 3) containing vertex coordinates. triangles : numpy.ndarray     Array of shape (n_triangles, 3) containing vertex indices for each triangular     face.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def load_freesurfer_surface(\n    file: str,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]:\n    \"\"\"\n    Load a FreeSurfer surface file.\n\n    Parameters\n    ----------\n    file : str\n        Path to the FreeSurfer surface file to be loaded.\n\n    Returns\n    -------\n    vertices : numpy.ndarray\n        Array of shape (n_vertices, 3) containing vertex coordinates.\n    triangles : numpy.ndarray\n        Array of shape (n_triangles, 3) containing vertex indices for each triangular\n        face.\n    \"\"\"\n    surface = nib.freesurfer.io.read_geometry(file)  # type: ignore[no-untyped-call]\n    vertices = np.asarray(surface[0], dtype=np.float32)\n    triangles = np.asarray(surface[1], dtype=np.int32)\n    return vertices, triangles\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.nitools.load_gifti_surface","title":"<code>load_gifti_surface(file: Path) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]</code>","text":"<p>Load a GIfTI surface file.</p>"},{"location":"api_docs/#spectranorm.utils.nitools.load_gifti_surface--parameters","title":"Parameters","text":"<p>file : Path     Path to the GIfTI surface file to be loaded (.gii).</p>"},{"location":"api_docs/#spectranorm.utils.nitools.load_gifti_surface--returns","title":"Returns","text":"<p>vertices : numpy.ndarray     Array of shape (n_vertices, 3) containing vertex coordinates. triangles : numpy.ndarray     Array of shape (n_triangles, 3) containing vertex indices for each triangular     face.</p> Source code in <code>src/spectranorm/utils/nitools.py</code> <pre><code>def load_gifti_surface(\n    file: Path,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.integer[Any]]]:\n    \"\"\"\n    Load a GIfTI surface file.\n\n    Parameters\n    ----------\n    file : Path\n        Path to the GIfTI surface file to be loaded (.gii).\n\n    Returns\n    -------\n    vertices : numpy.ndarray\n        Array of shape (n_vertices, 3) containing vertex coordinates.\n    triangles : numpy.ndarray\n        Array of shape (n_triangles, 3) containing vertex indices for each triangular\n        face.\n    \"\"\"\n    gifti_data = cast(\"nib.gifti.gifti.GiftiImage\", nib.loadsave.load(file))\n    vertices = gifti_data.darrays[0].data\n    triangles = gifti_data.darrays[1].data\n    return vertices, triangles\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.parallel","title":"<code>parallel</code>","text":"<p>utils/parallel.py</p> <p>Utility functions for spectranorm's parallel execution (e.g. via Joblib).</p>"},{"location":"api_docs/#spectranorm.utils.parallel.ParallelTqdm","title":"<code>ParallelTqdm</code>","text":"<p>               Bases: <code>Parallel</code></p> <p>joblib.Parallel, but with a tqdm progressbar</p>"},{"location":"api_docs/#spectranorm.utils.parallel.ParallelTqdm--additional-parameters","title":"Additional parameters:","text":"<p>total_tasks: int, default: None     the number of expected jobs. Used in the tqdm progressbar.     If None, try to infer from the length of the called iterator, and     fallback to use the number of remaining items as soon as we finish     dispatching.     Note: use a list instead of an iterator if you want the total_tasks     to be inferred from its length.</p> str, default: None <p>the description used in the tqdm progressbar.</p> bool, default: False <p>If True, a tqdm progressbar is not used.</p> bool, default: False <p>If True, show joblib header before the progressbar.</p>"},{"location":"api_docs/#spectranorm.utils.parallel.ParallelTqdm--removed-parameters","title":"Removed parameters:","text":"<p>verbose: will be ignored</p>"},{"location":"api_docs/#spectranorm.utils.parallel.ParallelTqdm--usage","title":"Usage:","text":"<p>from joblib import delayed from time import sleep ParallelTqdm(n_jobs=-1)([delayed(sleep)(.1) for _ in range(10)]) 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:02&lt;00:00,  3.12tasks/s]</p> Source code in <code>src/spectranorm/utils/parallel.py</code> <pre><code>class ParallelTqdm(Parallel):\n    \"\"\"joblib.Parallel, but with a tqdm progressbar\n\n    Additional parameters:\n    ----------------------\n    total_tasks: int, default: None\n        the number of expected jobs. Used in the tqdm progressbar.\n        If None, try to infer from the length of the called iterator, and\n        fallback to use the number of remaining items as soon as we finish\n        dispatching.\n        Note: use a list instead of an iterator if you want the total_tasks\n        to be inferred from its length.\n\n    desc: str, default: None\n        the description used in the tqdm progressbar.\n\n    disable_progressbar: bool, default: False\n        If True, a tqdm progressbar is not used.\n\n    show_joblib_header: bool, default: False\n        If True, show joblib header before the progressbar.\n\n    Removed parameters:\n    -------------------\n    verbose: will be ignored\n\n\n    Usage:\n    ------\n    &gt;&gt;&gt; from joblib import delayed\n    &gt;&gt;&gt; from time import sleep\n    &gt;&gt;&gt; ParallelTqdm(n_jobs=-1)([delayed(sleep)(.1) for _ in range(10)])\n    80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:02&lt;00:00,  3.12tasks/s]\n\n    \"\"\"\n\n    _original_iterator: Iterable[Any] | None  # mimic joblib internal attribute\n    n_dispatched_tasks: int\n    n_completed_tasks: int\n\n    def __init__(\n        self,\n        *,\n        total_tasks: int | None = None,\n        desc: str | None = None,\n        disable_progressbar: bool = False,\n        show_joblib_header: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        if \"verbose\" in kwargs:\n            err = (\n                \"verbose is not supported. \"\n                \"Use disable_progressbar and show_joblib_header instead.\"\n            )\n            raise ValueError(err)\n        super().__init__(verbose=(1 if show_joblib_header else 0), **kwargs)\n        self.total_tasks = total_tasks\n        self.desc = desc\n        self.disable_progressbar = disable_progressbar\n        self.progress_bar: tqdm.tqdm[Any] | None = None\n\n    def __call__(self, iterable: Iterable[Any]) -&gt; Any:\n        try:\n            if self.total_tasks is None and isinstance(iterable, Sized):\n                # try to infer total_tasks from the length of the called iterator\n                with suppress(TypeError, AttributeError):\n                    self.total_tasks = len(iterable)\n            # call parent function\n            return super().__call__(iterable)\n        finally:\n            # close tqdm progress bar\n            if self.progress_bar is not None:\n                self.progress_bar.close()\n\n    __call__.__doc__ = Parallel.__call__.__doc__\n\n    def dispatch_one_batch(self, iterator: Iterable[Any]) -&gt; Any:\n        # start progress_bar, if not started yet.\n        if self.progress_bar is None:\n            self.progress_bar = tqdm.tqdm(\n                desc=self.desc,\n                total=self.total_tasks,\n                disable=self.disable_progressbar,\n                unit=\"tasks\",\n            )\n        # call parent function\n        return super().dispatch_one_batch(iterator)\n\n    dispatch_one_batch.__doc__ = Parallel.dispatch_one_batch.__doc__\n\n    def print_progress(self) -&gt; None:\n        \"\"\"Display the process of the parallel execution using tqdm\"\"\"\n        # if we finish dispatching, find total_tasks from the number of remaining items\n        if self.total_tasks is None and self._original_iterator is None:\n            self.total_tasks = self.n_dispatched_tasks\n            if self.progress_bar is not None:\n                self.progress_bar.total = self.total_tasks\n                self.progress_bar.refresh()\n        # update progressbar\n        if self.progress_bar is not None:\n            self.progress_bar.update(self.n_completed_tasks - self.progress_bar.n)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.parallel.ParallelTqdm.print_progress","title":"<code>print_progress() -&gt; None</code>","text":"<p>Display the process of the parallel execution using tqdm</p> Source code in <code>src/spectranorm/utils/parallel.py</code> <pre><code>def print_progress(self) -&gt; None:\n    \"\"\"Display the process of the parallel execution using tqdm\"\"\"\n    # if we finish dispatching, find total_tasks from the number of remaining items\n    if self.total_tasks is None and self._original_iterator is None:\n        self.total_tasks = self.n_dispatched_tasks\n        if self.progress_bar is not None:\n            self.progress_bar.total = self.total_tasks\n            self.progress_bar.refresh()\n    # update progressbar\n    if self.progress_bar is not None:\n        self.progress_bar.update(self.n_completed_tasks - self.progress_bar.n)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.stats","title":"<code>stats</code>","text":"<p>utils/stats.py</p> <p>Statistical utility functions for the Spectranorm package.</p>"},{"location":"api_docs/#spectranorm.utils.stats.compute_censored_log_likelihood","title":"<code>compute_censored_log_likelihood(observations: npt.NDArray[np.floating[Any]], predicted_mus: npt.NDArray[np.floating[Any]], predicted_sigmas: npt.NDArray[np.floating[Any]], censored_quantile: float = 0.01) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute censored log likelihood, replacing extreme low likelihoods with a censoring threshold.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Observed data points (N,).</p> required <code>predicted_mus</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted means for each observation (N,).</p> required <code>predicted_sigmas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted standard deviations for each observation (N,).</p> required <code>censored_quantile</code> <code>float</code> <p>float (default=0.01) Quantile below which log-likelihoods are censored.</p> <code>0.01</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray: The censored log likelihood of all observations.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_censored_log_likelihood(\n    observations: npt.NDArray[np.floating[Any]],\n    predicted_mus: npt.NDArray[np.floating[Any]],\n    predicted_sigmas: npt.NDArray[np.floating[Any]],\n    censored_quantile: float = 0.01,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute censored log likelihood, replacing extreme low likelihoods with a censoring\n    threshold.\n\n    Args:\n        observations: np.ndarray\n            Observed data points (N,).\n        predicted_mus: np.ndarray\n            Predicted means for each observation (N,).\n        predicted_sigmas: np.ndarray\n            Predicted standard deviations for each observation (N,).\n        censored_quantile: float (default=0.01)\n            Quantile below which log-likelihoods are censored.\n\n    Returns:\n        np.ndarray:\n            The censored log likelihood of all observations.\n    \"\"\"\n    # Compute log likelihoods\n    log_likelihoods = compute_log_likelihood(\n        observations,\n        predicted_mus,\n        predicted_sigmas,\n    )\n\n    # Compute censoring threshold based on standard normal\n    censor_threshold = stats.norm.logpdf(\n        stats.norm.ppf(censored_quantile),\n        loc=0,\n        scale=1,\n    )\n\n    # Apply censoring (two-sided) and return\n    return np.where(\n        log_likelihoods &lt; censor_threshold,\n        np.log(2 * censored_quantile),\n        log_likelihoods,\n    )\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.stats.compute_centiles_from_z_scores","title":"<code>compute_centiles_from_z_scores(z_scores: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Convert z-scores to percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>z_scores</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of z-scores.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Array of percentiles corresponding to the z-scores.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_centiles_from_z_scores(\n    z_scores: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Convert z-scores to percentiles.\n\n    Args:\n        z_scores: np.ndarray\n            Array of z-scores.\n\n    Returns:\n        np.ndarray\n            Array of percentiles corresponding to the z-scores.\n    \"\"\"\n    # Convert z-scores to percentiles using the cumulative distribution function (CDF)\n    return np.asarray(stats.norm.cdf(z_scores) * 100)\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.stats.compute_correlation_significance_by_fisher_z","title":"<code>compute_correlation_significance_by_fisher_z(correlation_matrix: npt.NDArray[np.floating[Any]], n_samples: int, correlation_threshold: float = 0.0) -&gt; npt.NDArray[np.bool_]</code>","text":"<p>Compute the significance of correlations between variables in the data matrix, thresholded by a specified correlation value, using Fisher's z-transformation.</p> <p>Parameters:</p> Name Type Description Default <code>correlation_matrix</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray A 2D array of pairwise correlation values.</p> required <code>n_samples</code> <code>int</code> <p>int The number of samples used to compute correlation.</p> required <code>correlation_threshold</code> <code>float</code> <p>float (default=0.0) The correlation threshold above which correlations are to be considered significant.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>NDArray[bool_]</code> <p>np.ndarray A boolean matrix indicating significantly large correlations between variables. The True values indicate correlations that are significantly larger than the threshold.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_correlation_significance_by_fisher_z(\n    correlation_matrix: npt.NDArray[np.floating[Any]],\n    n_samples: int,\n    correlation_threshold: float = 0.0,\n) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Compute the significance of correlations between variables in the data matrix,\n    thresholded by a specified correlation value, using Fisher's z-transformation.\n\n    Args:\n        correlation_matrix: np.ndarray\n            A 2D array of pairwise correlation values.\n        n_samples: int\n            The number of samples used to compute correlation.\n        correlation_threshold: float (default=0.0)\n            The correlation threshold above which correlations are to be considered\n            significant.\n\n    Returns:\n        np.ndarray\n            A boolean matrix indicating significantly large correlations between\n            variables. The True values indicate correlations that are significantly\n            larger than the threshold.\n    \"\"\"\n    # Fisher's z-transformation\n    fisher_z = np.arctanh(correlation_matrix)\n    z_threshold = np.arctanh(correlation_threshold)\n    # Standard error of the Fisher z\n    se = 1 / np.sqrt(n_samples - 3)\n    # Apply the correlation threshold and return significance matrix\n    # 95% confidence interval\n    return np.asarray(np.abs(fisher_z) &gt; (z_threshold + (se * 1.96)))\n</code></pre>"},{"location":"api_docs/#spectranorm.utils.stats.compute_log_likelihood","title":"<code>compute_log_likelihood(observations: npt.NDArray[np.floating[Any]], predicted_mus: npt.NDArray[np.floating[Any]], predicted_sigmas: npt.NDArray[np.floating[Any]]) -&gt; npt.NDArray[np.floating[Any]]</code>","text":"<p>Compute the log likelihood of observations given predicted means and standard deviations.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Observed data points.</p> required <code>predicted_mus</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted means for the observations.</p> required <code>predicted_sigmas</code> <code>NDArray[floating[Any]]</code> <p>np.ndarray Predicted standard deviations for the observations.</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>np.ndarray Log likelihood of each observation.</p> Source code in <code>src/spectranorm/utils/stats.py</code> <pre><code>def compute_log_likelihood(\n    observations: npt.NDArray[np.floating[Any]],\n    predicted_mus: npt.NDArray[np.floating[Any]],\n    predicted_sigmas: npt.NDArray[np.floating[Any]],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Compute the log likelihood of observations given predicted means and standard\n    deviations.\n\n    Args:\n        observations: np.ndarray\n            Observed data points.\n        predicted_mus: np.ndarray\n            Predicted means for the observations.\n        predicted_sigmas: np.ndarray\n            Predicted standard deviations for the observations.\n\n    Returns:\n        np.ndarray\n            Log likelihood of each observation.\n    \"\"\"\n    return np.asarray(\n        stats.norm.logpdf(observations, loc=predicted_mus, scale=predicted_sigmas),\n    )\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""}]}